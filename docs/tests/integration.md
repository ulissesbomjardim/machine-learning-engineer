# üîó Testes de Integra√ß√£o

Documenta√ß√£o completa dos testes de integra√ß√£o, incluindo configura√ß√£o de ambiente, mocks de servi√ßos externos, testes de API end-to-end e valida√ß√£o de pipeline de Machine Learning.

## üéØ Vis√£o Geral

Os testes de integra√ß√£o verificam se os componentes do sistema trabalham corretamente em conjunto, simulando cen√°rios reais de uso e validando integra√ß√µes entre diferentes m√≥dulos.

## üèóÔ∏è Estrutura dos Testes de Integra√ß√£o

### üìÅ Organiza√ß√£o dos Arquivos

```
tests/
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Configura√ß√£o fixtures integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_api_integration.py     # Testes API completos
‚îÇ   ‚îú‚îÄ‚îÄ test_ml_pipeline.py         # Testes pipeline ML
‚îÇ   ‚îú‚îÄ‚îÄ test_database_integration.py # Testes integra√ß√£o DB
‚îÇ   ‚îú‚îÄ‚îÄ test_external_apis.py       # Testes APIs externas
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îÇ       ‚îú‚îÄ‚îÄ sample_data.json        # Dados de teste
‚îÇ       ‚îú‚îÄ‚îÄ mock_responses/         # Respostas mockadas
‚îÇ       ‚îî‚îÄ‚îÄ test_datasets/          # Datasets de teste
‚îî‚îÄ‚îÄ e2e/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_complete_workflow.py   # Fluxo completo E2E
    ‚îî‚îÄ‚îÄ test_user_scenarios.py      # Cen√°rios de usu√°rio
```

## ‚öôÔ∏è Configura√ß√£o dos Testes

### üìã conftest.py - Fixtures de Integra√ß√£o

```python
# tests/integration/conftest.py
import pytest
import asyncio
import pandas as pd
from pathlib import Path
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from unittest.mock import AsyncMock, patch
import json

from src.main import app
from src.database import get_db, Base
from src.config import settings
from src.services.database import DatabaseService
from src.services.external_apis import WeatherService, AirportService

# Database de teste
SQLALCHEMY_TEST_DATABASE_URL = "sqlite:///./test.db"

@pytest.fixture(scope="session")
def event_loop():
    """Cria loop de eventos para testes async"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
def test_engine():
    """Engine de teste do SQLAlchemy"""
    engine = create_engine(
        SQLALCHEMY_TEST_DATABASE_URL,
        connect_args={"check_same_thread": False}
    )
    Base.metadata.create_all(bind=engine)
    yield engine
    Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="session")
def test_session_factory(test_engine):
    """Factory de sess√µes de teste"""
    TestingSessionLocal = sessionmaker(
        autocommit=False,
        autoflush=False,
        bind=test_engine
    )
    return TestingSessionLocal

@pytest.fixture(scope="function")
def test_db(test_session_factory):
    """Sess√£o de banco para testes individuais"""
    session = test_session_factory()
    try:
        yield session
    finally:
        session.rollback()
        session.close()

@pytest.fixture(scope="function")
def test_app(test_db):
    """Cliente de teste FastAPI com DB mockado"""
    def override_get_db():
        try:
            yield test_db
        finally:
            pass
    
    app.dependency_overrides[get_db] = override_get_db
    
    with TestClient(app) as client:
        yield client
    
    app.dependency_overrides.clear()

@pytest.fixture
def sample_flight_data():
    """Dados de voo para testes"""
    return {
        "flight_number": "AA123",
        "airline": "American Airlines", 
        "origin_airport": "JFK",
        "destination_airport": "LAX",
        "scheduled_departure": "2024-01-15T10:00:00",
        "scheduled_arrival": "2024-01-15T13:30:00",
        "aircraft_type": "Boeing 737",
        "passenger_count": 150,
        "cargo_weight": 5000.5,
        "weather_conditions": {
            "temperature": 22.5,
            "humidity": 65.2,
            "wind_speed": 15.3,
            "visibility": 10.0,
            "precipitation": 0.0
        },
        "airport_info": {
            "origin": {
                "timezone": "America/New_York",
                "elevation": 13,
                "runway_count": 4
            },
            "destination": {
                "timezone": "America/Los_Angeles", 
                "elevation": 38,
                "runway_count": 4
            }
        }
    }

@pytest.fixture
def sample_historical_data():
    """Dataset hist√≥rico para testes"""
    data = {
        'flight_number': ['AA123', 'UA456', 'DL789'] * 100,
        'airline': ['American Airlines', 'United Airlines', 'Delta Airlines'] * 100,
        'origin_airport': ['JFK', 'LAX', 'ORD'] * 100,
        'destination_airport': ['LAX', 'ORD', 'JFK'] * 100,
        'scheduled_departure_hour': [10, 14, 18] * 100,
        'day_of_week': [1, 2, 3, 4, 5, 6, 7] * 43,  # ~300 registros
        'month': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] * 25,
        'weather_score': [0.8, 0.6, 0.9] * 100,
        'airport_congestion': [0.3, 0.7, 0.5] * 100,
        'delay_minutes': [0, 15, 30, 45, 0, 0, 10] * 43  # Target variable
    }
    
    return pd.DataFrame(data)

@pytest.fixture
def mock_weather_service():
    """Mock do servi√ßo de clima"""
    with patch('src.services.external_apis.WeatherService') as mock:
        service = AsyncMock()
        
        # Configurar respostas padr√£o
        service.get_current_weather.return_value = {
            "temperature": 25.0,
            "humidity": 60.0,
            "wind_speed": 10.0,
            "visibility": 10.0,
            "precipitation": 0.0,
            "weather_score": 0.85
        }
        
        service.get_forecast.return_value = [
            {
                "datetime": "2024-01-15T10:00:00",
                "temperature": 25.0,
                "conditions": "Clear"
            }
        ]
        
        mock.return_value = service
        yield service

@pytest.fixture  
def mock_airport_service():
    """Mock do servi√ßo de aeroportos"""
    with patch('src.services.external_apis.AirportService') as mock:
        service = AsyncMock()
        
        # Dados de aeroportos mockados
        airport_data = {
            "JFK": {
                "name": "John F. Kennedy International",
                "city": "New York",
                "country": "United States",
                "timezone": "America/New_York",
                "elevation": 13,
                "runway_count": 4,
                "capacity_score": 0.9
            },
            "LAX": {
                "name": "Los Angeles International",
                "city": "Los Angeles", 
                "country": "United States",
                "timezone": "America/Los_Angeles",
                "elevation": 38,
                "runway_count": 4,
                "capacity_score": 0.85
            }
        }
        
        service.get_airport_info.side_effect = lambda code: airport_data.get(code)
        service.get_congestion_level.return_value = 0.3
        
        mock.return_value = service
        yield service

@pytest.fixture
def test_model_artifacts(tmp_path):
    """Cria artefatos de modelo para teste"""
    
    # Criar diret√≥rio de modelos tempor√°rio
    model_dir = tmp_path / "models"
    model_dir.mkdir()
    
    # Mock de modelo treinado (pickle)
    import pickle
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    
    # Modelo simples para teste
    model = RandomForestRegressor(n_estimators=10, random_state=42)
    scaler = StandardScaler()
    
    # Treinar com dados dummy
    X_dummy = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]
    y_dummy = [10, 20, 30]
    
    scaler.fit(X_dummy)
    model.fit(scaler.transform(X_dummy), y_dummy)
    
    # Salvar artefatos
    model_path = model_dir / "flight_delay_model.pkl"
    scaler_path = model_dir / "feature_scaler.pkl"
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    
    # Metadados do modelo
    metadata = {
        "model_version": "1.0.0",
        "training_date": "2024-01-15",
        "features": ["hour", "day_of_week", "month", "weather_score", "congestion"],
        "metrics": {
            "mae": 12.5,
            "rmse": 18.7,
            "r2_score": 0.76
        }
    }
    
    metadata_path = model_dir / "model_metadata.json"
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f)
    
    return {
        "model_path": model_path,
        "scaler_path": scaler_path,
        "metadata_path": metadata_path,
        "model_dir": model_dir
    }

@pytest.fixture
def integration_config():
    """Configura√ß√£o espec√≠fica para testes de integra√ß√£o"""
    return {
        "api_timeout": 30,
        "max_retries": 3,
        "batch_size": 100,
        "weather_api_key": "test_key_123",
        "airport_api_key": "test_key_456",
        "model_threshold": 0.8,
        "cache_ttl": 300
    }
```

## üß™ Testes de API Completos

### üåê test_api_integration.py

```python
# tests/integration/test_api_integration.py
import pytest
import asyncio
from datetime import datetime, timedelta
import json

class TestAPIIntegration:
    """Testes completos de integra√ß√£o da API"""
    
    @pytest.mark.asyncio
    async def test_predict_endpoint_complete_flow(
        self, 
        test_app, 
        sample_flight_data,
        mock_weather_service,
        mock_airport_service,
        test_model_artifacts
    ):
        """Testa fluxo completo do endpoint de predi√ß√£o"""
        
        # Configurar mocks
        with pytest.MonkeyPatch.context() as mp:
            mp.setattr("src.ml.model_loader.MODEL_PATH", test_model_artifacts["model_dir"])
            
            # Fazer predi√ß√£o
            response = test_app.post(
                "/api/v1/predict",
                json=sample_flight_data
            )
            
            # Verificar resposta
            assert response.status_code == 200
            
            result = response.json()
            assert "prediction" in result
            assert "confidence" in result
            assert "metadata" in result
            
            # Verificar estrutura da predi√ß√£o
            prediction = result["prediction"]
            assert isinstance(prediction["delay_minutes"], (int, float))
            assert isinstance(prediction["probability_delayed"], float)
            assert 0 <= prediction["probability_delayed"] <= 1
            
            # Verificar metadados
            metadata = result["metadata"]
            assert "model_version" in metadata
            assert "prediction_time" in metadata
            assert "processing_time_ms" in metadata
    
    @pytest.mark.asyncio
    async def test_batch_prediction_endpoint(
        self,
        test_app,
        sample_flight_data,
        test_model_artifacts
    ):
        """Testa endpoint de predi√ß√£o em lote"""
        
        # Criar m√∫ltiplos voos
        flights = []
        for i in range(5):
            flight = sample_flight_data.copy()
            flight["flight_number"] = f"AA{123 + i}"
            flights.append(flight)
        
        with pytest.MonkeyPatch.context() as mp:
            mp.setattr("src.ml.model_loader.MODEL_PATH", test_model_artifacts["model_dir"])
            
            response = test_app.post(
                "/api/v1/predict/batch",
                json={"flights": flights}
            )
            
            assert response.status_code == 200
            
            result = response.json()
            assert "predictions" in result
            assert len(result["predictions"]) == 5
            
            # Verificar cada predi√ß√£o
            for pred in result["predictions"]:
                assert "flight_number" in pred
                assert "prediction" in pred
                assert "status" in pred
                assert pred["status"] == "success"
    
    @pytest.mark.asyncio 
    async def test_model_info_endpoint(self, test_app, test_model_artifacts):
        """Testa endpoint de informa√ß√µes do modelo"""
        
        with pytest.MonkeyPatch.context() as mp:
            mp.setattr("src.ml.model_loader.MODEL_PATH", test_model_artifacts["model_dir"])
            
            response = test_app.get("/api/v1/model/info")
            
            assert response.status_code == 200
            
            info = response.json()
            assert "version" in info
            assert "features" in info
            assert "metrics" in info
            assert "training_date" in info
    
    @pytest.mark.asyncio
    async def test_health_check_complete(self, test_app):
        """Testa health check completo do sistema"""
        
        response = test_app.get("/health")
        
        assert response.status_code == 200
        
        health = response.json()
        assert "status" in health
        assert "timestamp" in health
        assert "checks" in health
        
        # Verificar checks individuais
        checks = health["checks"]
        expected_checks = ["database", "model", "external_apis"]
        
        for check_name in expected_checks:
            assert check_name in checks
            assert "status" in checks[check_name]
            assert "response_time" in checks[check_name]
    
    def test_error_handling_invalid_data(self, test_app):
        """Testa tratamento de erros com dados inv√°lidos"""
        
        invalid_data = {
            "flight_number": "",  # Inv√°lido
            "origin_airport": "INVALID",  # C√≥digo inv√°lido
            "scheduled_departure": "invalid-date"  # Data inv√°lida
        }
        
        response = test_app.post(
            "/api/v1/predict",
            json=invalid_data
        )
        
        assert response.status_code == 422
        
        error = response.json()
        assert "detail" in error
        assert isinstance(error["detail"], list)
        
        # Verificar que todos os campos inv√°lidos foram identificados
        field_errors = [err["loc"][-1] for err in error["detail"]]
        assert "flight_number" in field_errors
        assert "origin_airport" in field_errors
    
    def test_rate_limiting(self, test_app, sample_flight_data):
        """Testa rate limiting da API"""
        
        # Fazer muitas requisi√ß√µes rapidamente
        responses = []
        for i in range(50):  # Assumindo limite de 100/min
            response = test_app.post(
                "/api/v1/predict",
                json=sample_flight_data
            )
            responses.append(response)
        
        # Todas as primeiras requisi√ß√µes devem funcionar
        success_responses = [r for r in responses if r.status_code == 200]
        rate_limited = [r for r in responses if r.status_code == 429]
        
        # Deve haver pelo menos algumas respostas de sucesso
        assert len(success_responses) > 0
        
        # Se houver rate limiting, verificar headers
        if rate_limited:
            limited_response = rate_limited[0]
            headers = limited_response.headers
            assert "X-RateLimit-Limit" in headers
            assert "X-RateLimit-Remaining" in headers

class TestDatabaseIntegration:
    """Testes de integra√ß√£o com banco de dados"""
    
    def test_flight_prediction_storage(
        self, 
        test_db, 
        sample_flight_data,
        test_model_artifacts
    ):
        """Testa armazenamento de predi√ß√µes no banco"""
        
        from src.services.database import DatabaseService
        
        db_service = DatabaseService(test_db)
        
        # Simular predi√ß√£o
        prediction_data = {
            "flight_id": "AA123_20240115",
            "flight_number": "AA123",
            "prediction": {
                "delay_minutes": 25.5,
                "probability_delayed": 0.75,
                "confidence": 0.82
            },
            "input_features": sample_flight_data,
            "model_version": "1.0.0",
            "prediction_time": datetime.now()
        }
        
        # Salvar predi√ß√£o
        prediction_id = db_service.save_prediction(prediction_data)
        assert prediction_id is not None
        
        # Recuperar predi√ß√£o
        stored_prediction = db_service.get_prediction(prediction_id)
        assert stored_prediction is not None
        assert stored_prediction.flight_number == "AA123"
        assert stored_prediction.delay_minutes == 25.5
    
    def test_historical_data_query(self, test_db, sample_historical_data):
        """Testa consultas de dados hist√≥ricos"""
        
        from src.services.database import DatabaseService
        
        db_service = DatabaseService(test_db)
        
        # Inserir dados hist√≥ricos (mock)
        # Em um teste real, voc√™ popularia a tabela de hist√≥rico
        
        # Testar consulta por aeroporto
        jfk_data = db_service.get_historical_data(
            origin_airport="JFK",
            start_date=datetime.now() - timedelta(days=30),
            end_date=datetime.now()
        )
        
        # Verificar estrutura dos dados
        assert isinstance(jfk_data, list)
        
        if jfk_data:  # Se houver dados
            record = jfk_data[0]
            expected_fields = [
                "flight_number", "origin_airport", 
                "destination_airport", "delay_minutes"
            ]
            
            for field in expected_fields:
                assert hasattr(record, field)
    
    def test_model_metrics_tracking(self, test_db):
        """Testa rastreamento de m√©tricas do modelo"""
        
        from src.services.database import DatabaseService
        
        db_service = DatabaseService(test_db)
        
        # Simular m√©tricas de performance
        metrics_data = {
            "model_version": "1.0.0",
            "evaluation_date": datetime.now(),
            "mae": 12.5,
            "rmse": 18.7,
            "r2_score": 0.76,
            "accuracy_threshold_15min": 0.82,
            "sample_count": 1000,
            "dataset_period": "2024-01-01_to_2024-01-31"
        }
        
        # Salvar m√©tricas
        metrics_id = db_service.save_model_metrics(metrics_data)
        assert metrics_id is not None
        
        # Recuperar m√©tricas mais recentes
        latest_metrics = db_service.get_latest_model_metrics()
        assert latest_metrics is not None
        assert latest_metrics.model_version == "1.0.0"
        assert latest_metrics.mae == 12.5

class TestExternalAPIIntegration:
    """Testes de integra√ß√£o com APIs externas"""
    
    @pytest.mark.asyncio
    async def test_weather_api_integration(self, mock_weather_service):
        """Testa integra√ß√£o com API de clima"""
        
        # Testar obten√ß√£o de clima atual
        weather = await mock_weather_service.get_current_weather("JFK")
        
        assert weather is not None
        assert "temperature" in weather
        assert "weather_score" in weather
        assert 0 <= weather["weather_score"] <= 1
        
        # Verificar que o servi√ßo foi chamado
        mock_weather_service.get_current_weather.assert_called_once_with("JFK")
    
    @pytest.mark.asyncio
    async def test_airport_api_integration(self, mock_airport_service):
        """Testa integra√ß√£o com API de aeroportos"""
        
        # Testar obten√ß√£o de informa√ß√µes do aeroporto
        airport_info = await mock_airport_service.get_airport_info("JFK")
        
        assert airport_info is not None
        assert "name" in airport_info
        assert "timezone" in airport_info
        assert "capacity_score" in airport_info
        
        # Testar n√≠vel de congestionamento
        congestion = await mock_airport_service.get_congestion_level("JFK")
        assert isinstance(congestion, float)
        assert 0 <= congestion <= 1
    
    @pytest.mark.asyncio
    async def test_api_failure_handling(self):
        """Testa tratamento de falhas em APIs externas"""
        
        from src.services.external_apis import WeatherService
        from unittest.mock import patch
        import aiohttp
        
        with patch('aiohttp.ClientSession.get') as mock_get:
            # Simular timeout
            mock_get.side_effect = asyncio.TimeoutError("API timeout")
            
            weather_service = WeatherService()
            
            # Deve retornar dados padr√£o em caso de falha
            weather = await weather_service.get_current_weather("JFK")
            
            assert weather is not None
            assert "error" in weather or "default" in weather
    
    @pytest.mark.asyncio
    async def test_api_retry_mechanism(self):
        """Testa mecanismo de retry para APIs"""
        
        from src.services.external_apis import WeatherService
        from unittest.mock import patch, AsyncMock
        
        with patch('aiohttp.ClientSession.get') as mock_get:
            # Primeira chamada falha, segunda funciona
            mock_response = AsyncMock()
            mock_response.status = 200
            mock_response.json.return_value = {"temperature": 25.0}
            
            mock_get.side_effect = [
                aiohttp.ClientConnectorError("Connection failed"),
                mock_response
            ]
            
            weather_service = WeatherService(max_retries=2)
            weather = await weather_service.get_current_weather("JFK")
            
            # Deve ter funcionado na segunda tentativa
            assert weather["temperature"] == 25.0
            assert mock_get.call_count == 2
```

## ü§ñ Testes de Pipeline ML

### üîÑ test_ml_pipeline.py

```python
# tests/integration/test_ml_pipeline.py
import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import tempfile

class TestMLPipelineIntegration:
    """Testes completos do pipeline de ML"""
    
    def test_complete_training_pipeline(
        self, 
        sample_historical_data,
        tmp_path
    ):
        """Testa pipeline completo de treinamento"""
        
        from src.ml.trainer import ModelTrainer
        from src.ml.preprocessor import DataPreprocessor
        
        # Preparar dados
        preprocessor = DataPreprocessor()
        
        # Preprocessing
        processed_data = preprocessor.fit_transform(sample_historical_data)
        assert processed_data is not None
        assert len(processed_data) > 0
        
        # Dividir dados
        X = processed_data.drop(['delay_minutes'], axis=1)
        y = processed_data['delay_minutes']
        
        # Treinar modelo
        trainer = ModelTrainer(model_save_path=tmp_path)
        
        model, metrics = trainer.train(X, y)
        
        # Verificar modelo treinado
        assert model is not None
        assert metrics is not None
        
        # Verificar m√©tricas
        assert "mae" in metrics
        assert "rmse" in metrics
        assert "r2_score" in metrics
        assert metrics["mae"] > 0
        assert metrics["rmse"] > 0
    
    def test_prediction_pipeline_end_to_end(
        self,
        sample_flight_data,
        test_model_artifacts
    ):
        """Testa pipeline completo de predi√ß√£o"""
        
        from src.ml.predictor import FlightDelayPredictor
        
        # Inicializar preditor com modelo de teste
        predictor = FlightDelayPredictor(
            model_path=test_model_artifacts["model_path"],
            scaler_path=test_model_artifacts["scaler_path"]
        )
        
        # Fazer predi√ß√£o
        prediction = predictor.predict(sample_flight_data)
        
        # Verificar estrutura da predi√ß√£o
        assert prediction is not None
        assert "delay_minutes" in prediction
        assert "probability_delayed" in prediction
        assert "confidence" in prediction
        
        # Verificar valores
        assert isinstance(prediction["delay_minutes"], (int, float))
        assert 0 <= prediction["probability_delayed"] <= 1
        assert 0 <= prediction["confidence"] <= 1
    
    def test_feature_engineering_pipeline(self, sample_historical_data):
        """Testa pipeline de engenharia de features"""
        
        from src.ml.feature_engineer import FeatureEngineer
        
        engineer = FeatureEngineer()
        
        # Aplicar engenharia de features
        enhanced_data = engineer.transform(sample_historical_data)
        
        # Verificar novas features criadas
        expected_features = [
            'hour_sin', 'hour_cos',           # Features c√≠clicas
            'day_of_week_sin', 'day_of_week_cos',
            'month_sin', 'month_cos',
            'weather_category',               # Feature categ√≥rica
            'congestion_level',               # Feature derivada
            'route_popularity',               # Feature de rota
            'airline_delay_history'           # Feature hist√≥rica
        ]
        
        for feature in expected_features:
            assert feature in enhanced_data.columns, f"Feature {feature} n√£o encontrada"
        
        # Verificar que n√£o h√° valores nulos em features cr√≠ticas
        critical_features = ['hour_sin', 'hour_cos', 'weather_category']
        for feature in critical_features:
            if feature in enhanced_data.columns:
                assert not enhanced_data[feature].isnull().any(), f"Valores nulos em {feature}"
    
    def test_model_validation_pipeline(
        self,
        sample_historical_data,
        tmp_path
    ):
        """Testa pipeline de valida√ß√£o do modelo"""
        
        from src.ml.validator import ModelValidator
        from src.ml.trainer import ModelTrainer
        
        # Treinar modelo para valida√ß√£o
        trainer = ModelTrainer(model_save_path=tmp_path)
        
        # Preparar dados
        X = sample_historical_data.drop(['delay_minutes'], axis=1)
        y = sample_historical_data['delay_minutes']
        
        # Treinar
        model, _ = trainer.train(X, y)
        
        # Validar modelo
        validator = ModelValidator()
        
        validation_results = validator.validate_model(
            model=model,
            X_test=X,
            y_test=y
        )
        
        # Verificar resultados da valida√ß√£o
        assert validation_results is not None
        assert "metrics" in validation_results
        assert "validation_passed" in validation_results
        
        metrics = validation_results["metrics"]
        assert "accuracy_by_threshold" in metrics
        assert "confusion_matrix" in metrics
        assert "classification_report" in metrics
    
    def test_data_drift_detection(self, sample_historical_data):
        """Testa detec√ß√£o de drift nos dados"""
        
        from src.ml.drift_detector import DataDriftDetector
        
        # Dividir dados em refer√™ncia e atual
        split_idx = len(sample_historical_data) // 2
        reference_data = sample_historical_data[:split_idx]
        current_data = sample_historical_data[split_idx:]
        
        # Simular drift modificando distribui√ß√£o
        current_data_with_drift = current_data.copy()
        current_data_with_drift['weather_score'] = current_data_with_drift['weather_score'] * 0.5  # Artificial drift
        
        detector = DataDriftDetector()
        
        # Detectar drift
        drift_report = detector.detect_drift(
            reference_data=reference_data,
            current_data=current_data_with_drift
        )
        
        # Verificar relat√≥rio
        assert drift_report is not None
        assert "overall_drift" in drift_report
        assert "feature_drift" in drift_report
        assert "drift_score" in drift_report
        
        # Deve detectar drift na feature weather_score
        feature_drift = drift_report["feature_drift"]
        assert "weather_score" in feature_drift
        
        weather_drift = feature_drift["weather_score"]
        assert weather_drift["has_drift"] == True
        assert weather_drift["p_value"] < 0.05  # Significativo
    
    def test_model_monitoring_pipeline(
        self,
        test_model_artifacts,
        sample_flight_data
    ):
        """Testa pipeline de monitoramento do modelo"""
        
        from src.ml.monitor import ModelMonitor
        
        monitor = ModelMonitor(
            model_path=test_model_artifacts["model_path"],
            metadata_path=test_model_artifacts["metadata_path"]
        )
        
        # Simular predi√ß√µes para monitoramento
        predictions = []
        actuals = []
        
        for i in range(10):
            flight_data = sample_flight_data.copy()
            flight_data["flight_number"] = f"TEST{i:03d}"
            
            # Fazer predi√ß√£o
            pred = monitor.predict_and_log(flight_data)
            predictions.append(pred["delay_minutes"])
            
            # Simular valor real (para teste)
            actual_delay = np.random.normal(pred["delay_minutes"], 10)
            actuals.append(max(0, actual_delay))  # Delays n√£o podem ser negativos
            
            # Registrar resultado real
            monitor.log_actual_result(
                flight_id=flight_data["flight_number"],
                actual_delay=actual_delay
            )
        
        # Gerar relat√≥rio de performance
        performance_report = monitor.generate_performance_report()
        
        assert performance_report is not None
        assert "current_mae" in performance_report
        assert "current_rmse" in performance_report
        assert "prediction_count" in performance_report
        assert "drift_alerts" in performance_report
        
        # Verificar alertas se houver degrada√ß√£o significativa
        if performance_report["current_mae"] > 20:  # Threshold de alerta
            assert len(performance_report["drift_alerts"]) > 0
    
    def test_automated_retraining_trigger(
        self,
        sample_historical_data,
        tmp_path
    ):
        """Testa trigger autom√°tico de retreinamento"""
        
        from src.ml.retraining import AutoRetrainingService
        
        service = AutoRetrainingService(
            model_dir=tmp_path,
            performance_threshold=0.15,  # MAE threshold
            data_drift_threshold=0.1
        )
        
        # Simular degrada√ß√£o de performance
        mock_current_performance = {
            "mae": 25.0,  # Acima do threshold
            "rmse": 35.0,
            "prediction_count": 1000
        }
        
        # Verificar se retreinamento √© necess√°rio
        should_retrain = service.should_trigger_retraining(
            current_performance=mock_current_performance,
            baseline_mae=15.0  # Performance baseline
        )
        
        assert should_retrain == True
        
        # Testar retreinamento autom√°tico
        if should_retrain:
            retrain_results = service.execute_retraining(
                training_data=sample_historical_data
            )
            
            assert retrain_results is not None
            assert "new_model_path" in retrain_results
            assert "performance_improvement" in retrain_results
            assert "retrain_timestamp" in retrain_results

class TestEndToEndWorkflow:
    """Testes de fluxo completo end-to-end"""
    
    @pytest.mark.asyncio
    async def test_complete_prediction_workflow(
        self,
        test_app,
        sample_flight_data,
        mock_weather_service,
        mock_airport_service,
        test_model_artifacts
    ):
        """Testa fluxo completo desde requisi√ß√£o at√© resposta"""
        
        with pytest.MonkeyPatch.context() as mp:
            mp.setattr("src.ml.model_loader.MODEL_PATH", test_model_artifacts["model_dir"])
            
            # 1. Fazer requisi√ß√£o de predi√ß√£o
            start_time = datetime.now()
            
            response = test_app.post(
                "/api/v1/predict",
                json=sample_flight_data
            )
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            # 2. Verificar resposta
            assert response.status_code == 200
            result = response.json()
            
            # 3. Verificar que todos os servi√ßos foram chamados
            mock_weather_service.get_current_weather.assert_called()
            mock_airport_service.get_airport_info.assert_called()
            
            # 4. Verificar performance (deve ser < 5 segundos)
            assert processing_time < 5.0
            
            # 5. Verificar estrutura completa da resposta
            assert "prediction" in result
            assert "confidence" in result
            assert "metadata" in result
            assert "external_data" in result
            
            external_data = result["external_data"]
            assert "weather" in external_data
            assert "airport_info" in external_data
    
    def test_batch_processing_workflow(
        self,
        test_app,
        sample_flight_data,
        test_model_artifacts
    ):
        """Testa processamento em lote completo"""
        
        # Criar lote de voos
        batch_size = 20
        flights = []
        
        for i in range(batch_size):
            flight = sample_flight_data.copy()
            flight["flight_number"] = f"BATCH{i:03d}"
            flight["scheduled_departure"] = (
                datetime.fromisoformat(flight["scheduled_departure"]) + 
                timedelta(hours=i)
            ).isoformat()
            flights.append(flight)
        
        with pytest.MonkeyPatch.context() as mp:
            mp.setattr("src.ml.model_loader.MODEL_PATH", test_model_artifacts["model_dir"])
            
            # Processar lote
            response = test_app.post(
                "/api/v1/predict/batch",
                json={"flights": flights}
            )
            
            assert response.status_code == 200
            
            result = response.json()
            predictions = result["predictions"]
            
            # Verificar que todas as predi√ß√µes foram processadas
            assert len(predictions) == batch_size
            
            # Verificar que n√£o h√° falhas
            failed_predictions = [p for p in predictions if p["status"] == "error"]
            assert len(failed_predictions) == 0
            
            # Verificar tempo de processamento por item
            total_time = result["metadata"]["total_processing_time_ms"]
            avg_time_per_item = total_time / batch_size
            
            # Deve processar cada item em menos de 500ms
            assert avg_time_per_item < 500
```

## üß™ Execu√ß√£o dos Testes

### üöÄ Comandos de Execu√ß√£o

```bash
# Executar apenas testes de integra√ß√£o
pytest tests/integration/ -v

# Executar com cobertura espec√≠fica
pytest tests/integration/ --cov=src --cov-report=html

# Executar testes espec√≠ficos
pytest tests/integration/test_api_integration.py::TestAPIIntegration::test_predict_endpoint_complete_flow -v

# Executar testes de integra√ß√£o com logs detalhados
pytest tests/integration/ -v -s --log-cli-level=INFO

# Executar testes paralelos (requer pytest-xdist)
pytest tests/integration/ -n auto

# Executar apenas testes marcados como 'slow'
pytest tests/integration/ -m "integration and not slow"
```

### ‚öôÔ∏è Configura√ß√£o de Ambiente para Testes

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  test-db:
    image: postgres:15
    environment:
      POSTGRES_DB: test_flight_delays
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_pass
    ports:
      - "5433:5432"
    volumes:
      - test_db_data:/var/lib/postgresql/data
  
  test-redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
  
  test-api:
    build:
      context: .
      dockerfile: docker/api.Dockerfile
    environment:
      - DATABASE_URL=postgresql://test_user:test_pass@test-db:5432/test_flight_delays
      - REDIS_URL=redis://test-redis:6379/0
      - ENVIRONMENT=test
    depends_on:
      - test-db
      - test-redis
    ports:
      - "8001:8000"

volumes:
  test_db_data:
```

### üîß Makefile para Testes

```makefile
# Makefile (adi√ß√µes para integra√ß√£o)
.PHONY: test-integration test-e2e test-all-integration

test-integration:
	@echo "üîó Executando testes de integra√ß√£o..."
	pytest tests/integration/ -v --cov=src --cov-report=term-missing

test-e2e:
	@echo "üåê Executando testes end-to-end..."
	docker-compose -f docker-compose.test.yml up -d
	sleep 10
	pytest tests/e2e/ -v
	docker-compose -f docker-compose.test.yml down

test-all-integration: test-integration test-e2e
	@echo "‚úÖ Todos os testes de integra√ß√£o conclu√≠dos!"

setup-test-env:
	@echo "‚öôÔ∏è Configurando ambiente de teste..."
	docker-compose -f docker-compose.test.yml up -d test-db test-redis
	sleep 5

cleanup-test-env:
	@echo "üßπ Limpando ambiente de teste..."
	docker-compose -f docker-compose.test.yml down -v
```

## üìä Relat√≥rios de Integra√ß√£o

### üìã Exemplo de Relat√≥rio de Teste

```
================== RELAT√ìRIO DE TESTES DE INTEGRA√á√ÉO ==================

üîó Testes de API Completos:
  ‚úÖ test_predict_endpoint_complete_flow         PASSED  (2.45s)
  ‚úÖ test_batch_prediction_endpoint             PASSED  (1.87s)
  ‚úÖ test_model_info_endpoint                   PASSED  (0.32s)
  ‚úÖ test_health_check_complete                 PASSED  (0.89s)
  ‚úÖ test_error_handling_invalid_data           PASSED  (0.21s)
  ‚úÖ test_rate_limiting                         PASSED  (3.12s)

üóÑÔ∏è Testes de Integra√ß√£o Database:
  ‚úÖ test_flight_prediction_storage             PASSED  (0.67s)
  ‚úÖ test_historical_data_query                 PASSED  (0.45s)
  ‚úÖ test_model_metrics_tracking                PASSED  (0.38s)

üåê Testes de APIs Externas:
  ‚úÖ test_weather_api_integration               PASSED  (0.28s)
  ‚úÖ test_airport_api_integration               PASSED  (0.19s)
  ‚úÖ test_api_failure_handling                  PASSED  (1.15s)
  ‚úÖ test_api_retry_mechanism                   PASSED  (2.34s)

ü§ñ Testes de Pipeline ML:
  ‚úÖ test_complete_training_pipeline            PASSED  (8.67s)
  ‚úÖ test_prediction_pipeline_end_to_end        PASSED  (1.23s)
  ‚úÖ test_feature_engineering_pipeline          PASSED  (2.11s)
  ‚úÖ test_model_validation_pipeline             PASSED  (3.45s)
  ‚úÖ test_data_drift_detection                  PASSED  (1.89s)
  ‚úÖ test_model_monitoring_pipeline             PASSED  (2.76s)
  ‚úÖ test_automated_retraining_trigger          PASSED  (4.32s)

üåê Testes End-to-End:
  ‚úÖ test_complete_prediction_workflow          PASSED  (4.21s)
  ‚úÖ test_batch_processing_workflow             PASSED  (6.78s)

üìä RESUMO:
  Total de Testes: 18
  Passou: 18 (100%)
  Falhou: 0 (0%)
  Tempo Total: 42.58s
  Cobertura: 89.4%

‚úÖ TODOS OS TESTES DE INTEGRA√á√ÉO PASSARAM!
```

## üîó Pr√≥ximos Passos

1. **[üìä Cobertura](coverage.md)** - An√°lise detalhada de cobertura
2. **[üß™ Testes](running-tests.md)** - Executar suite completa
3. **[üìì Notebooks](../notebooks/eda.md)** - An√°lise explorat√≥ria

---

## üìû Refer√™ncias

- üß™ **[pytest-asyncio](https://pytest-asyncio.readthedocs.io/)** - Testes async
- üåê **[TestClient](https://fastapi.tiangolo.com/tutorial/testing/)** - Testes FastAPI
- üóÑÔ∏è **[SQLAlchemy Testing](https://docs.sqlalchemy.org/en/14/orm/session_transaction.html#joining-a-session-into-an-external-transaction-such-as-for-test-suites)** - Testes com banco