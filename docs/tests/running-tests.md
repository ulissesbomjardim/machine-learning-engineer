# ğŸ§ª Executando Testes

Guia completo para executar e entender os testes automatizados do projeto Machine Learning Engineer Challenge.

## ğŸ“‹ VisÃ£o Geral dos Testes

O projeto possui uma suÃ­te abrangente de testes automatizados com **100+ testes** cobrindo todas as camadas da aplicaÃ§Ã£o:

- âš¡ **Testes de API** - Endpoints FastAPI
- ğŸ¤– **Testes de ML** - Pipeline de Machine Learning  
- ğŸ—„ï¸ **Testes de ServiÃ§os** - Camada de dados
- ğŸ”„ **Testes de IntegraÃ§Ã£o** - Fluxos completos
- ğŸ› ï¸ **Testes de UtilitÃ¡rios** - FunÃ§Ãµes auxiliares

## ğŸš€ Executando Testes

### âš¡ Comando RÃ¡pido

```bash
# Com ambiente Poetry ativo
task test

# Ou sem ativar ambiente
poetry run task test

# ExecuÃ§Ã£o direta com pytest
poetry run pytest
```

### ğŸ“Š Com RelatÃ³rio de Cobertura

```bash
# Testes com coverage
task test-cov

# Ou comando completo
poetry run pytest --cov=src --cov-report=term-missing --cov-report=html

# Visualizar relatÃ³rio HTML
open htmlcov/index.html  # macOS
start htmlcov/index.html  # Windows
xdg-open htmlcov/index.html  # Linux
```

### ğŸ¯ ExecuÃ§Ã£o Seletiva

```bash
# Executar arquivo especÃ­fico
pytest tests/test_routers.py -v

# Executar classe especÃ­fica
pytest tests/test_ml_pipeline.py::TestModelTraining -v

# Executar teste especÃ­fico
pytest tests/test_routers.py::TestAPIMain::test_health_endpoint -v

# Executar por marcaÃ§Ã£o
pytest -m "not slow" -v

# Executar testes que contÃ©m palavra
pytest -k "test_predict" -v
```

## ğŸ“Š Estrutura dos Testes

### ğŸ—‚ï¸ OrganizaÃ§Ã£o dos Arquivos

```
tests/
â”œâ”€â”€ ğŸ§ª conftest.py              # ConfiguraÃ§Ãµes e fixtures globais
â”œâ”€â”€ âš¡ test_routers.py          # Testes dos endpoints da API
â”œâ”€â”€ ğŸ¤– test_ml_pipeline.py      # Testes do pipeline de ML
â”œâ”€â”€ ğŸ”„ test_integration.py      # Testes de integraÃ§Ã£o end-to-end
â”œâ”€â”€ ğŸ—„ï¸ test_services.py         # Testes da camada de serviÃ§os
â”œâ”€â”€ ğŸ› ï¸ test_utils.py            # Testes de utilitÃ¡rios
â””â”€â”€ ğŸ“‹ run_tests.py            # Script de execuÃ§Ã£o personalizado
```

### ğŸ“ˆ Cobertura Atual

```
======================== Coverage Report ========================
Name                           Stmts   Miss  Cover   Missing
------------------------------------------------------------
src/routers/main.py               45      3    93%   
src/routers/model/predict.py      68      5    93%   
src/routers/model/load.py         42      2    95%   
src/routers/model/history.py      35      1    97%   
src/services/database.py          58      8    86%   
------------------------------------------------------------
TOTAL                            248     19    92%
```

## âš¡ Testes da API (test_routers.py)

### ğŸ¯ Cobertura dos Endpoints

| **Classe de Teste** | **Endpoints Testados** | **CenÃ¡rios** |
|---------------------|------------------------|--------------|
| `TestAPIMain` | `/`, `/health`, `/docs` | Status codes, response format |
| `TestPredictEndpoint` | `/model/predict` | PrediÃ§Ã£o Ãºnica, batch, validaÃ§Ã£o |
| `TestModelLoader` | `/model/load/*` | Carregamento, upload, erros |
| `TestHistoryEndpoint` | `/model/history/` | PaginaÃ§Ã£o, filtros, estatÃ­sticas |
| `TestAPIErrorHandling` | Todos | CÃ³digos de erro, mensagens |

**Exemplo de execuÃ§Ã£o:**
```bash
# Executar apenas testes da API
pytest tests/test_routers.py -v

# SaÃ­da esperada:
tests/test_routers.py::TestAPIMain::test_root_endpoint PASSED
tests/test_routers.py::TestAPIMain::test_health_endpoint PASSED
tests/test_routers.py::TestPredictEndpoint::test_predict_success PASSED
tests/test_routers.py::TestPredictEndpoint::test_predict_batch PASSED
...
======================== 25 passed in 8.45s ========================
```

### ğŸ’¡ Casos de Teste Principais

**Health Check:**
```python
def test_health_endpoint(api_client):
    response = api_client.get("/health")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] in ["healthy", "unhealthy"]
    assert "timestamp" in data
    assert "components" in data
```

**PrediÃ§Ã£o com ValidaÃ§Ã£o:**
```python
def test_predict_with_validation(api_client, sample_flight_data):
    response = api_client.post("/model/predict", json=sample_flight_data)
    assert response.status_code == 200
    
    data = response.json()
    assert "prediction" in data
    assert "probability" in data["prediction"]
    assert 0 <= data["prediction"]["probability"] <= 1
```

## ğŸ¤– Testes de ML (test_ml_pipeline.py)

### ğŸ§  Cobertura do Pipeline

| **Classe de Teste** | **Componente Testado** | **ValidaÃ§Ãµes** |
|---------------------|------------------------|----------------|
| `TestDataProcessing` | Preprocessamento | Limpeza, transformaÃ§Ãµes |
| `TestFeatureEngineering` | Feature engineering | CriaÃ§Ã£o de features |
| `TestModelTraining` | Treinamento | Algoritmos, hiperparÃ¢metros |
| `TestModelEvaluation` | AvaliaÃ§Ã£o | MÃ©tricas, validaÃ§Ã£o cruzada |
| `TestModelPersistence` | PersistÃªncia | Salvamento, carregamento |

**Exemplo de execuÃ§Ã£o:**
```bash
# Executar testes de ML
pytest tests/test_ml_pipeline.py -v --tb=short

# Com logs de ML
pytest tests/test_ml_pipeline.py -v -s
```

### ğŸ“Š Testes de Qualidade do Modelo

```python
def test_model_accuracy_threshold():
    """Garante que o modelo atende critÃ©rio mÃ­nimo de qualidade"""
    model = load_trained_model()
    X_test, y_test = load_test_data()
    
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    # CritÃ©rio de qualidade: mÃ­nimo 85% de accuracy
    assert accuracy >= 0.85, f"Accuracy {accuracy:.2f} abaixo do mÃ­nimo 0.85"
```

## ğŸ”„ Testes de IntegraÃ§Ã£o (test_integration.py)

### ğŸŒ Fluxos End-to-End

```python
def test_complete_prediction_workflow(api_client):
    """Testa fluxo completo: carregar modelo â†’ prediÃ§Ã£o â†’ histÃ³rico"""
    
    # 1. Carregar modelo
    load_response = api_client.get("/model/load/default")
    assert load_response.status_code == 200
    
    # 2. Fazer prediÃ§Ã£o
    predict_response = api_client.post("/model/predict", json=sample_data)
    assert predict_response.status_code == 200
    
    prediction_id = predict_response.json()["prediction_id"]
    
    # 3. Verificar no histÃ³rico
    history_response = api_client.get("/model/history/")
    assert history_response.status_code == 200
    
    history_data = history_response.json()
    prediction_ids = [p["prediction_id"] for p in history_data["predictions"]]
    assert prediction_id in prediction_ids
```

### ğŸ¯ Testes de Performance

```python
@pytest.mark.performance
def test_prediction_performance():
    """Garante que prediÃ§Ãµes sÃ£o executadas em tempo aceitÃ¡vel"""
    import time
    
    start_time = time.time()
    
    # Fazer mÃºltiplas prediÃ§Ãµes
    for _ in range(100):
        response = api_client.post("/model/predict", json=sample_data)
        assert response.status_code == 200
    
    elapsed_time = time.time() - start_time
    
    # CritÃ©rio: mÃ¡ximo 10ms por prediÃ§Ã£o em mÃ©dia
    assert elapsed_time / 100 < 0.01, f"Performance inadequada: {elapsed_time/100:.3f}s por prediÃ§Ã£o"
```

## ğŸ—„ï¸ Testes de ServiÃ§os (test_services.py)

### ğŸ’¾ Testes de Database

```python
def test_database_connection():
    """Testa conexÃ£o com banco de dados"""
    from src.services.database import get_database
    
    db = get_database()
    assert db is not None
    
    # Teste de inserÃ§Ã£o
    test_doc = {"test": "data", "timestamp": datetime.now()}
    result = db.predictions.insert_one(test_doc)
    assert result.inserted_id is not None
    
    # Limpeza
    db.predictions.delete_one({"_id": result.inserted_id})
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o dos Testes

### ğŸ“‹ Fixtures Principais (conftest.py)

```python
@pytest.fixture
def api_client():
    """Cliente de teste para a API"""
    from fastapi.testclient import TestClient
    from src.routers.main import app
    
    return TestClient(app)

@pytest.fixture
def sample_flight_data():
    """Dados de exemplo para testes"""
    return {
        "features": {
            "airline": "American Airlines",
            "flight_number": "AA123",
            "departure_airport": "JFK",
            "arrival_airport": "LAX",
            "scheduled_departure": "2024-01-15T10:00:00",
            "scheduled_arrival": "2024-01-15T14:00:00"
        }
    }

@pytest.fixture
def mock_model():
    """Modelo mock para testes sem dependÃªncias"""
    class MockModel:
        def predict(self, X):
            return [0] * len(X)  # Sempre nÃ£o cancelado
        def predict_proba(self, X):
            return [[0.8, 0.2]] * len(X)
    
    return MockModel()
```

### âš™ï¸ ConfiguraÃ§Ã£o pytest.ini

```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --strict-markers
    --tb=short
    --cov=src
    --cov-report=term-missing
    --cov-fail-under=85

markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    unit: marks tests as unit tests
    performance: marks tests as performance tests
```

## ğŸ“Š RelatÃ³rios de Cobertura

### ğŸ¯ Coverage HTML

```bash
# Gerar relatÃ³rio HTML completo
poetry run pytest --cov=src --cov-report=html --cov-report=term

# Estrutura do relatÃ³rio gerado:
htmlcov/
â”œâ”€â”€ index.html              # PÃ¡gina principal
â”œâ”€â”€ src_routers_main_py.html   # Coverage por arquivo
â””â”€â”€ ...                     # Outros arquivos
```

### ğŸ“‹ Coverage XML (CI/CD)

```bash
# Para integraÃ§Ã£o com CI/CD
poetry run pytest --cov=src --cov-report=xml

# Gera: coverage.xml
```

## ğŸš¨ Debugging de Testes

### ğŸ” ExecuÃ§Ã£o com Debug

```bash
# Modo verbose com traceback completo
pytest -vvv --tb=long

# Parar no primeiro erro
pytest -x

# Executar com pdb (debugger)
pytest --pdb

# Mostrar prints durante execuÃ§Ã£o
pytest -s

# Executar apenas testes que falharam na Ãºltima execuÃ§Ã£o
pytest --lf
```

### ğŸ“‹ Logs Detalhados

```python
import logging

def test_with_logging(caplog):
    """Teste que captura logs"""
    with caplog.at_level(logging.INFO):
        # CÃ³digo que gera logs
        pass
    
    assert "Expected log message" in caplog.text
```

## ğŸ¯ MarcaÃ§Ãµes de Teste

### ğŸ·ï¸ Usando Markers

```python
import pytest

@pytest.mark.slow
def test_expensive_operation():
    """Teste que demora para executar"""
    pass

@pytest.mark.integration
def test_full_workflow():
    """Teste de integraÃ§Ã£o"""
    pass

@pytest.mark.parametrize("input,expected", [
    ("JFK", "New York"),
    ("LAX", "Los Angeles"),
])
def test_airport_codes(input, expected):
    """Teste parametrizado"""
    pass
```

**Executar por marcaÃ§Ã£o:**
```bash
# Apenas testes rÃ¡pidos
pytest -m "not slow"

# Apenas testes de integraÃ§Ã£o
pytest -m integration

# CombinaÃ§Ãµes
pytest -m "integration and not slow"
```

## ğŸ”„ CI/CD Integration

### ğŸš€ GitHub Actions

```yaml
# .github/workflows/tests.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.12
        
    - name: Install Poetry
      run: pip install poetry
      
    - name: Install dependencies
      run: poetry install
      
    - name: Run tests
      run: poetry run pytest --cov=src --cov-report=xml
      
    - name: Upload coverage
      uses: codecov/codecov-action@v1
```

## ğŸ“š Melhores PrÃ¡ticas

### âœ… PrincÃ­pios de Bons Testes

1. **ğŸ¯ AAA Pattern**: Arrange, Act, Assert
2. **ğŸ”¬ Isolamento**: Testes independentes
3. **ğŸ“‹ Nomenclatura clara**: Nomes descritivos
4. **âš¡ Velocidade**: Testes rÃ¡pidos
5. **ğŸ” Determinismo**: Resultados consistentes

### ğŸš¨ Evitar

- âŒ **Testes interdependentes**
- âŒ **Hard-coded values** sem contexto
- âŒ **Testes muito longos**
- âŒ **MÃºltiplas assertivas nÃ£o relacionadas**
- âŒ **Dados de teste nÃ£o realistas**

## ğŸ“ Suporte

### ğŸ› Problemas com Testes

- ğŸ”§ [Troubleshooting](../dev/troubleshooting.md)
- ğŸ“– [Coverage Detalhado](coverage.md)
- ğŸ”„ [Testes de IntegraÃ§Ã£o](integration.md)
- ğŸ› [Issues](https://github.com/ulissesbomjardim/machine_learning_engineer/issues)