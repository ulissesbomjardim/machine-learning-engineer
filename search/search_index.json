{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Machine Learning Engineer Challenge","text":""},{"location":"#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Este reposit\u00f3rio cont\u00e9m a solu\u00e7\u00e3o completa para o Case T\u00e9cnico de Machine Learning Engineer, implementando uma arquitetura profissional para predi\u00e7\u00e3o de atrasos de voos usando FastAPI, Machine Learning e Docker.</p>"},{"location":"#destaques-do-projeto","title":"\ud83c\udf1f Destaques do Projeto","text":"<pre><code>graph TB\n    A[\ud83d\udcca An\u00e1lise de Dados] --&gt; B[\ud83e\udd16 Machine Learning]\n    B --&gt; C[\u26a1 API FastAPI]\n    C --&gt; D[\ud83d\udc33 Docker/Docker Compose]\n    D --&gt; E[\ud83e\uddea Testes Automatizados]\n    E --&gt; F[\ud83d\udcd6 Documenta\u00e7\u00e3o MkDocs]\n\n    style A fill:#e1f5fe\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9</code></pre>"},{"location":"#componentes-do-sistema","title":"\ud83c\udfaf Componentes do Sistema","text":"Componente Descri\u00e7\u00e3o Tecnologias \ud83d\udccaEDA An\u00e1lise explorat\u00f3ria <code>Pandas</code>, <code>Jupyter Notebooks</code> \ud83e\udd16ML Pipeline Pipeline de ML <code>Scikit-learn</code>, <code>Feature Eng.</code> \u26a1API REST Servi\u00e7o de predi\u00e7\u00e3o <code>FastAPI</code>, <code>Pydantic</code> \ud83d\udc33Containeriza\u00e7\u00e3o Deploy com containers <code>Docker</code>, <code>Docker Compose</code> \ud83e\uddeaTestes Testes unit\u00e1rios <code>Pytest</code>, <code>Coverage</code> \ud83d\udcd6Documenta\u00e7\u00e3o Docs t\u00e9cnicas <code>MkDocs</code>, <code>Material Theme</code>"},{"location":"#comecando-rapidamente","title":"\ud83d\ude80 Come\u00e7ando Rapidamente","text":""},{"location":"#pre-requisitos","title":"\ud83d\udccb Pr\u00e9-requisitos","text":"<ul> <li>\ud83d\udc0d Python 3.12.7</li> <li>\ud83d\udce6 Poetry (gerenciador de depend\u00eancias)</li> <li>\ud83d\udc33 Docker e Docker Compose (opcional)</li> <li>\ud83d\udcbb Git para versionamento</li> </ul>"},{"location":"#instalacao-rapida","title":"\u26a1 Instala\u00e7\u00e3o R\u00e1pida","text":"<pre><code># 1. Clone o reposit\u00f3rio\ngit clone https://github.com/ulissesbomjardim/machine_learning_engineer.git\ncd machine_learning_engineer\n\n# 2. Configure Python e Poetry\npoetry env use 3.12.7\npoetry install\n\n# 3. Ative o ambiente virtual\npoetry shell\n\n# 4. Execute os testes\ntask test\n\n# 5. Inicie a API\nuvicorn src.routers.main:app --reload\n</code></pre> <p>API Rodando</p> <p>\ud83c\udf10 API: http://localhost:8000 \ud83d\udcdaSwagger: http://localhost:8000/docs \ud83d\udcd6Documenta\u00e7\u00e3o: <code>mkdocs serve</code></p>"},{"location":"#estrutura-do-projeto","title":"\ud83d\udcc1 Estrutura do Projeto","text":"<pre><code>machine-learning-engineer/\n\u251c\u2500\u2500 \ud83d\udcbb src/\n\u2502   \u251c\u2500\u2500 routers/          # \ud83d\udd17 Endpoints da API\n\u2502   \u2502   \u251c\u2500\u2500 main.py       # \ud83d\ude80 App principal\n\u2502   \u2502   \u2514\u2500\u2500 model/        # \ud83e\udd16 Rotas ML\n\u2502   \u2514\u2500\u2500 services/         # \u2699\ufe0f Servi\u00e7os\n\u251c\u2500\u2500 \ud83e\uddea tests/            # Testes unit\u00e1rios\n\u251c\u2500\u2500 \ud83d\udcca data/             # Datasets\n\u251c\u2500\u2500 \ud83d\uddc4\ufe0f model/           # Modelos treinados\n\u251c\u2500\u2500 \ud83d\udcd3 notebook/         # Jupyter Notebooks\n\u251c\u2500\u2500 \ud83d\udcd6 docs/             # Documenta\u00e7\u00e3o MkDocs\n\u251c\u2500\u2500 \ud83d\udc33 docker/           # Configura\u00e7\u00f5es Docker\n\u2514\u2500\u2500 \u2699\ufe0f pyproject.toml    # Depend\u00eancias Poetry\n</code></pre>"},{"location":"#sobre-o-challenge","title":"\ud83e\udd16 Sobre o Challenge","text":""},{"location":"#objetivo-principal","title":"\ud83c\udfaf Objetivo Principal","text":"<p>Desenvolver um sistema completo de predi\u00e7\u00e3o de atrasos de voos que inclui:</p> <ul> <li>\ud83d\udcca An\u00e1lise explorat\u00f3ria dos dados hist\u00f3ricos</li> <li>\ud83e\udd16 Modelo de Machine Learning para classifica\u00e7\u00e3o</li> <li>\u26a1 API REST para predi\u00e7\u00f5es em tempo real</li> <li>\ud83d\udc33 Deploy containerizado para produ\u00e7\u00e3o</li> <li>\ud83e\uddea Testes automatizados para garantir qualidade</li> </ul>"},{"location":"#funcionalidades-implementadas","title":"\ud83d\udccb Funcionalidades Implementadas","text":"Endpoint M\u00e9todo Descri\u00e7\u00e3o <code>/</code> GET Informa\u00e7\u00f5es da API <code>/health</code> GET Status do sistema <code>/docs</code> GET Documenta\u00e7\u00e3o Swagger <code>/model/predict</code> POST Predi\u00e7\u00e3o de atraso <code>/model/load</code> GET/POST Carregamento de modelos <code>/model/history</code> GET Hist\u00f3rico de predi\u00e7\u00f5es"},{"location":"#pipeline-de-machine-learning","title":"\ud83e\udde0 Pipeline de Machine Learning","text":"<pre><code>graph LR\n    A[\ud83d\udcca Dados Brutos] --&gt; B[\ud83d\udd27 Preprocessing]\n    B --&gt; C[\u2699\ufe0f Feature Engineering]\n    C --&gt; D[\ud83e\udd16 Treinamento]\n    D --&gt; E[\ud83d\udcc8 Avalia\u00e7\u00e3o]\n    E --&gt; F[\ud83d\udcbe Persist\u00eancia]\n    F --&gt; G[\u26a1 API Deployment]\n\n    style A fill:#e3f2fd\n    style B fill:#f3e5f5\n    style C fill:#e8f5e8\n    style D fill:#fff3e0\n    style E fill:#fce4ec\n    style F fill:#f1f8e9\n    style G fill:#e1f5fe</code></pre>"},{"location":"#navegacao-da-documentacao","title":"\ud83d\udcda Navega\u00e7\u00e3o da Documenta\u00e7\u00e3o","text":""},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<ul> <li>\ud83d\udce5 Instala\u00e7\u00e3o - Como instalar depend\u00eancias</li> <li>\u2699\ufe0f Setup - Configura\u00e7\u00e3o do ambiente</li> <li>\ud83c\udfc3 Executando - Como rodar o projeto</li> </ul>"},{"location":"#arquitetura","title":"\ud83c\udfd7\ufe0f Arquitetura","text":"<ul> <li>\ud83d\udccb Vis\u00e3o Geral - Arquitetura do sistema</li> <li>\ud83e\udde9 Componentes - Detalhes dos m\u00f3dulos</li> <li>\ud83e\udd16 Pipeline ML - Fluxo de Machine Learning</li> </ul>"},{"location":"#machine-learning","title":"\ud83e\udd16 Machine Learning","text":"<ul> <li>\ud83d\udcca An\u00e1lise de Dados - EDA e insights</li> <li>\ud83d\udd27 Preprocessing - Limpeza e transforma\u00e7\u00e3o</li> <li>\ud83c\udfaf Treinamento - Algoritmos e hiperpar\u00e2metros</li> <li>\ud83d\udcc8 Avalia\u00e7\u00e3o - M\u00e9tricas e valida\u00e7\u00e3o</li> </ul>"},{"location":"#api-reference","title":"\u26a1 API Reference","text":"<ul> <li>\ud83d\udd17 Endpoints - Documenta\u00e7\u00e3o completa da API</li> <li>\ud83d\udccb Modelos - Schemas Pydantic</li> <li>\ud83d\udca1 Exemplos - Casos de uso pr\u00e1ticos</li> </ul>"},{"location":"#docker","title":"\ud83d\udc33 Docker","text":"<ul> <li>\u2699\ufe0f Setup - Configura\u00e7\u00e3o Docker</li> <li>\ud83d\udd27 Compose - Docker Compose</li> <li>\ud83d\ude80 Deploy - Deploy em produ\u00e7\u00e3o</li> </ul>"},{"location":"#testes","title":"\ud83e\uddea Testes","text":"<ul> <li>\ud83c\udfc3 Executando - Como rodar testes</li> <li>\ud83d\udcca Coverage - Cobertura de c\u00f3digo</li> <li>\ud83d\udd04 Integra\u00e7\u00e3o - Testes de integra\u00e7\u00e3o</li> </ul>"},{"location":"#notebooks","title":"\ud83d\udcd3 Notebooks","text":"<ul> <li>\ud83d\udcca EDA - An\u00e1lise explorat\u00f3ria</li> <li>\ud83e\udd16 Modelagem - Desenvolvimento do modelo</li> <li>\ud83e\uddea Experimentos - Experimentos e otimiza\u00e7\u00e3o</li> </ul>"},{"location":"#comandos-uteis","title":"\ud83d\udee0\ufe0f Comandos \u00dateis","text":""},{"location":"#poetry","title":"\ud83d\udce6 Poetry","text":"<pre><code>poetry install          # Instalar depend\u00eancias\npoetry shell           # Ativar ambiente virtual\npoetry add &lt;package&gt;   # Adicionar pacote\n</code></pre>"},{"location":"#testes_1","title":"\ud83e\uddea Testes","text":"<pre><code>task test              # Executar todos os testes\ntask test-cov         # Testes com coverage\npytest -v             # Testes verbose\n</code></pre>"},{"location":"#api","title":"\u26a1 API","text":"<pre><code>uvicorn src.routers.main:app --reload  # Desenvolvimento\nuvicorn src.routers.main:app --host 0.0.0.0 --port 8000  # Produ\u00e7\u00e3o\n</code></pre>"},{"location":"#docker_1","title":"\ud83d\udc33 Docker","text":"<pre><code>docker-compose up --build    # Build e executar\ndocker-compose up           # Executar containers\ndocker-compose down         # Parar containers\n</code></pre>"},{"location":"#documentacao","title":"\ud83d\udcd6 Documenta\u00e7\u00e3o","text":"<pre><code>mkdocs serve           # Servidor local\nmkdocs build          # Build est\u00e1tico\nmkdocs gh-deploy      # Deploy GitHub Pages\n</code></pre>"},{"location":"#contribuindo","title":"\ud83e\udd1d Contribuindo","text":"<p>Este projeto segue as melhores pr\u00e1ticas de desenvolvimento:</p> <ul> <li>\ud83c\udfa8 Formata\u00e7\u00e3o: <code>black</code> e <code>isort</code></li> <li>\ud83e\uddea Testes: <code>pytest</code> com cobertura</li> <li>\ud83d\udccb Linting: <code>ruff</code> para qualidade do c\u00f3digo</li> <li>\ud83d\udcd6 Docs: <code>mkdocs</code> para documenta\u00e7\u00e3o</li> </ul> <p>Para contribuir:</p> <ol> <li>Fork o reposit\u00f3rio</li> <li>Crie uma branch para sua feature</li> <li>Execute os testes: <code>task test</code></li> <li>Fa\u00e7a commit das mudan\u00e7as</li> <li>Abra um Pull Request</li> </ol>"},{"location":"#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udce7 Email: ulisses.bomjardim@gmail.com</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udca1 Discuss\u00f5es: GitHub Discussions</li> </ul>   **\ud83c\udf89 Explore a documenta\u00e7\u00e3o para descobrir todos os recursos! \ud83c\udf89**  *\ud83d\udcc5 \u00daltima atualiza\u00e7\u00e3o: 2024-12-21*"},{"location":"api/endpoints/","title":"\ud83d\udd17 API Endpoints","text":"<p>Documenta\u00e7\u00e3o completa de todos os endpoints dispon\u00edveis na API do Machine Learning Engineer Challenge.</p>"},{"location":"api/endpoints/#visao-geral-da-api","title":"\ud83d\udccb Vis\u00e3o Geral da API","text":"<p>A API foi desenvolvida com FastAPI seguindo os princ\u00edpios REST e fornece endpoints para predi\u00e7\u00e3o de cancelamento de voos, gerenciamento de modelos e consulta de hist\u00f3rico.</p>"},{"location":"api/endpoints/#base-url","title":"\ud83c\udf10 Base URL","text":"<pre><code>http://localhost:8000\n</code></pre>"},{"location":"api/endpoints/#caracteristicas","title":"\ud83d\udcca Caracter\u00edsticas","text":"<ul> <li>\u2705 REST API padr\u00e3o</li> <li>\ud83d\udcda Documenta\u00e7\u00e3o autom\u00e1tica (Swagger/OpenAPI)</li> <li>\ud83d\udd12 Valida\u00e7\u00e3o autom\u00e1tica com Pydantic</li> <li>\u26a1 Responses ass\u00edncronos </li> <li>\ud83d\udea8 Error handling padronizado</li> <li>\ud83d\udcc8 Health monitoring</li> </ul>"},{"location":"api/endpoints/#endpoints-gerais","title":"\ud83c\udfe0 Endpoints Gerais","text":""},{"location":"api/endpoints/#get-informacoes-da-api","title":"GET / - Informa\u00e7\u00f5es da API","text":"<p>Retorna informa\u00e7\u00f5es b\u00e1sicas sobre a API.</p> <p>Request: <pre><code>curl -X GET \"http://localhost:8000/\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Flight Delay Prediction API\",\n  \"version\": \"1.0.0\",\n  \"status\": \"running\",\n  \"endpoints\": {\n    \"health\": \"/health\",\n    \"docs\": \"/docs\",\n    \"predict\": \"/model/predict\",\n    \"load_model\": \"/model/load/default\",\n    \"history\": \"/model/history/\"\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#get-health-health-check","title":"GET /health - Health Check","text":"<p>Verifica o status de sa\u00fade da API e componentes.</p> <p>Request: <pre><code>curl -X GET \"http://localhost:8000/health\"\n</code></pre></p> <p>Response Saud\u00e1vel: <pre><code>{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2024-12-21T10:00:00Z\",\n  \"version\": \"1.0.0\",\n  \"components\": {\n    \"api\": \"healthy\",\n    \"database\": \"healthy\",\n    \"model\": \"loaded\"\n  },\n  \"uptime\": \"2h 30m 15s\"\n}\n</code></pre></p> <p>Response com Problemas: <pre><code>{\n  \"status\": \"unhealthy\",\n  \"timestamp\": \"2024-12-21T10:00:00Z\",\n  \"version\": \"1.0.0\",\n  \"components\": {\n    \"api\": \"healthy\",\n    \"database\": \"disconnected\",\n    \"model\": \"not_loaded\"\n  },\n  \"errors\": [\n    \"Database connection failed\",\n    \"Model not loaded\"\n  ]\n}\n</code></pre></p>"},{"location":"api/endpoints/#endpoints-de-machine-learning","title":"\ud83e\udd16 Endpoints de Machine Learning","text":""},{"location":"api/endpoints/#post-modelpredict-predicao-de-cancelamento","title":"POST /model/predict - Predi\u00e7\u00e3o de Cancelamento","text":"<p>Realiza predi\u00e7\u00e3o de cancelamento de voo baseada nos dados fornecidos.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/model/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"features\": {\n         \"airline\": \"American Airlines\",\n         \"flight_number\": \"AA123\",\n         \"departure_airport\": \"JFK\",\n         \"arrival_airport\": \"LAX\",\n         \"scheduled_departure\": \"2024-01-15T10:00:00\",\n         \"scheduled_arrival\": \"2024-01-15T14:00:00\",\n         \"aircraft_type\": \"Boeing 737\",\n         \"weather_condition\": \"Clear\"\n       }\n     }'\n</code></pre></p> <p>Response: <pre><code>{\n  \"prediction\": {\n    \"cancelled\": false,\n    \"probability\": 0.15,\n    \"confidence\": \"high\"\n  },\n  \"features_processed\": {\n    \"airline_encoded\": 1,\n    \"departure_hour\": 10,\n    \"flight_duration_minutes\": 360,\n    \"route_popularity\": 0.85\n  },\n  \"model_info\": {\n    \"name\": \"decision_tree_v1\",\n    \"version\": \"1.0.0\",\n    \"accuracy\": 0.94\n  },\n  \"prediction_id\": \"pred_20241221_100000_abc123\",\n  \"timestamp\": \"2024-12-21T10:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#post-modelpredict-batch","title":"POST /model/predict (Batch)","text":"<p>Predi\u00e7\u00e3o em lote para m\u00faltiplos voos.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/model/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"batch\": [\n         {\n           \"features\": {\n             \"airline\": \"American Airlines\",\n             \"flight_number\": \"AA123\",\n             \"departure_airport\": \"JFK\",\n             \"arrival_airport\": \"LAX\"\n           }\n         },\n         {\n           \"features\": {\n             \"airline\": \"Delta\",\n             \"flight_number\": \"DL456\",\n             \"departure_airport\": \"ATL\",\n             \"arrival_airport\": \"SEA\"\n           }\n         }\n       ]\n     }'\n</code></pre></p> <p>Response: <pre><code>{\n  \"batch_predictions\": [\n    {\n      \"prediction\": {\n        \"cancelled\": false,\n        \"probability\": 0.15\n      },\n      \"prediction_id\": \"pred_20241221_100001_abc123\"\n    },\n    {\n      \"prediction\": {\n        \"cancelled\": true,\n        \"probability\": 0.78\n      },\n      \"prediction_id\": \"pred_20241221_100002_def456\"\n    }\n  ],\n  \"batch_summary\": {\n    \"total_predictions\": 2,\n    \"cancelled_count\": 1,\n    \"average_probability\": 0.465\n  },\n  \"batch_id\": \"batch_20241221_100000_xyz789\",\n  \"timestamp\": \"2024-12-21T10:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#endpoints-de-gerenciamento-de-modelos","title":"\ud83d\udce5 Endpoints de Gerenciamento de Modelos","text":""},{"location":"api/endpoints/#get-modelloaddefault-carregar-modelo-padrao","title":"GET /model/load/default - Carregar Modelo Padr\u00e3o","text":"<p>Carrega o modelo padr\u00e3o pr\u00e9-treinado.</p> <p>Request: <pre><code>curl -X GET \"http://localhost:8000/model/load/default\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Default model loaded successfully\",\n  \"model_info\": {\n    \"name\": \"decision_tree_v1\",\n    \"version\": \"1.0.0\",\n    \"file_path\": \"./model/modelo_arvore_decisao.pkl\",\n    \"file_size_mb\": 24.5,\n    \"accuracy\": 0.94,\n    \"features\": [\n      \"airline\",\n      \"departure_airport\",\n      \"arrival_airport\",\n      \"scheduled_departure\",\n      \"aircraft_type\"\n    ]\n  },\n  \"loaded_at\": \"2024-12-21T10:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#post-modelload-upload-de-modelo","title":"POST /model/load/ - Upload de Modelo","text":"<p>Upload de um novo modelo via arquivo.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/model/load/\" \\\n     -F \"model_file=@model.pkl\" \\\n     -F \"model_name=custom_model_v2\" \\\n     -F \"version=2.0.0\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Model uploaded and loaded successfully\",\n  \"model_info\": {\n    \"name\": \"custom_model_v2\",\n    \"version\": \"2.0.0\",\n    \"file_size_mb\": 18.2,\n    \"upload_id\": \"upload_20241221_100000_xyz123\"\n  },\n  \"validation\": {\n    \"format_valid\": true,\n    \"features_compatible\": true,\n    \"test_prediction_successful\": true\n  },\n  \"loaded_at\": \"2024-12-21T10:00:00Z\"\n}\n</code></pre></p>"},{"location":"api/endpoints/#endpoints-de-historico","title":"\ud83d\udcca Endpoints de Hist\u00f3rico","text":""},{"location":"api/endpoints/#get-modelhistory-consultar-historico","title":"GET /model/history/ - Consultar Hist\u00f3rico","text":"<p>Consulta hist\u00f3rico de predi\u00e7\u00f5es com filtros e pagina\u00e7\u00e3o.</p> <p>Par\u00e2metros de Query:</p> Par\u00e2metro Tipo Descri\u00e7\u00e3o Padr\u00e3o <code>limit</code> int N\u00famero m\u00e1ximo de registros 10 <code>offset</code> int N\u00famero de registros para pular 0 <code>start_date</code> datetime Data inicial (ISO format) - <code>end_date</code> datetime Data final (ISO format) - <code>cancelled_only</code> bool Filtrar apenas cancelados false <code>airline</code> str Filtrar por companhia a\u00e9rea - <p>Request: <pre><code>curl -X GET \"http://localhost:8000/model/history/?limit=5&amp;cancelled_only=true\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"predictions\": [\n    {\n      \"prediction_id\": \"pred_20241221_095900_abc123\",\n      \"timestamp\": \"2024-12-21T09:59:00Z\",\n      \"input_features\": {\n        \"airline\": \"American Airlines\",\n        \"flight_number\": \"AA123\",\n        \"departure_airport\": \"JFK\",\n        \"arrival_airport\": \"LAX\"\n      },\n      \"prediction_result\": {\n        \"cancelled\": true,\n        \"probability\": 0.85,\n        \"confidence\": \"high\"\n      },\n      \"model_used\": \"decision_tree_v1\"\n    }\n  ],\n  \"pagination\": {\n    \"limit\": 5,\n    \"offset\": 0,\n    \"total_records\": 150,\n    \"has_next\": true,\n    \"has_previous\": false\n  },\n  \"filters_applied\": {\n    \"cancelled_only\": true\n  },\n  \"summary_stats\": {\n    \"total_predictions\": 150,\n    \"cancelled_predictions\": 45,\n    \"cancellation_rate\": 0.30\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#get-modelhistorystats-estatisticas-do-historico","title":"GET /model/history/stats - Estat\u00edsticas do Hist\u00f3rico","text":"<p>Estat\u00edsticas agregadas do hist\u00f3rico de predi\u00e7\u00f5es.</p> <p>Request: <pre><code>curl -X GET \"http://localhost:8000/model/history/stats\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"overall_stats\": {\n    \"total_predictions\": 1500,\n    \"total_cancelled\": 450,\n    \"cancellation_rate\": 0.30,\n    \"average_confidence\": 0.78\n  },\n  \"time_series\": {\n    \"predictions_by_day\": {\n      \"2024-12-20\": 150,\n      \"2024-12-21\": 125\n    },\n    \"cancellation_rate_by_day\": {\n      \"2024-12-20\": 0.28,\n      \"2024-12-21\": 0.32\n    }\n  },\n  \"breakdown\": {\n    \"by_airline\": {\n      \"American Airlines\": {\"total\": 400, \"cancelled\": 120},\n      \"Delta\": {\"total\": 350, \"cancelled\": 95}\n    },\n    \"by_hour\": {\n      \"06:00-09:00\": {\"total\": 300, \"cancelled\": 60},\n      \"09:00-12:00\": {\"total\": 400, \"cancelled\": 100}\n    }\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#error-responses","title":"\ud83d\udea8 Error Responses","text":""},{"location":"api/endpoints/#codigos-de-status-http","title":"C\u00f3digos de Status HTTP","text":"C\u00f3digo Descri\u00e7\u00e3o Quando Ocorre 200 Success Request executado com sucesso 400 Bad Request Dados de entrada inv\u00e1lidos 404 Not Found Recurso n\u00e3o encontrado 422 Validation Error Falha na valida\u00e7\u00e3o Pydantic 500 Internal Server Error Erro interno do servidor"},{"location":"api/endpoints/#formato-de-erro-padronizado","title":"Formato de Erro Padronizado","text":"<pre><code>{\n  \"error\": {\n    \"code\": \"VALIDATION_ERROR\",\n    \"message\": \"Input validation failed\",\n    \"details\": [\n      {\n        \"field\": \"scheduled_departure\",\n        \"message\": \"Invalid datetime format\",\n        \"input_value\": \"invalid-date\"\n      }\n    ],\n    \"timestamp\": \"2024-12-21T10:00:00Z\",\n    \"request_id\": \"req_20241221_100000_xyz123\"\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#exemplos-de-erros-comuns","title":"Exemplos de Erros Comuns","text":"<p>Dados inv\u00e1lidos (400): <pre><code>{\n  \"error\": {\n    \"code\": \"INVALID_INPUT\",\n    \"message\": \"Required field missing\",\n    \"details\": \"Field 'airline' is required\"\n  }\n}\n</code></pre></p> <p>Modelo n\u00e3o carregado (500): <pre><code>{\n  \"error\": {\n    \"code\": \"MODEL_NOT_LOADED\",\n    \"message\": \"No model is currently loaded\",\n    \"details\": \"Load a model using /model/load/default or /model/load/\"\n  }\n}\n</code></pre></p>"},{"location":"api/endpoints/#schemas-de-dados","title":"\ud83d\udd12 Schemas de Dados","text":""},{"location":"api/endpoints/#flightpredictionrequest","title":"FlightPredictionRequest","text":"<pre><code>{\n  \"features\": {\n    \"airline\": \"string\",\n    \"flight_number\": \"string\",\n    \"departure_airport\": \"string (IATA code)\",\n    \"arrival_airport\": \"string (IATA code)\",\n    \"scheduled_departure\": \"datetime (ISO format)\",\n    \"scheduled_arrival\": \"datetime (ISO format)\",\n    \"aircraft_type\": \"string (optional)\",\n    \"weather_condition\": \"string (optional)\"\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#predictionresponse","title":"PredictionResponse","text":"<pre><code>{\n  \"prediction\": {\n    \"cancelled\": \"boolean\",\n    \"probability\": \"float (0-1)\",\n    \"confidence\": \"string (low/medium/high)\"\n  },\n  \"prediction_id\": \"string\",\n  \"timestamp\": \"datetime (ISO format)\",\n  \"model_info\": {\n    \"name\": \"string\",\n    \"version\": \"string\",\n    \"accuracy\": \"float (0-1)\"\n  }\n}\n</code></pre>"},{"location":"api/endpoints/#testando-a-api","title":"\ud83e\uddea Testando a API","text":""},{"location":"api/endpoints/#ferramentas-recomendadas","title":"\ud83d\udd27 Ferramentas Recomendadas","text":"<ul> <li>Swagger UI: <code>http://localhost:8000/docs</code></li> <li>ReDoc: <code>http://localhost:8000/redoc</code></li> <li>Postman: Importar OpenAPI spec</li> <li>curl: Comandos via terminal</li> <li>httpx: Cliente Python para testes</li> </ul>"},{"location":"api/endpoints/#scripts-de-teste","title":"\ud83d\ude80 Scripts de Teste","text":"<p>Teste b\u00e1sico de funcionamento: <pre><code>#!/bin/bash\n# test_api_basic.sh\n\necho \"Testing API endpoints...\"\n\n# Health check\necho \"1. Health check:\"\ncurl -s http://localhost:8000/health | jq '.'\n\n# Load default model\necho \"2. Loading default model:\"\ncurl -s http://localhost:8000/model/load/default | jq '.'\n\n# Make prediction\necho \"3. Making prediction:\"\ncurl -s -X POST http://localhost:8000/model/predict \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"features\": {\n      \"airline\": \"American Airlines\",\n      \"flight_number\": \"AA123\",\n      \"departure_airport\": \"JFK\",\n      \"arrival_airport\": \"LAX\",\n      \"scheduled_departure\": \"2024-01-15T10:00:00\",\n      \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n    }\n  }' | jq '.'\n\necho \"API test completed!\"\n</code></pre></p>"},{"location":"api/endpoints/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li>\ud83d\udccb Modelos de Dados - Schemas Pydantic detalhados</li> <li>\ud83d\udca1 Exemplos Pr\u00e1ticos - Casos de uso completos</li> <li>\ud83e\uddea Testes da API - Testes automatizados</li> <li>\ud83c\udfd7\ufe0f Arquitetura - Design da API</li> </ul>"},{"location":"api/endpoints/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udcda Swagger UI: <code>http://localhost:8000/docs</code></li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udce7 Email: ulisses.bomjardim@gmail.com</li> </ul>"},{"location":"api/examples/","title":"\ud83d\udccb Exemplos Pr\u00e1ticos da API","text":"<p>Exemplos pr\u00e1ticos de uso da API de predi\u00e7\u00e3o de atrasos de voos, incluindo casos de uso reais, c\u00f3digo de exemplo e integra\u00e7\u00e3o com diferentes linguagens de programa\u00e7\u00e3o.</p>"},{"location":"api/examples/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Esta se\u00e7\u00e3o fornece exemplos concretos de como utilizar a API de predi\u00e7\u00e3o de atrasos, desde chamadas simples at\u00e9 integra\u00e7\u00f5es complexas em sistemas de produ\u00e7\u00e3o.</p>"},{"location":"api/examples/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"api/examples/#1-primeiro-request","title":"1. \ud83d\udce1 Primeiro Request","text":"<pre><code># Teste b\u00e1sico da API\ncurl -X GET \"http://localhost:8000/health\" \\\n  -H \"Content-Type: application/json\"\n\n# Resposta esperada\n{\n  \"status\": \"healthy\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"version\": \"1.0.0\"\n}\n</code></pre>"},{"location":"api/examples/#2-predicao-simples","title":"2. \ud83c\udfaf Predi\u00e7\u00e3o Simples","text":"<pre><code>curl -X POST \"http://localhost:8000/predict\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"airline\": \"TAM\",\n    \"departure_airport\": \"SBGR\",\n    \"arrival_airport\": \"SBRJ\",\n    \"departure_time\": \"2024-01-15T14:30:00\",\n    \"aircraft_type\": \"A320\",\n    \"weather_conditions\": \"partly_cloudy\",\n    \"temperature\": 25.5,\n    \"wind_speed\": 15,\n    \"visibility\": 8.0\n  }'\n</code></pre>"},{"location":"api/examples/#exemplos-por-linguagem","title":"\ud83d\udcbb Exemplos por Linguagem","text":""},{"location":"api/examples/#python","title":"\ud83d\udc0d Python","text":""},{"location":"api/examples/#exemplo-basico","title":"Exemplo B\u00e1sico","text":"<pre><code>import requests\nimport json\nfrom datetime import datetime\n\n# Configura\u00e7\u00e3o da API\nAPI_BASE_URL = \"http://localhost:8000\"\nheaders = {\"Content-Type\": \"application/json\"}\n\ndef predict_flight_delay(flight_data):\n    \"\"\"Prediz atraso de voo usando a API\"\"\"\n\n    url = f\"{API_BASE_URL}/predict\"\n\n    try:\n        response = requests.post(url, json=flight_data, headers=headers)\n        response.raise_for_status()\n\n        result = response.json()\n        return result\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Erro na requisi\u00e7\u00e3o: {e}\")\n        return None\n\n# Exemplo de uso\nflight_data = {\n    \"airline\": \"GOL\",\n    \"departure_airport\": \"SBSP\",\n    \"arrival_airport\": \"SBGL\",\n    \"departure_time\": \"2024-01-15T18:45:00\",\n    \"aircraft_type\": \"B737\",\n    \"weather_conditions\": \"rain\", \n    \"temperature\": 22.0,\n    \"wind_speed\": 25,\n    \"visibility\": 3.0\n}\n\nresult = predict_flight_delay(flight_data)\n\nif result:\n    print(f\"Probabilidade de atraso: {result['delay_probability']:.2%}\")\n    print(f\"Predi\u00e7\u00e3o: {'ATRASO' if result['is_delayed'] else 'PONTUAL'}\")\n    print(f\"Confian\u00e7a: {result['confidence']:.2f}\")\n</code></pre>"},{"location":"api/examples/#cliente-python-avancado","title":"Cliente Python Avan\u00e7ado","text":"<pre><code>import asyncio\nimport aiohttp\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\n\n@dataclass\nclass FlightPredictionResult:\n    is_delayed: bool\n    delay_probability: float\n    confidence: float\n    predicted_delay_minutes: Optional[int]\n    factors: Dict[str, float]\n    timestamp: str\n\nclass FlightDelayAPIClient:\n    \"\"\"Cliente avan\u00e7ado para API de predi\u00e7\u00e3o de atrasos\"\"\"\n\n    def __init__(self, base_url: str = \"http://localhost:8000\"):\n        self.base_url = base_url\n        self.session = None\n\n    async def __aenter__(self):\n        self.session = aiohttp.ClientSession()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            await self.session.close()\n\n    async def health_check(self) -&gt; Dict:\n        \"\"\"Verifica sa\u00fade da API\"\"\"\n        async with self.session.get(f\"{self.base_url}/health\") as response:\n            return await response.json()\n\n    async def predict_single(self, flight_data: Dict) -&gt; FlightPredictionResult:\n        \"\"\"Predi\u00e7\u00e3o para um voo\"\"\"\n        async with self.session.post(\n            f\"{self.base_url}/predict\", \n            json=flight_data\n        ) as response:\n            data = await response.json()\n\n            return FlightPredictionResult(\n                is_delayed=data['is_delayed'],\n                delay_probability=data['delay_probability'],\n                confidence=data['confidence'],\n                predicted_delay_minutes=data.get('predicted_delay_minutes'),\n                factors=data.get('factors', {}),\n                timestamp=data['timestamp']\n            )\n\n    async def predict_batch(self, flights: List[Dict]) -&gt; List[FlightPredictionResult]:\n        \"\"\"Predi\u00e7\u00e3o em lote\"\"\"\n        async with self.session.post(\n            f\"{self.base_url}/predict/batch\",\n            json={\"flights\": flights}\n        ) as response:\n            data = await response.json()\n\n            return [\n                FlightPredictionResult(**result) \n                for result in data['predictions']\n            ]\n\n# Exemplo de uso ass\u00edncrono\nasync def main():\n    flights = [\n        {\n            \"airline\": \"TAM\",\n            \"departure_airport\": \"SBGR\",\n            \"arrival_airport\": \"SBGL\",\n            \"departure_time\": \"2024-01-15T08:30:00\",\n            \"aircraft_type\": \"A320\",\n            \"weather_conditions\": \"clear\",\n            \"temperature\": 28.0,\n            \"wind_speed\": 10,\n            \"visibility\": 10.0\n        },\n        {\n            \"airline\": \"GOL\", \n            \"departure_airport\": \"SBSP\",\n            \"arrival_airport\": \"SBRJ\",\n            \"departure_time\": \"2024-01-15T19:15:00\",\n            \"aircraft_type\": \"B737\",\n            \"weather_conditions\": \"thunderstorm\",\n            \"temperature\": 24.0,\n            \"wind_speed\": 40,\n            \"visibility\": 1.5\n        }\n    ]\n\n    async with FlightDelayAPIClient() as client:\n        # Verificar sa\u00fade\n        health = await client.health_check()\n        print(f\"API Status: {health['status']}\")\n\n        # Predi\u00e7\u00e3o em lote\n        results = await client.predict_batch(flights)\n\n        for i, result in enumerate(results):\n            print(f\"\\nVoo {i+1}:\")\n            print(f\"  Atraso previsto: {'SIM' if result.is_delayed else 'N\u00c3O'}\")\n            print(f\"  Probabilidade: {result.delay_probability:.2%}\")\n            print(f\"  Confian\u00e7a: {result.confidence:.2f}\")\n\n# Executar\n# asyncio.run(main())\n</code></pre>"},{"location":"api/examples/#javascriptnodejs","title":"\ud83d\udfe8 JavaScript/Node.js","text":""},{"location":"api/examples/#exemplo-basico_1","title":"Exemplo B\u00e1sico","text":"<pre><code>const axios = require('axios');\n\nconst API_BASE_URL = 'http://localhost:8000';\n\nclass FlightDelayPredictor {\n  constructor(baseUrl = API_BASE_URL) {\n    this.baseUrl = baseUrl;\n    this.client = axios.create({\n      baseURL: baseUrl,\n      headers: {\n        'Content-Type': 'application/json'\n      }\n    });\n  }\n\n  async healthCheck() {\n    try {\n      const response = await this.client.get('/health');\n      return response.data;\n    } catch (error) {\n      console.error('Health check failed:', error.message);\n      throw error;\n    }\n  }\n\n  async predictDelay(flightData) {\n    try {\n      const response = await this.client.post('/predict', flightData);\n      return response.data;\n    } catch (error) {\n      console.error('Prediction failed:', error.message);\n      throw error;\n    }\n  }\n\n  async predictBatch(flights) {\n    try {\n      const response = await this.client.post('/predict/batch', { flights });\n      return response.data;\n    } catch (error) {\n      console.error('Batch prediction failed:', error.message);\n      throw error;\n    }\n  }\n}\n\n// Exemplo de uso\nasync function main() {\n  const predictor = new FlightDelayPredictor();\n\n  try {\n    // Verificar sa\u00fade da API\n    const health = await predictor.healthCheck();\n    console.log('API Status:', health.status);\n\n    // Dados do voo\n    const flightData = {\n      airline: 'AZUL',\n      departure_airport: 'SBCF',\n      arrival_airport: 'SBGR',\n      departure_time: '2024-01-15T16:20:00',\n      aircraft_type: 'A320',\n      weather_conditions: 'cloudy',\n      temperature: 20.5,\n      wind_speed: 18,\n      visibility: 6.0\n    };\n\n    // Fazer predi\u00e7\u00e3o\n    const result = await predictor.predictDelay(flightData);\n\n    console.log('\\n=== RESULTADO DA PREDI\u00c7\u00c3O ===');\n    console.log(`Voo: ${flightData.departure_airport} \u2192 ${flightData.arrival_airport}`);\n    console.log(`Companhia: ${flightData.airline}`);\n    console.log(`Predi\u00e7\u00e3o: ${result.is_delayed ? 'ATRASO' : 'PONTUAL'}`);\n    console.log(`Probabilidade: ${(result.delay_probability * 100).toFixed(1)}%`);\n    console.log(`Confian\u00e7a: ${result.confidence.toFixed(2)}`);\n\n  } catch (error) {\n    console.error('Erro:', error.message);\n  }\n}\n\n// Executar\n// main();\n</code></pre>"},{"location":"api/examples/#react-component","title":"React Component","text":"<pre><code>import React, { useState, useEffect } from 'react';\nimport axios from 'axios';\n\nconst FlightDelayPredictor = () =&gt; {\n  const [flightData, setFlightData] = useState({\n    airline: '',\n    departure_airport: '',\n    arrival_airport: '',\n    departure_time: '',\n    aircraft_type: '',\n    weather_conditions: 'clear',\n    temperature: 25,\n    wind_speed: 10,\n    visibility: 10\n  });\n\n  const [prediction, setPrediction] = useState(null);\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState(null);\n\n  const handleInputChange = (e) =&gt; {\n    const { name, value } = e.target;\n    setFlightData(prev =&gt; ({\n      ...prev,\n      [name]: value\n    }));\n  };\n\n  const handleSubmit = async (e) =&gt; {\n    e.preventDefault();\n    setLoading(true);\n    setError(null);\n\n    try {\n      const response = await axios.post(\n        'http://localhost:8000/predict',\n        flightData\n      );\n      setPrediction(response.data);\n    } catch (err) {\n      setError(err.response?.data?.detail || 'Erro na predi\u00e7\u00e3o');\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return (\n    &lt;div className=\"flight-predictor\"&gt;\n      &lt;h2&gt;Predi\u00e7\u00e3o de Atrasos de Voo&lt;/h2&gt;\n\n      &lt;form onSubmit={handleSubmit}&gt;\n        &lt;div className=\"form-group\"&gt;\n          &lt;label&gt;Companhia A\u00e9rea:&lt;/label&gt;\n          &lt;select \n            name=\"airline\" \n            value={flightData.airline}\n            onChange={handleInputChange}\n            required\n          &gt;\n            &lt;option value=\"\"&gt;Selecione...&lt;/option&gt;\n            &lt;option value=\"TAM\"&gt;TAM&lt;/option&gt;\n            &lt;option value=\"GOL\"&gt;GOL&lt;/option&gt;\n            &lt;option value=\"AZUL\"&gt;Azul&lt;/option&gt;\n            &lt;option value=\"LATAM\"&gt;LATAM&lt;/option&gt;\n          &lt;/select&gt;\n        &lt;/div&gt;\n\n        &lt;div className=\"form-group\"&gt;\n          &lt;label&gt;Aeroporto de Origem:&lt;/label&gt;\n          &lt;select \n            name=\"departure_airport\"\n            value={flightData.departure_airport}\n            onChange={handleInputChange}\n            required\n          &gt;\n            &lt;option value=\"\"&gt;Selecione...&lt;/option&gt;\n            &lt;option value=\"SBGR\"&gt;S\u00e3o Paulo (Guarulhos)&lt;/option&gt;\n            &lt;option value=\"SBSP\"&gt;S\u00e3o Paulo (Congonhas)&lt;/option&gt;\n            &lt;option value=\"SBRJ\"&gt;Rio de Janeiro (Santos Dumont)&lt;/option&gt;\n            &lt;option value=\"SBGL\"&gt;Rio de Janeiro (Gale\u00e3o)&lt;/option&gt;\n          &lt;/select&gt;\n        &lt;/div&gt;\n\n        &lt;div className=\"form-group\"&gt;\n          &lt;label&gt;Data/Hora da Partida:&lt;/label&gt;\n          &lt;input\n            type=\"datetime-local\"\n            name=\"departure_time\"\n            value={flightData.departure_time}\n            onChange={handleInputChange}\n            required\n          /&gt;\n        &lt;/div&gt;\n\n        &lt;button type=\"submit\" disabled={loading}&gt;\n          {loading ? 'Predizendo...' : 'Prever Atraso'}\n        &lt;/button&gt;\n      &lt;/form&gt;\n\n      {error &amp;&amp; (\n        &lt;div className=\"error\"&gt;\n          Erro: {error}\n        &lt;/div&gt;\n      )}\n\n      {prediction &amp;&amp; (\n        &lt;div className=\"prediction-result\"&gt;\n          &lt;h3&gt;Resultado da Predi\u00e7\u00e3o&lt;/h3&gt;\n          &lt;div className={`prediction ${prediction.is_delayed ? 'delayed' : 'ontime'}`}&gt;\n            &lt;span className=\"status\"&gt;\n              {prediction.is_delayed ? '\u26a0\ufe0f ATRASO' : '\u2705 PONTUAL'}\n            &lt;/span&gt;\n            &lt;span className=\"probability\"&gt;\n              Probabilidade: {(prediction.delay_probability * 100).toFixed(1)}%\n            &lt;/span&gt;\n            &lt;span className=\"confidence\"&gt;\n              Confian\u00e7a: {prediction.confidence.toFixed(2)}\n            &lt;/span&gt;\n          &lt;/div&gt;\n        &lt;/div&gt;\n      )}\n    &lt;/div&gt;\n  );\n};\n\nexport default FlightDelayPredictor;\n</code></pre>"},{"location":"api/examples/#java","title":"\u2615 Java","text":"<pre><code>import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.http.HttpEntity;\nimport org.apache.http.client.methods.CloseableHttpResponse;\nimport org.apache.http.client.methods.HttpGet;\nimport org.apache.http.client.methods.HttpPost;\nimport org.apache.http.entity.StringEntity;\nimport org.apache.http.impl.client.CloseableHttpClient;\nimport org.apache.http.impl.client.HttpClients;\nimport org.apache.http.util.EntityUtils;\n\nimport java.io.IOException;\nimport java.time.LocalDateTime;\nimport java.time.format.DateTimeFormatter;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class FlightDelayAPIClient {\n\n    private final String baseUrl;\n    private final ObjectMapper objectMapper;\n    private final CloseableHttpClient httpClient;\n\n    public FlightDelayAPIClient(String baseUrl) {\n        this.baseUrl = baseUrl;\n        this.objectMapper = new ObjectMapper();\n        this.httpClient = HttpClients.createDefault();\n    }\n\n    public class FlightData {\n        public String airline;\n        public String departureAirport;\n        public String arrivalAirport;\n        public String departureTime;\n        public String aircraftType;\n        public String weatherConditions;\n        public double temperature;\n        public int windSpeed;\n        public double visibility;\n\n        // Construtor e getters/setters\n        public FlightData(String airline, String departureAirport, String arrivalAirport,\n                         LocalDateTime departureTime, String aircraftType, \n                         String weatherConditions, double temperature, \n                         int windSpeed, double visibility) {\n            this.airline = airline;\n            this.departureAirport = departureAirport;\n            this.arrivalAirport = arrivalAirport;\n            this.departureTime = departureTime.format(DateTimeFormatter.ISO_LOCAL_DATE_TIME);\n            this.aircraftType = aircraftType;\n            this.weatherConditions = weatherConditions;\n            this.temperature = temperature;\n            this.windSpeed = windSpeed;\n            this.visibility = visibility;\n        }\n    }\n\n    public class PredictionResult {\n        public boolean isDelayed;\n        public double delayProbability;\n        public double confidence;\n        public String timestamp;\n    }\n\n    public Map&lt;String, Object&gt; healthCheck() throws IOException {\n        HttpGet request = new HttpGet(baseUrl + \"/health\");\n\n        try (CloseableHttpResponse response = httpClient.execute(request)) {\n            HttpEntity entity = response.getEntity();\n            String jsonResponse = EntityUtils.toString(entity);\n\n            return objectMapper.readValue(jsonResponse, Map.class);\n        }\n    }\n\n    public PredictionResult predictDelay(FlightData flightData) throws IOException {\n        HttpPost request = new HttpPost(baseUrl + \"/predict\");\n        request.setHeader(\"Content-Type\", \"application/json\");\n\n        // Converter para JSON\n        String jsonData = objectMapper.writeValueAsString(flightData);\n        request.setEntity(new StringEntity(jsonData));\n\n        try (CloseableHttpResponse response = httpClient.execute(request)) {\n            HttpEntity entity = response.getEntity();\n            String jsonResponse = EntityUtils.toString(entity);\n\n            return objectMapper.readValue(jsonResponse, PredictionResult.class);\n        }\n    }\n\n    public static void main(String[] args) {\n        FlightDelayAPIClient client = new FlightDelayAPIClient(\"http://localhost:8000\");\n\n        try {\n            // Verificar sa\u00fade\n            Map&lt;String, Object&gt; health = client.healthCheck();\n            System.out.println(\"API Status: \" + health.get(\"status\"));\n\n            // Criar dados do voo\n            FlightData flight = new FlightData(\n                \"TAM\",\n                \"SBGR\",\n                \"SBRJ\", \n                LocalDateTime.of(2024, 1, 15, 14, 30),\n                \"A320\",\n                \"partly_cloudy\",\n                25.5,\n                15,\n                8.0\n            );\n\n            // Fazer predi\u00e7\u00e3o\n            PredictionResult result = client.predictDelay(flight);\n\n            System.out.println(\"\\n=== RESULTADO ===\");\n            System.out.println(\"Atraso previsto: \" + (result.isDelayed ? \"SIM\" : \"N\u00c3O\"));\n            System.out.println(\"Probabilidade: \" + String.format(\"%.1f%%\", result.delayProbability * 100));\n            System.out.println(\"Confian\u00e7a: \" + String.format(\"%.2f\", result.confidence));\n\n        } catch (IOException e) {\n            System.err.println(\"Erro: \" + e.getMessage());\n        }\n    }\n}\n</code></pre>"},{"location":"api/examples/#casos-de-uso-avancados","title":"\ud83d\udd04 Casos de Uso Avan\u00e7ados","text":""},{"location":"api/examples/#1-monitoramento-em-tempo-real","title":"1. \ud83c\udfaf Monitoramento em Tempo Real","text":"<pre><code>import asyncio\nimport websockets\nimport json\nfrom datetime import datetime, timedelta\n\nclass RealTimeFlightMonitor:\n    \"\"\"Monitor de voos em tempo real\"\"\"\n\n    def __init__(self, api_client):\n        self.api_client = api_client\n        self.monitored_flights = {}\n\n    async def monitor_flight(self, flight_id, flight_data, check_interval=300):\n        \"\"\"Monitora um voo espec\u00edfico\"\"\"\n\n        while datetime.now() &lt; datetime.fromisoformat(flight_data['departure_time']):\n            try:\n                # Atualizar dados meteorol\u00f3gicos (simulado)\n                updated_data = await self.update_weather_data(flight_data)\n\n                # Fazer nova predi\u00e7\u00e3o\n                prediction = await self.api_client.predict_single(updated_data)\n\n                # Verificar mudan\u00e7as significativas\n                await self.check_prediction_changes(flight_id, prediction)\n\n                # Aguardar pr\u00f3xima verifica\u00e7\u00e3o\n                await asyncio.sleep(check_interval)\n\n            except Exception as e:\n                print(f\"Erro ao monitorar voo {flight_id}: {e}\")\n                await asyncio.sleep(60)  # Retry em 1 minuto\n\n    async def update_weather_data(self, flight_data):\n        \"\"\"Atualiza dados meteorol\u00f3gicos (integra\u00e7\u00e3o com API de clima)\"\"\"\n        # Implementar integra\u00e7\u00e3o com servi\u00e7o de clima\n        return flight_data\n\n    async def check_prediction_changes(self, flight_id, new_prediction):\n        \"\"\"Verifica mudan\u00e7as na predi\u00e7\u00e3o e envia alertas\"\"\"\n\n        previous = self.monitored_flights.get(flight_id)\n\n        if previous:\n            prob_change = abs(new_prediction.delay_probability - previous.delay_probability)\n\n            if prob_change &gt; 0.2:  # Mudan\u00e7a &gt; 20%\n                await self.send_alert(flight_id, new_prediction, prob_change)\n\n        self.monitored_flights[flight_id] = new_prediction\n\n    async def send_alert(self, flight_id, prediction, change):\n        \"\"\"Envia alerta de mudan\u00e7a na predi\u00e7\u00e3o\"\"\"\n        alert = {\n            'flight_id': flight_id,\n            'new_probability': prediction.delay_probability,\n            'change': change,\n            'timestamp': datetime.now().isoformat(),\n            'message': f'Mudan\u00e7a significativa na predi\u00e7\u00e3o de atraso: {change:.1%}'\n        }\n\n        print(f\"\ud83d\udea8 ALERTA: {alert['message']}\")\n        # Implementar envio por webhook, email, etc.\n</code></pre>"},{"location":"api/examples/#2-analise-de-tendencias","title":"2. \ud83d\udcca An\u00e1lise de Tend\u00eancias","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\nclass FlightDelayAnalyzer:\n    \"\"\"Analisador de tend\u00eancias de atrasos\"\"\"\n\n    def __init__(self, api_client):\n        self.api_client = api_client\n        self.predictions_history = []\n\n    async def analyze_route_trends(self, route, days_back=30):\n        \"\"\"Analisa tend\u00eancias de uma rota espec\u00edfica\"\"\"\n\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=days_back)\n\n        # Simular dados hist\u00f3ricos da rota\n        historical_data = self.generate_historical_data(route, start_date, end_date)\n\n        # Fazer predi\u00e7\u00f5es para todos os dados\n        predictions = []\n        for data in historical_data:\n            pred = await self.api_client.predict_single(data)\n            predictions.append({\n                'date': data['departure_time'],\n                'probability': pred.delay_probability,\n                'weather': data['weather_conditions'],\n                'temperature': data['temperature']\n            })\n\n        # An\u00e1lise estat\u00edstica\n        df = pd.DataFrame(predictions)\n        df['date'] = pd.to_datetime(df['date'])\n\n        return {\n            'average_delay_prob': df['probability'].mean(),\n            'trend': self.calculate_trend(df['probability']),\n            'weather_impact': df.groupby('weather')['probability'].mean().to_dict(),\n            'daily_averages': df.groupby(df['date'].dt.date)['probability'].mean().to_dict()\n        }\n\n    def generate_historical_data(self, route, start_date, end_date):\n        \"\"\"Gera dados hist\u00f3ricos simulados\"\"\"\n        # Implementar carregamento de dados reais\n        return []\n\n    def calculate_trend(self, probabilities):\n        \"\"\"Calcula tend\u00eancia da s\u00e9rie temporal\"\"\"\n        # Regress\u00e3o linear simples\n        x = range(len(probabilities))\n        slope = np.polyfit(x, probabilities, 1)[0]\n\n        if slope &gt; 0.01:\n            return \"increasing\"\n        elif slope &lt; -0.01:\n            return \"decreasing\"\n        else:\n            return \"stable\"\n</code></pre>"},{"location":"api/examples/#integracao-com-sistemas","title":"\ud83d\udd27 Integra\u00e7\u00e3o com Sistemas","text":""},{"location":"api/examples/#1-sistema-de-reservas","title":"1. \ud83c\udfab Sistema de Reservas","text":"<pre><code>class BookingSystemIntegration:\n    \"\"\"Integra\u00e7\u00e3o com sistema de reservas\"\"\"\n\n    def __init__(self, api_client, booking_db):\n        self.api_client = api_client\n        self.booking_db = booking_db\n\n    async def predict_for_booking(self, booking_id):\n        \"\"\"Prediz atraso para uma reserva espec\u00edfica\"\"\"\n\n        # Buscar dados da reserva\n        booking = await self.booking_db.get_booking(booking_id)\n\n        if not booking:\n            raise ValueError(f\"Reserva {booking_id} n\u00e3o encontrada\")\n\n        # Converter para formato da API\n        flight_data = {\n            'airline': booking['airline'],\n            'departure_airport': booking['departure_airport'],\n            'arrival_airport': booking['arrival_airport'],\n            'departure_time': booking['departure_time'],\n            'aircraft_type': booking['aircraft_type'],\n            # Buscar dados meteorol\u00f3gicos atuais\n            **await self.get_current_weather(booking['departure_airport'])\n        }\n\n        # Fazer predi\u00e7\u00e3o\n        prediction = await self.api_client.predict_single(flight_data)\n\n        # Salvar resultado no banco\n        await self.booking_db.save_prediction(booking_id, prediction)\n\n        return prediction\n\n    async def get_current_weather(self, airport_code):\n        \"\"\"Busca condi\u00e7\u00f5es meteorol\u00f3gicas atuais\"\"\"\n        # Integrar com API meteorol\u00f3gica\n        return {\n            'weather_conditions': 'partly_cloudy',\n            'temperature': 25.0,\n            'wind_speed': 15,\n            'visibility': 8.0\n        }\n</code></pre>"},{"location":"api/examples/#2-app-mobile","title":"2. \ud83d\udcf1 App Mobile","text":"<pre><code>// Kotlin/Android Example\nimport kotlinx.coroutines.*\nimport retrofit2.*\nimport retrofit2.converter.gson.GsonConverterFactory\nimport retrofit2.http.*\n\ndata class FlightData(\n    val airline: String,\n    val departure_airport: String,\n    val arrival_airport: String,\n    val departure_time: String,\n    val aircraft_type: String,\n    val weather_conditions: String,\n    val temperature: Double,\n    val wind_speed: Int,\n    val visibility: Double\n)\n\ndata class PredictionResponse(\n    val is_delayed: Boolean,\n    val delay_probability: Double,\n    val confidence: Double,\n    val timestamp: String\n)\n\ninterface FlightDelayAPI {\n    @POST(\"predict\")\n    suspend fun predictDelay(@Body flightData: FlightData): PredictionResponse\n\n    @GET(\"health\")\n    suspend fun healthCheck(): Map&lt;String, Any&gt;\n}\n\nclass FlightDelayRepository {\n    private val api: FlightDelayAPI\n\n    init {\n        val retrofit = Retrofit.Builder()\n            .baseUrl(\"http://10.0.2.2:8000/\") // Android emulator\n            .addConverterFactory(GsonConverterFactory.create())\n            .build()\n\n        api = retrofit.create(FlightDelayAPI::class.java)\n    }\n\n    suspend fun predictFlightDelay(flightData: FlightData): Result&lt;PredictionResponse&gt; {\n        return try {\n            val prediction = api.predictDelay(flightData)\n            Result.success(prediction)\n        } catch (e: Exception) {\n            Result.failure(e)\n        }\n    }\n}\n\n// Usage in ViewModel\nclass FlightPredictionViewModel : ViewModel() {\n    private val repository = FlightDelayRepository()\n\n    fun predictDelay(flightData: FlightData) {\n        viewModelScope.launch {\n            val result = repository.predictFlightDelay(flightData)\n\n            result.onSuccess { prediction -&gt;\n                // Update UI with prediction\n                _predictionResult.value = prediction\n            }.onFailure { error -&gt;\n                // Handle error\n                _errorMessage.value = error.message\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"api/examples/#testes-de-integracao","title":"\ud83e\uddea Testes de Integra\u00e7\u00e3o","text":"<pre><code>import pytest\nimport asyncio\nfrom unittest.mock import AsyncMock, MagicMock\n\nclass TestAPIIntegration:\n    \"\"\"Testes de integra\u00e7\u00e3o da API\"\"\"\n\n    @pytest.fixture\n    async def api_client(self):\n        \"\"\"Cliente de teste\"\"\"\n        return FlightDelayAPIClient(\"http://localhost:8000\")\n\n    @pytest.mark.asyncio\n    async def test_health_endpoint(self, api_client):\n        \"\"\"Testa endpoint de sa\u00fade\"\"\"\n        async with api_client as client:\n            health = await client.health_check()\n\n            assert health['status'] == 'healthy'\n            assert 'timestamp' in health\n            assert 'version' in health\n\n    @pytest.mark.asyncio\n    async def test_single_prediction(self, api_client):\n        \"\"\"Testa predi\u00e7\u00e3o individual\"\"\"\n        flight_data = {\n            \"airline\": \"TAM\",\n            \"departure_airport\": \"SBGR\",\n            \"arrival_airport\": \"SBRJ\",\n            \"departure_time\": \"2024-01-15T14:30:00\",\n            \"aircraft_type\": \"A320\",\n            \"weather_conditions\": \"clear\",\n            \"temperature\": 25.0,\n            \"wind_speed\": 10,\n            \"visibility\": 10.0\n        }\n\n        async with api_client as client:\n            result = await client.predict_single(flight_data)\n\n            assert isinstance(result.is_delayed, bool)\n            assert 0 &lt;= result.delay_probability &lt;= 1\n            assert 0 &lt;= result.confidence &lt;= 1\n            assert result.timestamp is not None\n\n    @pytest.mark.asyncio\n    async def test_batch_prediction(self, api_client):\n        \"\"\"Testa predi\u00e7\u00e3o em lote\"\"\"\n        flights = [\n            {\n                \"airline\": \"GOL\",\n                \"departure_airport\": \"SBSP\",\n                \"arrival_airport\": \"SBGL\",\n                \"departure_time\": \"2024-01-15T08:00:00\",\n                \"aircraft_type\": \"B737\",\n                \"weather_conditions\": \"cloudy\",\n                \"temperature\": 20.0,\n                \"wind_speed\": 15,\n                \"visibility\": 5.0\n            },\n            {\n                \"airline\": \"AZUL\", \n                \"departure_airport\": \"SBCF\",\n                \"arrival_airport\": \"SBRJ\",\n                \"departure_time\": \"2024-01-15T16:30:00\", \n                \"aircraft_type\": \"E190\",\n                \"weather_conditions\": \"rain\",\n                \"temperature\": 18.0,\n                \"wind_speed\": 25,\n                \"visibility\": 2.0\n            }\n        ]\n\n        async with api_client as client:\n            results = await client.predict_batch(flights)\n\n            assert len(results) == len(flights)\n\n            for result in results:\n                assert isinstance(result.is_delayed, bool)\n                assert 0 &lt;= result.delay_probability &lt;= 1\n</code></pre>"},{"location":"api/examples/#codigos-de-erro","title":"\ud83d\udccb C\u00f3digos de Erro","text":"C\u00f3digo Descri\u00e7\u00e3o Solu\u00e7\u00e3o 400 Dados inv\u00e1lidos Verificar formato dos dados de entrada 401 N\u00e3o autorizado Verificar token de autentica\u00e7\u00e3o 404 Endpoint n\u00e3o encontrado Verificar URL da API 422 Erro de valida\u00e7\u00e3o Verificar tipos e valores dos campos 500 Erro interno Verificar logs do servidor 503 Servi\u00e7o indispon\u00edvel Tentar novamente mais tarde"},{"location":"api/examples/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udccb Endpoints - Documenta\u00e7\u00e3o completa da API</li> <li>\ud83c\udfd7\ufe0f Modelos - Schemas de dados</li> <li>\ud83e\uddea Testes - Testes de integra\u00e7\u00e3o</li> </ol>"},{"location":"api/examples/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udc1b Issues - Reportar problemas</li> <li>\ud83d\udcd6 Documenta\u00e7\u00e3o - Guia completo</li> <li>\ud83d\udd27 Troubleshooting - Solu\u00e7\u00e3o de problemas</li> </ul>"},{"location":"api/models/","title":"\ud83d\udccb Modelos de Dados (Pydantic)","text":"<p>Documenta\u00e7\u00e3o completa dos schemas Pydantic utilizados na API do projeto Machine Learning Engineer Challenge.</p>"},{"location":"api/models/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>A API utiliza Pydantic para valida\u00e7\u00e3o autom\u00e1tica, serializa\u00e7\u00e3o e documenta\u00e7\u00e3o dos dados. Todos os modelos seguem as melhores pr\u00e1ticas de tipagem e valida\u00e7\u00e3o.</p>"},{"location":"api/models/#beneficios-do-pydantic","title":"\ud83c\udfaf Benef\u00edcios do Pydantic","text":"<ul> <li>\u2705 Valida\u00e7\u00e3o autom\u00e1tica de tipos e valores</li> <li>\ud83d\udcda Documenta\u00e7\u00e3o autom\u00e1tica no Swagger</li> <li>\ud83d\udd04 Serializa\u00e7\u00e3o JSON autom\u00e1tica  </li> <li>\ud83d\udea8 Mensagens de erro claras e detalhadas</li> <li>\ud83d\udee1\ufe0f Seguran\u00e7a na valida\u00e7\u00e3o de entrada</li> </ul>"},{"location":"api/models/#modelos-de-voo-flight-models","title":"\ud83d\udee9\ufe0f Modelos de Voo (Flight Models)","text":""},{"location":"api/models/#flightfeatures","title":"FlightFeatures","text":"<p>Representa as caracter\u00edsticas de entrada de um voo para predi\u00e7\u00e3o.</p> <pre><code>from pydantic import BaseModel, Field, validator\nfrom datetime import datetime\nfrom typing import Optional\n\nclass FlightFeatures(BaseModel):\n    \"\"\"Features de entrada para predi\u00e7\u00e3o de cancelamento de voo\"\"\"\n\n    airline: str = Field(\n        ..., \n        description=\"Nome da companhia a\u00e9rea\",\n        example=\"American Airlines\",\n        min_length=1,\n        max_length=100\n    )\n\n    flight_number: str = Field(\n        ...,\n        description=\"N\u00famero do voo (formato: AA123)\",\n        example=\"AA123\",\n        regex=r'^[A-Z]{2,3}\\d{1,4}$'\n    )\n\n    departure_airport: str = Field(\n        ...,\n        description=\"C\u00f3digo IATA do aeroporto de origem\",\n        example=\"JFK\",\n        min_length=3,\n        max_length=3,\n        regex=r'^[A-Z]{3}$'\n    )\n\n    arrival_airport: str = Field(\n        ...,\n        description=\"C\u00f3digo IATA do aeroporto de destino\", \n        example=\"LAX\",\n        min_length=3,\n        max_length=3,\n        regex=r'^[A-Z]{3}$'\n    )\n\n    scheduled_departure: datetime = Field(\n        ...,\n        description=\"Data e hora prevista de partida (ISO 8601)\",\n        example=\"2024-01-15T10:00:00\"\n    )\n\n    scheduled_arrival: datetime = Field(\n        ...,\n        description=\"Data e hora prevista de chegada (ISO 8601)\",\n        example=\"2024-01-15T14:00:00\"\n    )\n\n    aircraft_type: Optional[str] = Field(\n        None,\n        description=\"Tipo de aeronave\",\n        example=\"Boeing 737-800\",\n        max_length=50\n    )\n\n    weather_condition: Optional[str] = Field(\n        None,\n        description=\"Condi\u00e7\u00e3o clim\u00e1tica no momento da partida\",\n        example=\"Clear\",\n        max_length=30\n    )\n\n    @validator('scheduled_arrival')\n    def arrival_after_departure(cls, v, values):\n        \"\"\"Valida que chegada \u00e9 ap\u00f3s partida\"\"\"\n        if 'scheduled_departure' in values and v &lt;= values['scheduled_departure']:\n            raise ValueError('Hor\u00e1rio de chegada deve ser ap\u00f3s partida')\n        return v\n\n    @validator('departure_airport', 'arrival_airport')\n    def airports_different(cls, v, values):\n        \"\"\"Valida que aeroportos s\u00e3o diferentes\"\"\"\n        if 'departure_airport' in values and v == values['departure_airport']:\n            raise ValueError('Aeroportos de origem e destino devem ser diferentes')\n        return v\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"airline\": \"American Airlines\",\n                \"flight_number\": \"AA123\", \n                \"departure_airport\": \"JFK\",\n                \"arrival_airport\": \"LAX\",\n                \"scheduled_departure\": \"2024-01-15T10:00:00\",\n                \"scheduled_arrival\": \"2024-01-15T14:00:00\",\n                \"aircraft_type\": \"Boeing 737-800\",\n                \"weather_condition\": \"Clear\"\n            }\n        }\n</code></pre>"},{"location":"api/models/#batchflightfeatures","title":"BatchFlightFeatures","text":"<p>Para predi\u00e7\u00f5es em lote de m\u00faltiplos voos.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field, validator\n\nclass BatchFlightFeatures(BaseModel):\n    \"\"\"Lista de voos para predi\u00e7\u00e3o em lote\"\"\"\n\n    flights: List[FlightFeatures] = Field(\n        ...,\n        description=\"Lista de voos para predi\u00e7\u00e3o\",\n        min_items=1,\n        max_items=100\n    )\n\n    @validator('flights')\n    def validate_batch_size(cls, v):\n        \"\"\"Valida tamanho do batch\"\"\"\n        if len(v) &gt; 100:\n            raise ValueError('M\u00e1ximo de 100 voos por batch')\n        return v\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"flights\": [\n                    {\n                        \"airline\": \"American Airlines\",\n                        \"flight_number\": \"AA123\",\n                        \"departure_airport\": \"JFK\", \n                        \"arrival_airport\": \"LAX\",\n                        \"scheduled_departure\": \"2024-01-15T10:00:00\",\n                        \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n                    },\n                    {\n                        \"airline\": \"Delta\",\n                        \"flight_number\": \"DL456\",\n                        \"departure_airport\": \"ATL\",\n                        \"arrival_airport\": \"SEA\", \n                        \"scheduled_departure\": \"2024-01-15T11:00:00\",\n                        \"scheduled_arrival\": \"2024-01-15T14:30:00\"\n                    }\n                ]\n            }\n        }\n</code></pre>"},{"location":"api/models/#modelos-de-predicao-prediction-models","title":"\ud83c\udfaf Modelos de Predi\u00e7\u00e3o (Prediction Models)","text":""},{"location":"api/models/#predictionrequest","title":"PredictionRequest","text":"<p>Encapsula request para endpoint de predi\u00e7\u00e3o.</p> <pre><code>from typing import Union\n\nclass PredictionRequest(BaseModel):\n    \"\"\"Request para predi\u00e7\u00e3o - suporta \u00fanico ou batch\"\"\"\n\n    features: Union[FlightFeatures, BatchFlightFeatures] = Field(\n        ...,\n        description=\"Features do voo ou lista de voos\"\n    )\n\n    model_version: Optional[str] = Field(\n        None,\n        description=\"Vers\u00e3o espec\u00edfica do modelo (opcional)\",\n        example=\"v1.0.0\"\n    )\n\n    include_explanation: bool = Field(\n        False,\n        description=\"Incluir explica\u00e7\u00e3o da predi\u00e7\u00e3o (SHAP values)\"\n    )\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"features\": {\n                    \"airline\": \"American Airlines\", \n                    \"flight_number\": \"AA123\",\n                    \"departure_airport\": \"JFK\",\n                    \"arrival_airport\": \"LAX\",\n                    \"scheduled_departure\": \"2024-01-15T10:00:00\",\n                    \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n                },\n                \"include_explanation\": False\n            }\n        }\n</code></pre>"},{"location":"api/models/#predictionresult","title":"PredictionResult","text":"<p>Resultado individual de uma predi\u00e7\u00e3o.</p> <pre><code>from enum import Enum\n\nclass ConfidenceLevel(str, Enum):\n    \"\"\"N\u00edveis de confian\u00e7a da predi\u00e7\u00e3o\"\"\"\n    LOW = \"low\"\n    MEDIUM = \"medium\" \n    HIGH = \"high\"\n\nclass PredictionResult(BaseModel):\n    \"\"\"Resultado de uma predi\u00e7\u00e3o individual\"\"\"\n\n    cancelled: bool = Field(\n        ...,\n        description=\"Predi\u00e7\u00e3o: voo ser\u00e1 cancelado?\"\n    )\n\n    probability: float = Field(\n        ...,\n        ge=0.0,\n        le=1.0, \n        description=\"Probabilidade de cancelamento (0-1)\"\n    )\n\n    confidence: ConfidenceLevel = Field(\n        ...,\n        description=\"N\u00edvel de confian\u00e7a na predi\u00e7\u00e3o\"\n    )\n\n    risk_factors: Optional[List[str]] = Field(\n        None,\n        description=\"Principais fatores de risco identificados\"\n    )\n\n    @validator('confidence', pre=True)\n    def calculate_confidence(cls, v, values):\n        \"\"\"Calcula confian\u00e7a baseada na probabilidade\"\"\"\n        if isinstance(v, str):\n            return v\n\n        prob = values.get('probability', 0.5)\n\n        if prob &lt;= 0.3 or prob &gt;= 0.7:\n            return ConfidenceLevel.HIGH\n        elif prob &lt;= 0.4 or prob &gt;= 0.6:\n            return ConfidenceLevel.MEDIUM\n        else:\n            return ConfidenceLevel.LOW\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"cancelled\": False,\n                \"probability\": 0.23,\n                \"confidence\": \"high\",\n                \"risk_factors\": [\"weather_condition\", \"busy_route\"]\n            }\n        }\n</code></pre>"},{"location":"api/models/#predictionresponse","title":"PredictionResponse","text":"<p>Response completa para predi\u00e7\u00f5es \u00fanicas.</p> <pre><code>from uuid import uuid4\n\nclass ModelInfo(BaseModel):\n    \"\"\"Informa\u00e7\u00f5es do modelo utilizado\"\"\"\n\n    name: str = Field(..., description=\"Nome do modelo\")\n    version: str = Field(..., description=\"Vers\u00e3o do modelo\") \n    algorithm: str = Field(..., description=\"Algoritmo utilizado\")\n    accuracy: float = Field(..., description=\"Acur\u00e1cia do modelo\")\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"name\": \"flight_cancellation_model\",\n                \"version\": \"1.0.0\", \n                \"algorithm\": \"Decision Tree\",\n                \"accuracy\": 0.94\n            }\n        }\n\nclass PredictionResponse(BaseModel):\n    \"\"\"Response completa de predi\u00e7\u00e3o \u00fanica\"\"\"\n\n    prediction: PredictionResult = Field(\n        ...,\n        description=\"Resultado da predi\u00e7\u00e3o\"\n    )\n\n    prediction_id: str = Field(\n        default_factory=lambda: f\"pred_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid4().hex[:8]}\",\n        description=\"ID \u00fanico da predi\u00e7\u00e3o\"\n    )\n\n    timestamp: datetime = Field(\n        default_factory=datetime.now,\n        description=\"Timestamp da predi\u00e7\u00e3o\"\n    )\n\n    model_info: ModelInfo = Field(\n        ...,\n        description=\"Informa\u00e7\u00f5es do modelo utilizado\"\n    )\n\n    processing_time_ms: float = Field(\n        ...,\n        description=\"Tempo de processamento em milissegundos\"\n    )\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"prediction\": {\n                    \"cancelled\": False,\n                    \"probability\": 0.23, \n                    \"confidence\": \"high\"\n                },\n                \"prediction_id\": \"pred_20241221_101500_abc12345\",\n                \"timestamp\": \"2024-12-21T10:15:00Z\",\n                \"model_info\": {\n                    \"name\": \"flight_cancellation_model\",\n                    \"version\": \"1.0.0\",\n                    \"algorithm\": \"Decision Tree\", \n                    \"accuracy\": 0.94\n                },\n                \"processing_time_ms\": 15.6\n            }\n        }\n</code></pre>"},{"location":"api/models/#batchpredictionresponse","title":"BatchPredictionResponse","text":"<p>Response para predi\u00e7\u00f5es em lote.</p> <pre><code>class BatchPredictionItem(BaseModel):\n    \"\"\"Item individual em predi\u00e7\u00e3o batch\"\"\"\n\n    prediction: PredictionResult = Field(..., description=\"Resultado da predi\u00e7\u00e3o\")\n    prediction_id: str = Field(..., description=\"ID \u00fanico desta predi\u00e7\u00e3o\")\n    input_index: int = Field(..., description=\"\u00cdndice no input batch\")\n\nclass BatchSummary(BaseModel):\n    \"\"\"Sum\u00e1rio das predi\u00e7\u00f5es em batch\"\"\"\n\n    total_predictions: int = Field(..., description=\"Total de predi\u00e7\u00f5es realizadas\")\n    cancelled_count: int = Field(..., description=\"N\u00famero de voos preditos como cancelados\")\n    cancellation_rate: float = Field(..., description=\"Taxa de cancelamento do batch\")\n    average_probability: float = Field(..., description=\"Probabilidade m\u00e9dia\")\n    processing_time_ms: float = Field(..., description=\"Tempo total de processamento\")\n\nclass BatchPredictionResponse(BaseModel):\n    \"\"\"Response para predi\u00e7\u00f5es em lote\"\"\"\n\n    predictions: List[BatchPredictionItem] = Field(\n        ...,\n        description=\"Lista de predi\u00e7\u00f5es individuais\"\n    )\n\n    batch_id: str = Field(\n        default_factory=lambda: f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid4().hex[:8]}\",\n        description=\"ID \u00fanico do batch\"\n    )\n\n    timestamp: datetime = Field(\n        default_factory=datetime.now,\n        description=\"Timestamp do processamento\"\n    )\n\n    summary: BatchSummary = Field(\n        ...,\n        description=\"Sum\u00e1rio das predi\u00e7\u00f5es\"\n    )\n\n    model_info: ModelInfo = Field(\n        ..., \n        description=\"Informa\u00e7\u00f5es do modelo utilizado\"\n    )\n</code></pre>"},{"location":"api/models/#modelos-de-upload-model-management","title":"\ud83d\udce5 Modelos de Upload (Model Management)","text":""},{"location":"api/models/#modeluploadrequest","title":"ModelUploadRequest","text":"<p>Para upload de novos modelos.</p> <pre><code>from fastapi import UploadFile\nfrom pydantic import BaseModel, Field\n\nclass ModelMetadata(BaseModel):\n    \"\"\"Metadados do modelo\"\"\"\n\n    name: str = Field(..., description=\"Nome do modelo\", max_length=50)\n    version: str = Field(..., description=\"Vers\u00e3o (semver)\", regex=r'^\\d+\\.\\d+\\.\\d+$')\n    algorithm: str = Field(..., description=\"Algoritmo utilizado\", max_length=30)\n    description: Optional[str] = Field(None, description=\"Descri\u00e7\u00e3o\", max_length=200)\n\n    # M\u00e9tricas de performance\n    accuracy: Optional[float] = Field(None, ge=0, le=1, description=\"Acur\u00e1cia\")\n    precision: Optional[float] = Field(None, ge=0, le=1, description=\"Precis\u00e3o\")\n    recall: Optional[float] = Field(None, ge=0, le=1, description=\"Recall\") \n    f1_score: Optional[float] = Field(None, ge=0, le=1, description=\"F1 Score\")\n\n    # Features esperadas\n    expected_features: List[str] = Field(\n        ...,\n        description=\"Lista de features que o modelo espera\"\n    )\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"name\": \"flight_cancellation_v2\",\n                \"version\": \"2.0.0\",\n                \"algorithm\": \"Random Forest\", \n                \"description\": \"Modelo melhorado com mais features\",\n                \"accuracy\": 0.96,\n                \"precision\": 0.91,\n                \"recall\": 0.89,\n                \"f1_score\": 0.90,\n                \"expected_features\": [\n                    \"airline_encoded\", \"hour_departure\", \"day_of_week\",\n                    \"flight_duration\", \"weather_encoded\"\n                ]\n            }\n        }\n\nclass ModelUploadResponse(BaseModel):\n    \"\"\"Response do upload de modelo\"\"\"\n\n    status: str = Field(..., description=\"Status do upload\")\n    message: str = Field(..., description=\"Mensagem descritiva\")\n\n    model_id: str = Field(..., description=\"ID \u00fanico do modelo\")\n    upload_timestamp: datetime = Field(..., description=\"Timestamp do upload\")\n\n    validation_results: Dict[str, Any] = Field(\n        ...,\n        description=\"Resultados da valida\u00e7\u00e3o do modelo\"\n    )\n\n    file_info: Dict[str, Any] = Field(\n        ...,\n        description=\"Informa\u00e7\u00f5es do arquivo\"\n    )\n</code></pre>"},{"location":"api/models/#modelos-de-historico-history-models","title":"\ud83d\udcca Modelos de Hist\u00f3rico (History Models)","text":""},{"location":"api/models/#historyfilter","title":"HistoryFilter","text":"<p>Filtros para consulta de hist\u00f3rico.</p> <pre><code>class HistoryFilter(BaseModel):\n    \"\"\"Filtros para consulta de hist\u00f3rico\"\"\"\n\n    start_date: Optional[datetime] = Field(\n        None, \n        description=\"Data inicial (ISO 8601)\"\n    )\n\n    end_date: Optional[datetime] = Field(\n        None,\n        description=\"Data final (ISO 8601)\"\n    )\n\n    airline: Optional[str] = Field(\n        None,\n        description=\"Filtrar por companhia a\u00e9rea\"\n    )\n\n    cancelled_only: bool = Field(\n        False,\n        description=\"Mostrar apenas voos cancelados\"\n    )\n\n    min_probability: Optional[float] = Field(\n        None,\n        ge=0.0,\n        le=1.0,\n        description=\"Probabilidade m\u00ednima de cancelamento\"\n    )\n\n    confidence_level: Optional[ConfidenceLevel] = Field(\n        None,\n        description=\"Filtrar por n\u00edvel de confian\u00e7a\"\n    )\n\n    @validator('end_date')\n    def end_after_start(cls, v, values):\n        \"\"\"Valida que data final \u00e9 ap\u00f3s inicial\"\"\"\n        if v and 'start_date' in values and values['start_date'] and v &lt;= values['start_date']:\n            raise ValueError('Data final deve ser ap\u00f3s data inicial')\n        return v\n\nclass PaginationParams(BaseModel):\n    \"\"\"Par\u00e2metros de pagina\u00e7\u00e3o\"\"\"\n\n    limit: int = Field(\n        default=10,\n        ge=1,\n        le=100,\n        description=\"N\u00famero m\u00e1ximo de registros por p\u00e1gina\"\n    )\n\n    offset: int = Field(\n        default=0,\n        ge=0,\n        description=\"N\u00famero de registros para pular\"\n    )\n\n    sort_by: str = Field(\n        default=\"timestamp\",\n        description=\"Campo para ordena\u00e7\u00e3o\"\n    )\n\n    sort_order: str = Field(\n        default=\"desc\",\n        regex=\"^(asc|desc)$\",\n        description=\"Ordem da classifica\u00e7\u00e3o\"\n    )\n\nclass HistoryQuery(BaseModel):\n    \"\"\"Query completa para hist\u00f3rico\"\"\"\n\n    filters: Optional[HistoryFilter] = Field(None, description=\"Filtros aplicados\")\n    pagination: PaginationParams = Field(default_factory=PaginationParams, description=\"Pagina\u00e7\u00e3o\")\n</code></pre>"},{"location":"api/models/#historyresponse","title":"HistoryResponse","text":"<p>Response da consulta de hist\u00f3rico.</p> <pre><code>class HistoryPrediction(BaseModel):\n    \"\"\"Predi\u00e7\u00e3o no hist\u00f3rico\"\"\"\n\n    prediction_id: str = Field(..., description=\"ID da predi\u00e7\u00e3o\")\n    timestamp: datetime = Field(..., description=\"Timestamp da predi\u00e7\u00e3o\")\n\n    input_features: FlightFeatures = Field(..., description=\"Features de entrada\")\n    prediction_result: PredictionResult = Field(..., description=\"Resultado da predi\u00e7\u00e3o\")\n\n    model_version: str = Field(..., description=\"Vers\u00e3o do modelo usado\")\n    processing_time_ms: float = Field(..., description=\"Tempo de processamento\")\n\nclass HistoryStats(BaseModel):\n    \"\"\"Estat\u00edsticas do hist\u00f3rico\"\"\"\n\n    total_predictions: int = Field(..., description=\"Total de predi\u00e7\u00f5es\")\n    cancelled_predictions: int = Field(..., description=\"Predi\u00e7\u00f5es de cancelamento\")\n    cancellation_rate: float = Field(..., description=\"Taxa de cancelamento\")\n\n    average_probability: float = Field(..., description=\"Probabilidade m\u00e9dia\")\n    confidence_distribution: Dict[str, int] = Field(..., description=\"Distribui\u00e7\u00e3o de confian\u00e7a\")\n\n    date_range: Dict[str, datetime] = Field(..., description=\"Range de datas\")\n\nclass HistoryResponse(BaseModel):\n    \"\"\"Response da consulta de hist\u00f3rico\"\"\"\n\n    predictions: List[HistoryPrediction] = Field(\n        ...,\n        description=\"Lista de predi\u00e7\u00f5es\"\n    )\n\n    pagination: Dict[str, Any] = Field(\n        ..., \n        description=\"Informa\u00e7\u00f5es de pagina\u00e7\u00e3o\"\n    )\n\n    filters_applied: Optional[Dict[str, Any]] = Field(\n        None,\n        description=\"Filtros aplicados na consulta\"\n    )\n\n    statistics: HistoryStats = Field(\n        ...,\n        description=\"Estat\u00edsticas agregadas\"\n    )\n\n    query_time_ms: float = Field(\n        ...,\n        description=\"Tempo de execu\u00e7\u00e3o da query\"\n    )\n</code></pre>"},{"location":"api/models/#modelos-de-erro-error-models","title":"\ud83d\udea8 Modelos de Erro (Error Models)","text":""},{"location":"api/models/#errordetail","title":"ErrorDetail","text":"<p>Detalhes de erro estruturado.</p> <pre><code>class ErrorDetail(BaseModel):\n    \"\"\"Detalhe espec\u00edfico de erro\"\"\"\n\n    field: Optional[str] = Field(None, description=\"Campo que causou o erro\")\n    message: str = Field(..., description=\"Mensagem do erro\")\n    code: Optional[str] = Field(None, description=\"C\u00f3digo do erro\")\n    input_value: Optional[Any] = Field(None, description=\"Valor de entrada inv\u00e1lido\")\n\nclass APIError(BaseModel):\n    \"\"\"Modelo padr\u00e3o de erro da API\"\"\"\n\n    error_type: str = Field(..., description=\"Tipo do erro\")\n    message: str = Field(..., description=\"Mensagem principal\")\n\n    details: Optional[List[ErrorDetail]] = Field(\n        None,\n        description=\"Detalhes espec\u00edficos do erro\"\n    )\n\n    timestamp: datetime = Field(\n        default_factory=datetime.now,\n        description=\"Timestamp do erro\"\n    )\n\n    request_id: str = Field(\n        default_factory=lambda: uuid4().hex[:12],\n        description=\"ID da requisi\u00e7\u00e3o para tracking\"\n    )\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"error_type\": \"VALIDATION_ERROR\",\n                \"message\": \"Erro de valida\u00e7\u00e3o nos dados de entrada\",\n                \"details\": [\n                    {\n                        \"field\": \"scheduled_departure\",\n                        \"message\": \"Formato de data inv\u00e1lido\", \n                        \"code\": \"invalid_datetime\",\n                        \"input_value\": \"invalid-date\"\n                    }\n                ],\n                \"timestamp\": \"2024-12-21T10:15:00Z\",\n                \"request_id\": \"abc123def456\"\n            }\n        }\n</code></pre>"},{"location":"api/models/#utilitarios-de-validacao","title":"\ud83d\udd27 Utilit\u00e1rios de Valida\u00e7\u00e3o","text":""},{"location":"api/models/#validators-customizados","title":"Validators Customizados","text":"<pre><code>from pydantic import validator\nimport re\nfrom datetime import datetime, timedelta\n\ndef validate_flight_number(flight_number: str) -&gt; str:\n    \"\"\"Valida formato do n\u00famero do voo\"\"\"\n    pattern = r'^[A-Z]{2,3}\\d{1,4}[A-Z]?$'\n    if not re.match(pattern, flight_number):\n        raise ValueError('Formato inv\u00e1lido. Use formato como: AA123, TAM1234')\n    return flight_number.upper()\n\ndef validate_airport_code(code: str) -&gt; str:\n    \"\"\"Valida c\u00f3digo IATA do aeroporto\"\"\"\n    if len(code) != 3 or not code.isalpha():\n        raise ValueError('C\u00f3digo IATA deve ter 3 letras')\n    return code.upper()\n\ndef validate_future_datetime(dt: datetime) -&gt; datetime:\n    \"\"\"Valida que data \u00e9 no futuro (para voos futuros)\"\"\"\n    if dt &lt;= datetime.now():\n        raise ValueError('Data deve ser no futuro')\n    return dt\n\ndef validate_reasonable_flight_duration(arrival: datetime, departure: datetime) -&gt; datetime:\n    \"\"\"Valida dura\u00e7\u00e3o razo\u00e1vel do voo (max 20 horas)\"\"\"\n    duration = arrival - departure\n    if duration &gt; timedelta(hours=20):\n        raise ValueError('Dura\u00e7\u00e3o do voo muito longa (&gt;20 horas)')\n    if duration &lt; timedelta(minutes=30):\n        raise ValueError('Dura\u00e7\u00e3o do voo muito curta (&lt;30 min)')\n    return arrival\n</code></pre>"},{"location":"api/models/#base-models-reutilizaveis","title":"Base Models Reutiliz\u00e1veis","text":"<pre><code>class TimestampedModel(BaseModel):\n    \"\"\"Base model com timestamp autom\u00e1tico\"\"\"\n\n    created_at: datetime = Field(default_factory=datetime.now)\n    updated_at: Optional[datetime] = None\n\n    class Config:\n        # Permitir campos extras em subclasses\n        extra = \"allow\"\n\nclass VersionedModel(BaseModel):\n    \"\"\"Base model com versionamento\"\"\"\n\n    version: str = Field(default=\"1.0.0\", regex=r'^\\d+\\.\\d+\\.\\d+$')\n    created_at: datetime = Field(default_factory=datetime.now)\n\nclass AuditableModel(BaseModel):\n    \"\"\"Base model com auditoria\"\"\"\n\n    created_by: Optional[str] = None\n    created_at: datetime = Field(default_factory=datetime.now)\n    updated_by: Optional[str] = None \n    updated_at: Optional[datetime] = None\n</code></pre>"},{"location":"api/models/#uso-pratico","title":"\ud83d\udcda Uso Pr\u00e1tico","text":""},{"location":"api/models/#exemplo-de-validacao","title":"Exemplo de Valida\u00e7\u00e3o","text":"<pre><code>from fastapi import HTTPException\nfrom pydantic import ValidationError\n\ntry:\n    # Criar modelo a partir de dados\n    flight = FlightFeatures(**request_data)\n\nexcept ValidationError as e:\n    # Converter erro Pydantic para resposta HTTP\n    errors = []\n    for error in e.errors():\n        errors.append({\n            \"field\": \".\".join(str(x) for x in error[\"loc\"]),\n            \"message\": error[\"msg\"],\n            \"input_value\": error.get(\"input\")\n        })\n\n    raise HTTPException(\n        status_code=422,\n        detail={\n            \"error_type\": \"VALIDATION_ERROR\",\n            \"message\": \"Dados de entrada inv\u00e1lidos\",\n            \"details\": errors\n        }\n    )\n</code></pre>"},{"location":"api/models/#exemplo-de-response","title":"Exemplo de Response","text":"<pre><code>@app.post(\"/predict\", response_model=PredictionResponse)\nasync def predict(request: PredictionRequest):\n    # Processamento...\n\n    return PredictionResponse(\n        prediction=PredictionResult(\n            cancelled=False,\n            probability=0.23,\n            confidence=\"high\"\n        ),\n        model_info=ModelInfo(\n            name=\"flight_model_v1\",\n            version=\"1.0.0\", \n            algorithm=\"Decision Tree\",\n            accuracy=0.94\n        ),\n        processing_time_ms=15.6\n    )\n</code></pre>"},{"location":"api/models/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li>\ud83d\udd17 Endpoints da API - Como usar os modelos nos endpoints</li> <li>\ud83d\udca1 Exemplos Pr\u00e1ticos - Casos de uso completos</li> <li>\ud83e\uddea Testes - Como testar valida\u00e7\u00f5es</li> <li>\ud83c\udfd7\ufe0f Arquitetura - Integra\u00e7\u00e3o com outros componentes</li> </ul>"},{"location":"api/models/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udcda Pydantic Docs: https://docs.pydantic.dev</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udce7 Email: ulisses.bomjardim@gmail.com</li> </ul>"},{"location":"architecture/components/","title":"\ud83e\udde9 Componentes da Arquitetura","text":"<p>Documenta\u00e7\u00e3o detalhada dos componentes do projeto Machine Learning Engineer Challenge e suas responsabilidades.</p>"},{"location":"architecture/components/#visao-geral-dos-componentes","title":"\ud83d\udccb Vis\u00e3o Geral dos Componentes","text":"<p>O projeto segue uma arquitetura em camadas bem definida, com separa\u00e7\u00e3o clara de responsabilidades entre os diferentes componentes.</p> <pre><code>graph TB\n    subgraph \"\ud83c\udf10 Presentation Layer\"\n        A[FastAPI Router]\n        B[Pydantic Models]\n        C[Swagger/OpenAPI]\n    end\n\n    subgraph \"\ud83e\udde0 Business Layer\"\n        D[ML Services]\n        E[Prediction Engine]\n        F[Model Manager]\n    end\n\n    subgraph \"\ud83d\uddc4\ufe0f Data Layer\"\n        G[Database Service]\n        H[Model Repository]\n        I[Data Processors]\n    end\n\n    subgraph \"\ud83d\udd27 Infrastructure Layer\"\n        J[Configuration]\n        K[Logging]\n        L[Health Monitoring]\n    end\n\n    A --&gt; D\n    B --&gt; A\n    C --&gt; A\n    D --&gt; E\n    D --&gt; F\n    E --&gt; G\n    F --&gt; H\n    G --&gt; I\n    J --&gt; A\n    K --&gt; D\n    L --&gt; A\n\n    style A fill:#e8f5e8\n    style D fill:#fff3e0\n    style G fill:#f3e5f5\n    style J fill:#fce4ec</code></pre>"},{"location":"architecture/components/#camada-de-apresentacao-presentation-layer","title":"\u26a1 Camada de Apresenta\u00e7\u00e3o (Presentation Layer)","text":""},{"location":"architecture/components/#fastapi-router-srcrouters","title":"\ud83d\udd17 FastAPI Router (<code>src/routers/</code>)","text":""},{"location":"architecture/components/#responsabilidades","title":"Responsabilidades","text":"<ul> <li>\ud83c\udf10 Exposi\u00e7\u00e3o de endpoints REST</li> <li>\ud83d\udccb Valida\u00e7\u00e3o de requests e responses</li> <li>\ud83d\udd12 Autentica\u00e7\u00e3o e autoriza\u00e7\u00e3o (futuro)</li> <li>\ud83d\udcda Documenta\u00e7\u00e3o autom\u00e1tica da API</li> <li>\u26a0\ufe0f Tratamento de exce\u00e7\u00f5es HTTP</li> </ul>"},{"location":"architecture/components/#estrutura-de-arquivos","title":"Estrutura de Arquivos","text":"<pre><code>src/routers/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py                 # \ud83d\ude80 Aplica\u00e7\u00e3o FastAPI principal\n\u2514\u2500\u2500 model/                  # \ud83e\udd16 Endpoints de Machine Learning\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 predict.py         # \ud83c\udfaf Endpoint de predi\u00e7\u00e3o\n    \u251c\u2500\u2500 load.py           # \ud83d\udce5 Carregamento de modelos\n    \u2514\u2500\u2500 history.py        # \ud83d\udcca Hist\u00f3rico de predi\u00e7\u00f5es\n</code></pre>"},{"location":"architecture/components/#implementacao-principal-mainpy","title":"Implementa\u00e7\u00e3o Principal (<code>main.py</code>)","text":"<pre><code>from fastapi import FastAPI, HTTPException, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nimport logging\n\napp = FastAPI(\n    title=\"Flight Delay Prediction API\",\n    description=\"API para predi\u00e7\u00e3o de cancelamento de voos\",\n    version=\"1.0.0\"\n)\n\n# Middleware CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configurar adequadamente em produ\u00e7\u00e3o\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Incluir routers\nfrom .model import predict, load, history\n\napp.include_router(predict.router, prefix=\"/model\", tags=[\"predictions\"])\napp.include_router(load.router, prefix=\"/model\", tags=[\"model-management\"])\napp.include_router(history.router, prefix=\"/model\", tags=[\"history\"])\n\n@app.get(\"/\", tags=[\"general\"])\nasync def root():\n    \"\"\"Endpoint raiz com informa\u00e7\u00f5es da API\"\"\"\n    return {\n        \"message\": \"Flight Delay Prediction API\",\n        \"version\": \"1.0.0\",\n        \"status\": \"running\"\n    }\n\n@app.get(\"/health\", tags=[\"general\"])\nasync def health_check():\n    \"\"\"Health check da aplica\u00e7\u00e3o\"\"\"\n    # L\u00f3gica de verifica\u00e7\u00e3o de sa\u00fade\n    return {\"status\": \"healthy\", \"timestamp\": datetime.now()}\n</code></pre>"},{"location":"architecture/components/#modelos-pydantic-models","title":"\ud83d\udccb Modelos Pydantic (<code>models/</code>)","text":""},{"location":"architecture/components/#schemas-de-requestresponse","title":"Schemas de Request/Response","text":"<pre><code># models/flight.py\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any\n\nclass FlightFeatures(BaseModel):\n    \"\"\"Features de entrada para predi\u00e7\u00e3o\"\"\"\n    airline: str = Field(..., description=\"Companhia a\u00e9rea\")\n    flight_number: str = Field(..., description=\"N\u00famero do voo\")\n    departure_airport: str = Field(..., description=\"Aeroporto de origem (c\u00f3digo IATA)\")\n    arrival_airport: str = Field(..., description=\"Aeroporto de destino (c\u00f3digo IATA)\")\n    scheduled_departure: datetime = Field(..., description=\"Hor\u00e1rio previsto de partida\")\n    scheduled_arrival: datetime = Field(..., description=\"Hor\u00e1rio previsto de chegada\")\n    aircraft_type: Optional[str] = Field(None, description=\"Tipo de aeronave\")\n    weather_condition: Optional[str] = Field(None, description=\"Condi\u00e7\u00e3o clim\u00e1tica\")\n\nclass PredictionRequest(BaseModel):\n    \"\"\"Request para predi\u00e7\u00e3o\"\"\"\n    features: FlightFeatures\n\n    class Config:\n        json_schema_extra = {\n            \"example\": {\n                \"features\": {\n                    \"airline\": \"American Airlines\",\n                    \"flight_number\": \"AA123\",\n                    \"departure_airport\": \"JFK\",\n                    \"arrival_airport\": \"LAX\",\n                    \"scheduled_departure\": \"2024-01-15T10:00:00\",\n                    \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n                }\n            }\n        }\n\nclass PredictionResult(BaseModel):\n    \"\"\"Resultado da predi\u00e7\u00e3o\"\"\"\n    cancelled: bool = Field(..., description=\"Predi\u00e7\u00e3o de cancelamento\")\n    probability: float = Field(..., ge=0, le=1, description=\"Probabilidade de cancelamento\")\n    confidence: str = Field(..., description=\"N\u00edvel de confian\u00e7a (low/medium/high)\")\n\nclass PredictionResponse(BaseModel):\n    \"\"\"Response completa de predi\u00e7\u00e3o\"\"\"\n    prediction: PredictionResult\n    prediction_id: str = Field(..., description=\"ID \u00fanico da predi\u00e7\u00e3o\")\n    timestamp: datetime = Field(..., description=\"Timestamp da predi\u00e7\u00e3o\")\n    model_info: Dict[str, Any] = Field(..., description=\"Informa\u00e7\u00f5es do modelo usado\")\n</code></pre>"},{"location":"architecture/components/#camada-de-negocio-business-layer","title":"\ud83e\udde0 Camada de Neg\u00f3cio (Business Layer)","text":""},{"location":"architecture/components/#servicos-de-machine-learning-servicesml","title":"\ud83e\udd16 Servi\u00e7os de Machine Learning (<code>services/ml/</code>)","text":""},{"location":"architecture/components/#ml-service-manager","title":"ML Service Manager","text":"<pre><code># services/ml/ml_service.py\nfrom typing import Dict, List, Any, Optional\nimport logging\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nlogger = logging.getLogger(__name__)\n\nclass MLService:\n    \"\"\"Servi\u00e7o principal de Machine Learning\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.feature_processor = None\n        self.is_loaded = False\n\n    async def load_model(self, model_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Carrega modelo de Machine Learning\"\"\"\n        try:\n            import joblib\n            self.model = joblib.load(model_path)\n            self.is_loaded = True\n\n            logger.info(f\"Modelo carregado: {model_path}\")\n\n            return {\n                \"status\": \"success\",\n                \"model_path\": model_path,\n                \"loaded_at\": datetime.now()\n            }\n\n        except Exception as e:\n            logger.error(f\"Erro ao carregar modelo: {e}\")\n            raise\n\n    async def predict(self, features: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Realiza predi\u00e7\u00e3o usando o modelo carregado\"\"\"\n        if not self.is_loaded:\n            raise ValueError(\"Modelo n\u00e3o carregado\")\n\n        try:\n            # Preprocessar features\n            processed_features = self._preprocess_features(features)\n\n            # Fazer predi\u00e7\u00e3o\n            prediction = self.model.predict(processed_features)[0]\n            probability = self.model.predict_proba(processed_features)[0][1]\n\n            # Calcular confian\u00e7a\n            confidence = self._calculate_confidence(probability)\n\n            return {\n                \"cancelled\": bool(prediction),\n                \"probability\": float(probability),\n                \"confidence\": confidence\n            }\n\n        except Exception as e:\n            logger.error(f\"Erro na predi\u00e7\u00e3o: {e}\")\n            raise\n\n    def _preprocess_features(self, features: Dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"Preprocessa features para o modelo\"\"\"\n        # Feature engineering\n        processed = {\n            'airline_encoded': self._encode_airline(features['airline']),\n            'hour_departure': pd.to_datetime(features['scheduled_departure']).hour,\n            'day_of_week': pd.to_datetime(features['scheduled_departure']).dayofweek,\n            'flight_duration': self._calculate_duration(\n                features['scheduled_departure'], \n                features['scheduled_arrival']\n            )\n        }\n\n        # Converter para array numpy na ordem esperada pelo modelo\n        feature_array = np.array([[\n            processed['airline_encoded'],\n            processed['hour_departure'],\n            processed['day_of_week'],\n            processed['flight_duration']\n        ]])\n\n        return feature_array\n\n    def _calculate_confidence(self, probability: float) -&gt; str:\n        \"\"\"Calcula n\u00edvel de confian\u00e7a baseado na probabilidade\"\"\"\n        if probability &lt; 0.3 or probability &gt; 0.7:\n            return \"high\"\n        elif probability &lt; 0.4 or probability &gt; 0.6:\n            return \"medium\"\n        else:\n            return \"low\"\n\n# Inst\u00e2ncia singleton\nml_service = MLService()\n</code></pre>"},{"location":"architecture/components/#feature-engineering-service","title":"\ud83d\udcca Feature Engineering Service","text":"<pre><code># services/ml/feature_engineering.py\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nclass FeatureEngineer:\n    \"\"\"Servi\u00e7o de engenharia de features\"\"\"\n\n    def __init__(self):\n        self.airline_mapping = {\n            \"American Airlines\": 1,\n            \"Delta\": 2,\n            \"United\": 3,\n            # ... outros mapeamentos\n        }\n\n    def engineer_features(self, raw_features: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Aplica feature engineering completo\"\"\"\n\n        features = raw_features.copy()\n\n        # Features temporais\n        departure_dt = pd.to_datetime(features['scheduled_departure'])\n        arrival_dt = pd.to_datetime(features['scheduled_arrival'])\n\n        engineered = {\n            # Features originais processadas\n            'airline_encoded': self.airline_mapping.get(features['airline'], 0),\n            'departure_airport': features['departure_airport'],\n            'arrival_airport': features['arrival_airport'],\n\n            # Features temporais\n            'hour_departure': departure_dt.hour,\n            'day_of_week': departure_dt.dayofweek,\n            'month': departure_dt.month,\n            'is_weekend': int(departure_dt.dayofweek &gt;= 5),\n\n            # Features de dura\u00e7\u00e3o\n            'flight_duration_hours': (arrival_dt - departure_dt).total_seconds() / 3600,\n\n            # Features categ\u00f3ricas derivadas\n            'time_period': self._get_time_period(departure_dt.hour),\n            'route_type': self._classify_route(\n                features['departure_airport'], \n                features['arrival_airport']\n            ),\n\n            # Features de popularidade (seria calculado a partir de dados hist\u00f3ricos)\n            'route_popularity': self._get_route_popularity(\n                features['departure_airport'], \n                features['arrival_airport']\n            )\n        }\n\n        return engineered\n\n    def _get_time_period(self, hour: int) -&gt; str:\n        \"\"\"Classifica per\u00edodo do dia\"\"\"\n        if 0 &lt;= hour &lt; 6:\n            return \"early_morning\"\n        elif 6 &lt;= hour &lt; 12:\n            return \"morning\"\n        elif 12 &lt;= hour &lt; 18:\n            return \"afternoon\"\n        else:\n            return \"evening\"\n\n    def _classify_route(self, origin: str, destination: str) -&gt; str:\n        \"\"\"Classifica tipo de rota\"\"\"\n        # L\u00f3gica simplificada - em produ\u00e7\u00e3o seria mais sofisticada\n        major_hubs = {\"JFK\", \"LAX\", \"ORD\", \"ATL\", \"DFW\"}\n\n        if origin in major_hubs and destination in major_hubs:\n            return \"hub_to_hub\"\n        elif origin in major_hubs or destination in major_hubs:\n            return \"hub_connected\"\n        else:\n            return \"regional\"\n</code></pre>"},{"location":"architecture/components/#camada-de-dados-data-layer","title":"\ud83d\uddc4\ufe0f Camada de Dados (Data Layer)","text":""},{"location":"architecture/components/#database-service-servicesdatabasepy","title":"\ud83d\udcbe Database Service (<code>services/database.py</code>)","text":"<pre><code># services/database.py\nimport os\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    from pymongo import MongoClient\n    from pymongo.errors import ConnectionFailure\n    MONGO_AVAILABLE = True\nexcept ImportError:\n    from mongomock import MongoClient\n    MONGO_AVAILABLE = False\n    logger.warning(\"PyMongo n\u00e3o dispon\u00edvel, usando mongomock\")\n\nclass DatabaseService:\n    \"\"\"Servi\u00e7o de gerenciamento de banco de dados\"\"\"\n\n    def __init__(self):\n        self.client = None\n        self.db = None\n        self.connected = False\n\n    async def connect(self) -&gt; bool:\n        \"\"\"Conecta ao banco de dados\"\"\"\n        try:\n            if MONGO_AVAILABLE:\n                # Tentar conex\u00e3o real com MongoDB\n                mongo_url = os.getenv(\"DATABASE_URL\", \"mongodb://localhost:27017\")\n                self.client = MongoClient(mongo_url, serverSelectionTimeoutMS=5000)\n\n                # Testar conex\u00e3o\n                self.client.admin.command('ping')\n                logger.info(\"Conectado ao MongoDB\")\n            else:\n                # Usar mock para desenvolvimento\n                self.client = MongoClient()\n                logger.info(\"Usando MongoDB mock\")\n\n            self.db = self.client.flight_predictions\n            self.connected = True\n            return True\n\n        except Exception as e:\n            logger.error(f\"Erro na conex\u00e3o com banco: {e}\")\n            # Fallback para mock\n            self.client = MongoClient()\n            self.db = self.client.flight_predictions\n            self.connected = True\n            return False\n\n    async def save_prediction(self, prediction_data: Dict[str, Any]) -&gt; str:\n        \"\"\"Salva predi\u00e7\u00e3o no hist\u00f3rico\"\"\"\n        try:\n            collection = self.db.predictions\n\n            # Adicionar timestamp se n\u00e3o existir\n            if 'timestamp' not in prediction_data:\n                prediction_data['timestamp'] = datetime.now()\n\n            result = collection.insert_one(prediction_data)\n\n            logger.info(f\"Predi\u00e7\u00e3o salva: {result.inserted_id}\")\n            return str(result.inserted_id)\n\n        except Exception as e:\n            logger.error(f\"Erro ao salvar predi\u00e7\u00e3o: {e}\")\n            raise\n\n    async def get_predictions(self, \n                            limit: int = 10, \n                            offset: int = 0,\n                            filters: Optional[Dict[str, Any]] = None) -&gt; Dict[str, Any]:\n        \"\"\"Recupera hist\u00f3rico de predi\u00e7\u00f5es\"\"\"\n        try:\n            collection = self.db.predictions\n\n            # Aplicar filtros se fornecidos\n            query = filters or {}\n\n            # Executar query com pagina\u00e7\u00e3o\n            cursor = collection.find(query).sort(\"timestamp\", -1).skip(offset).limit(limit)\n            predictions = list(cursor)\n\n            # Contar total de registros\n            total_count = collection.count_documents(query)\n\n            # Converter ObjectId para string\n            for pred in predictions:\n                pred['_id'] = str(pred['_id'])\n\n            return {\n                \"predictions\": predictions,\n                \"total_count\": total_count,\n                \"limit\": limit,\n                \"offset\": offset\n            }\n\n        except Exception as e:\n            logger.error(f\"Erro ao recuperar predi\u00e7\u00f5es: {e}\")\n            raise\n\n# Inst\u00e2ncia singleton\ndatabase_service = DatabaseService()\n</code></pre>"},{"location":"architecture/components/#camada-de-infraestrutura-infrastructure-layer","title":"\ud83d\udd27 Camada de Infraestrutura (Infrastructure Layer)","text":""},{"location":"architecture/components/#configuration-manager","title":"\u2699\ufe0f Configuration Manager","text":"<pre><code># config/settings.py\nfrom pydantic import BaseSettings\nfrom typing import Optional\nimport os\n\nclass Settings(BaseSettings):\n    \"\"\"Configura\u00e7\u00f5es da aplica\u00e7\u00e3o\"\"\"\n\n    # Configura\u00e7\u00f5es gerais\n    app_name: str = \"Flight Delay Prediction API\"\n    app_version: str = \"1.0.0\"\n    environment: str = \"development\"\n\n    # Configura\u00e7\u00f5es do servidor\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    reload: bool = True\n\n    # Configura\u00e7\u00f5es do banco\n    database_url: Optional[str] = None\n    database_name: str = \"flight_predictions\"\n\n    # Configura\u00e7\u00f5es do modelo\n    model_path: str = \"./model/modelo_arvore_decisao.pkl\"\n    model_auto_load: bool = True\n\n    # Configura\u00e7\u00f5es de logging\n    log_level: str = \"INFO\"\n    log_file: Optional[str] = None\n\n    # Configura\u00e7\u00f5es de API\n    api_cors_origins: list = [\"*\"]\n    api_rate_limit: int = 100  # requests por minuto\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\nsettings = Settings()\n</code></pre>"},{"location":"architecture/components/#logging-configuration","title":"\ud83d\udcca Logging Configuration","text":"<pre><code># config/logging.py\nimport logging\nimport logging.config\nfrom typing import Dict, Any\nfrom .settings import settings\n\ndef get_logging_config() -&gt; Dict[str, Any]:\n    \"\"\"Configura\u00e7\u00e3o completa de logging\"\"\"\n\n    config = {\n        \"version\": 1,\n        \"disable_existing_loggers\": False,\n        \"formatters\": {\n            \"detailed\": {\n                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            },\n            \"simple\": {\n                \"format\": \"%(levelname)s - %(message)s\"\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"level\": \"INFO\",\n                \"formatter\": \"simple\"\n            }\n        },\n        \"loggers\": {\n            \"\": {  # root logger\n                \"level\": settings.log_level,\n                \"handlers\": [\"console\"]\n            },\n            \"uvicorn\": {\n                \"level\": \"INFO\"\n            },\n            \"fastapi\": {\n                \"level\": \"INFO\"\n            }\n        }\n    }\n\n    # Adicionar arquivo de log se especificado\n    if settings.log_file:\n        config[\"handlers\"][\"file\"] = {\n            \"class\": \"logging.FileHandler\",\n            \"filename\": settings.log_file,\n            \"level\": \"DEBUG\",\n            \"formatter\": \"detailed\"\n        }\n        config[\"loggers\"][\"\"][\"handlers\"].append(\"file\")\n\n    return config\n\ndef setup_logging():\n    \"\"\"Inicializa configura\u00e7\u00e3o de logging\"\"\"\n    logging.config.dictConfig(get_logging_config())\n</code></pre>"},{"location":"architecture/components/#health-check-service","title":"\ud83c\udfe5 Health Check Service","text":"<pre><code># services/health.py\nfrom typing import Dict, Any\nfrom datetime import datetime\nimport logging\nimport psutil\nimport os\n\nlogger = logging.getLogger(__name__)\n\nclass HealthCheckService:\n    \"\"\"Servi\u00e7o de monitoramento de sa\u00fade da aplica\u00e7\u00e3o\"\"\"\n\n    def __init__(self):\n        self.start_time = datetime.now()\n\n    async def get_health_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Retorna status completo de sa\u00fade\"\"\"\n\n        components = {\n            \"api\": await self._check_api_health(),\n            \"database\": await self._check_database_health(),\n            \"model\": await self._check_model_health(),\n            \"system\": await self._check_system_health()\n        }\n\n        # Determinar status geral\n        overall_status = \"healthy\"\n        if any(comp[\"status\"] == \"unhealthy\" for comp in components.values()):\n            overall_status = \"unhealthy\"\n        elif any(comp[\"status\"] == \"degraded\" for comp in components.values()):\n            overall_status = \"degraded\"\n\n        return {\n            \"status\": overall_status,\n            \"timestamp\": datetime.now(),\n            \"uptime\": str(datetime.now() - self.start_time),\n            \"components\": components\n        }\n\n    async def _check_api_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Verifica sa\u00fade da API\"\"\"\n        return {\n            \"status\": \"healthy\",\n            \"response_time_ms\": 1.2,  # Seria medido na pr\u00e1tica\n            \"active_connections\": 5   # Seria obtido do servidor\n        }\n\n    async def _check_database_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Verifica sa\u00fade do banco de dados\"\"\"\n        try:\n            from .database import database_service\n\n            if database_service.connected:\n                # Testar opera\u00e7\u00e3o simples\n                await database_service.db.admin.command('ping')\n                return {\n                    \"status\": \"healthy\",\n                    \"connection\": \"established\",\n                    \"response_time_ms\": 5.2\n                }\n            else:\n                return {\n                    \"status\": \"degraded\",\n                    \"connection\": \"mock\",\n                    \"message\": \"Using fallback database\"\n                }\n\n        except Exception as e:\n            return {\n                \"status\": \"unhealthy\",\n                \"error\": str(e)\n            }\n\n    async def _check_model_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Verifica sa\u00fade do modelo ML\"\"\"\n        try:\n            from .ml.ml_service import ml_service\n\n            if ml_service.is_loaded:\n                return {\n                    \"status\": \"healthy\",\n                    \"model_loaded\": True,\n                    \"model_type\": \"DecisionTreeClassifier\"\n                }\n            else:\n                return {\n                    \"status\": \"unhealthy\",\n                    \"model_loaded\": False,\n                    \"message\": \"Model not loaded\"\n                }\n\n        except Exception as e:\n            return {\n                \"status\": \"unhealthy\",\n                \"error\": str(e)\n            }\n\n    async def _check_system_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Verifica sa\u00fade do sistema\"\"\"\n        try:\n            cpu_percent = psutil.cpu_percent()\n            memory = psutil.virtual_memory()\n            disk = psutil.disk_usage('/')\n\n            status = \"healthy\"\n            if cpu_percent &gt; 80 or memory.percent &gt; 85:\n                status = \"degraded\"\n            if cpu_percent &gt; 95 or memory.percent &gt; 95:\n                status = \"unhealthy\"\n\n            return {\n                \"status\": status,\n                \"cpu_percent\": cpu_percent,\n                \"memory_percent\": memory.percent,\n                \"disk_percent\": disk.percent,\n                \"available_memory_gb\": round(memory.available / (1024**3), 2)\n            }\n\n        except Exception as e:\n            return {\n                \"status\": \"unknown\",\n                \"error\": str(e)\n            }\n\n# Inst\u00e2ncia singleton\nhealth_service = HealthCheckService()\n</code></pre>"},{"location":"architecture/components/#integracao-entre-componentes","title":"\ud83d\udd04 Integra\u00e7\u00e3o entre Componentes","text":""},{"location":"architecture/components/#dependency-injection","title":"\ud83c\udfaf Dependency Injection","text":"<pre><code># dependencies.py\nfrom fastapi import Depends\nfrom .services.ml.ml_service import ml_service\nfrom .services.database import database_service\nfrom .services.health import health_service\n\nasync def get_ml_service():\n    \"\"\"Dependency para servi\u00e7o de ML\"\"\"\n    return ml_service\n\nasync def get_database_service():\n    \"\"\"Dependency para servi\u00e7o de banco\"\"\"\n    return database_service\n\nasync def get_health_service():\n    \"\"\"Dependency para servi\u00e7o de sa\u00fade\"\"\"\n    return health_service\n</code></pre>"},{"location":"architecture/components/#uso-nos-endpoints","title":"\ud83d\udcca Uso nos Endpoints","text":"<pre><code># routers/model/predict.py\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom ...dependencies import get_ml_service, get_database_service\nfrom ...models.flight import PredictionRequest, PredictionResponse\n\nrouter = APIRouter()\n\n@router.post(\"/predict\", response_model=PredictionResponse)\nasync def predict_cancellation(\n    request: PredictionRequest,\n    ml_service = Depends(get_ml_service),\n    db_service = Depends(get_database_service)\n):\n    \"\"\"Endpoint de predi\u00e7\u00e3o com inje\u00e7\u00e3o de depend\u00eancias\"\"\"\n\n    try:\n        # Fazer predi\u00e7\u00e3o\n        prediction = await ml_service.predict(request.features.dict())\n\n        # Salvar no hist\u00f3rico\n        prediction_data = {\n            \"input_features\": request.features.dict(),\n            \"prediction_result\": prediction,\n            \"timestamp\": datetime.now()\n        }\n\n        prediction_id = await db_service.save_prediction(prediction_data)\n\n        # Retornar response\n        return PredictionResponse(\n            prediction=prediction,\n            prediction_id=prediction_id,\n            timestamp=datetime.now(),\n            model_info={\"name\": \"decision_tree_v1\", \"version\": \"1.0.0\"}\n        )\n\n    except Exception as e:\n        logger.error(f\"Erro na predi\u00e7\u00e3o: {e}\")\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"architecture/components/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li>\ud83e\udd16 Pipeline de ML - Detalhes do pipeline de Machine Learning</li> <li>\u26a1 API Endpoints - Documenta\u00e7\u00e3o completa da API</li> <li>\ud83e\uddea Testes de Componentes - Como testar cada componente</li> <li>\ud83d\udc33 Deploy - Deploy em produ\u00e7\u00e3o</li> </ul>"},{"location":"architecture/components/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83c\udfd7\ufe0f Arquitetura Geral</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce7 Email</li> </ul>"},{"location":"architecture/ml-pipeline/","title":"\ud83e\udd16 Pipeline de Machine Learning","text":"<p>Documenta\u00e7\u00e3o detalhada do pipeline de Machine Learning do projeto Flight Delay Prediction.</p>"},{"location":"architecture/ml-pipeline/#visao-geral-do-pipeline","title":"\ud83d\udccb Vis\u00e3o Geral do Pipeline","text":"<p>O pipeline de ML foi projetado seguindo as melhores pr\u00e1ticas da ind\u00fastria, implementando um fluxo completo desde a ingest\u00e3o de dados at\u00e9 o deploy do modelo em produ\u00e7\u00e3o.</p> <pre><code>graph TB\n    subgraph \"\ud83d\udcca Data Ingestion\"\n        A[Raw Flight Data] --&gt; B[Data Validation]\n        B --&gt; C[Data Quality Check]\n    end\n\n    subgraph \"\ud83d\udd27 Data Processing\"\n        C --&gt; D[Data Cleaning]\n        D --&gt; E[Feature Engineering]\n        E --&gt; F[Data Splitting]\n    end\n\n    subgraph \"\ud83e\udd16 Model Development\"\n        F --&gt; G[Algorithm Selection]\n        G --&gt; H[Hyperparameter Tuning]\n        H --&gt; I[Model Training]\n        I --&gt; J[Model Validation]\n    end\n\n    subgraph \"\ud83d\udcc8 Model Evaluation\"\n        J --&gt; K[Performance Metrics]\n        K --&gt; L[Business Metrics]\n        L --&gt; M[Model Comparison]\n    end\n\n    subgraph \"\ud83d\ude80 Model Deployment\"\n        M --&gt; N[Model Serialization]\n        N --&gt; O[API Integration]\n        O --&gt; P[Production Monitoring]\n    end\n\n    style A fill:#e3f2fd\n    style D fill:#f3e5f5\n    style G fill:#e8f5e8\n    style K fill:#fff3e0\n    style N fill:#fce4ec</code></pre>"},{"location":"architecture/ml-pipeline/#1-ingestao-e-validacao-de-dados","title":"\ud83d\udcca 1. Ingest\u00e3o e Valida\u00e7\u00e3o de Dados","text":""},{"location":"architecture/ml-pipeline/#fonte-de-dados","title":"\ud83d\udd0d Fonte de Dados","text":"<p>Dataset Principal: <code>data/input/voos.json</code> - \ud83d\udcca Volume: ~100.000 registros de voos - \ud83d\udcc5 Per\u00edodo: 2019-2023 (dados hist\u00f3ricos) - \ud83d\uddc2\ufe0f Formato: JSON estruturado - \ud83d\udcdd Features: 15+ vari\u00e1veis incluindo companhia, aeroportos, hor\u00e1rios</p>"},{"location":"architecture/ml-pipeline/#validacao-inicial","title":"\u2705 Valida\u00e7\u00e3o Inicial","text":"<pre><code>def validate_raw_data(df: pd.DataFrame) -&gt; Dict[str, Any]:\n    \"\"\"Valida\u00e7\u00e3o inicial dos dados brutos\"\"\"\n\n    validation_report = {\n        \"total_records\": len(df),\n        \"missing_data\": {},\n        \"data_types\": {},\n        \"quality_issues\": []\n    }\n\n    # Verificar dados faltantes\n    missing = df.isnull().sum()\n    validation_report[\"missing_data\"] = {\n        col: {\"count\": missing[col], \"percentage\": missing[col]/len(df)*100}\n        for col in missing.index if missing[col] &gt; 0\n    }\n\n    # Verificar tipos de dados\n    validation_report[\"data_types\"] = df.dtypes.to_dict()\n\n    # Verificar consist\u00eancia de datas\n    if 'partida_prevista' in df.columns and 'chegada_prevista' in df.columns:\n        invalid_dates = df[df['partida_prevista'] &gt;= df['chegada_prevista']]\n        if len(invalid_dates) &gt; 0:\n            validation_report[\"quality_issues\"].append({\n                \"issue\": \"invalid_flight_duration\",\n                \"count\": len(invalid_dates),\n                \"description\": \"Voos com chegada antes da partida\"\n            })\n\n    return validation_report\n</code></pre> <p>Resultados da Valida\u00e7\u00e3o: <pre><code>{\n    \"total_records\": 95437,\n    \"missing_data\": {\n        \"weather_condition\": {\"count\": 12453, \"percentage\": 13.04},\n        \"aircraft_type\": {\"count\": 8901, \"percentage\": 9.32}\n    },\n    \"quality_issues\": [\n        {\n            \"issue\": \"invalid_flight_duration\", \n            \"count\": 234,\n            \"description\": \"Voos com chegada antes da partida\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"architecture/ml-pipeline/#2-processamento-e-limpeza-de-dados","title":"\ud83d\udd27 2. Processamento e Limpeza de Dados","text":""},{"location":"architecture/ml-pipeline/#limpeza-de-dados","title":"\ud83e\uddf9 Limpeza de Dados","text":"<pre><code>def clean_flight_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Pipeline completo de limpeza de dados\"\"\"\n\n    logger.info(f\"Iniciando limpeza. Dataset original: {df.shape}\")\n\n    # 1. Remover duplicatas\n    df = df.drop_duplicates()\n    logger.info(f\"Ap\u00f3s remover duplicatas: {df.shape}\")\n\n    # 2. Converter tipos de dados\n    datetime_columns = ['partida_prevista', 'chegada_prevista']\n    for col in datetime_columns:\n        df[col] = pd.to_datetime(df[col], errors='coerce')\n\n    # 3. Remover registros com datas inv\u00e1lidas\n    df = df.dropna(subset=datetime_columns)\n    logger.info(f\"Ap\u00f3s remover datas inv\u00e1lidas: {df.shape}\")\n\n    # 4. Remover voos com dura\u00e7\u00e3o inconsistente\n    df = df[df['partida_prevista'] &lt; df['chegada_prevista']]\n    logger.info(f\"Ap\u00f3s valida\u00e7\u00e3o de dura\u00e7\u00e3o: {df.shape}\")\n\n    # 5. Tratar outliers de dura\u00e7\u00e3o\n    df['duracao_horas'] = (df['chegada_prevista'] - df['partida_prevista']).dt.total_seconds() / 3600\n    df = df[(df['duracao_horas'] &gt;= 0.5) &amp; (df['duracao_horas'] &lt;= 20)]\n    logger.info(f\"Ap\u00f3s remover outliers de dura\u00e7\u00e3o: {df.shape}\")\n\n    # 6. Padronizar strings\n    string_columns = ['companhia', 'aeroporto_origem', 'aeroporto_destino']\n    for col in string_columns:\n        df[col] = df[col].str.strip().str.upper()\n\n    # 7. Tratar valores faltantes\n    df['atraso_partida'] = df['atraso_partida'].fillna(0)\n    df['weather_condition'] = df['weather_condition'].fillna('UNKNOWN')\n\n    logger.info(f\"Limpeza conclu\u00edda. Dataset final: {df.shape}\")\n    return df\n</code></pre>"},{"location":"architecture/ml-pipeline/#feature-engineering","title":"\ud83c\udfad Feature Engineering","text":"<pre><code>def engineer_features(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Engenharia de features completa\"\"\"\n\n    # Features temporais\n    df['hora_partida'] = df['partida_prevista'].dt.hour\n    df['dia_semana'] = df['partida_prevista'].dt.dayofweek  # 0=Monday\n    df['mes'] = df['partida_prevista'].dt.month\n    df['trimestre'] = df['partida_prevista'].dt.quarter\n    df['is_weekend'] = (df['dia_semana'] &gt;= 5).astype(int)\n    df['is_feriado'] = df['partida_prevista'].apply(is_holiday)\n\n    # Features de per\u00edodo do dia\n    df['periodo_dia'] = pd.cut(\n        df['hora_partida'], \n        bins=[0, 6, 12, 18, 24], \n        labels=['MADRUGADA', 'MANHA', 'TARDE', 'NOITE'],\n        include_lowest=True\n    )\n\n    # Features de dura\u00e7\u00e3o e dist\u00e2ncia\n    df['duracao_planejada'] = df['duracao_horas']\n    df['is_voo_longo'] = (df['duracao_planejada'] &gt; 4).astype(int)\n\n    # Features de rota\n    df['rota'] = df['aeroporto_origem'] + '-' + df['aeroporto_destino']\n\n    # Popularidade da rota (baseada em frequ\u00eancia)\n    route_counts = df['rota'].value_counts()\n    df['popularidade_rota'] = df['rota'].map(route_counts)\n    df['popularidade_rota_norm'] = df['popularidade_rota'] / df['popularidade_rota'].max()\n\n    # Features de aeroporto\n    airport_stats = calculate_airport_statistics(df)\n    df['origem_hub_size'] = df['aeroporto_origem'].map(airport_stats['hub_size'])\n    df['destino_hub_size'] = df['aeroporto_destino'].map(airport_stats['hub_size'])\n\n    # Features de companhia a\u00e9rea\n    airline_stats = calculate_airline_statistics(df)\n    df['companhia_reliability'] = df['companhia'].map(airline_stats['reliability'])\n    df['companhia_volume'] = df['companhia'].map(airline_stats['volume'])\n\n    # Features sazonais\n    df['is_alta_temporada'] = df['mes'].isin([12, 1, 6, 7]).astype(int)\n\n    # Features clim\u00e1ticas (se dispon\u00edvel)\n    if 'weather_condition' in df.columns:\n        df['weather_risk'] = df['weather_condition'].map({\n            'CLEAR': 0, 'CLOUDY': 1, 'RAIN': 2, \n            'STORM': 3, 'SNOW': 3, 'FOG': 2, 'UNKNOWN': 1\n        })\n\n    return df\n\ndef calculate_airport_statistics(df: pd.DataFrame) -&gt; Dict[str, Dict]:\n    \"\"\"Calcula estat\u00edsticas dos aeroportos\"\"\"\n\n    # Contagem de voos por aeroporto\n    origem_counts = df['aeroporto_origem'].value_counts()\n    destino_counts = df['aeroporto_destino'].value_counts()\n    total_counts = origem_counts.add(destino_counts, fill_value=0)\n\n    # Classificar tamanho do hub\n    hub_size = {}\n    for airport, count in total_counts.items():\n        if count &gt;= 5000:\n            hub_size[airport] = 3  # Hub grande\n        elif count &gt;= 1000:\n            hub_size[airport] = 2  # Hub m\u00e9dio\n        else:\n            hub_size[airport] = 1  # Hub pequeno\n\n    return {'hub_size': hub_size}\n</code></pre>"},{"location":"architecture/ml-pipeline/#3-desenvolvimento-do-modelo","title":"\ud83e\udd16 3. Desenvolvimento do Modelo","text":""},{"location":"architecture/ml-pipeline/#selecao-de-algoritmos","title":"\ud83c\udfaf Sele\u00e7\u00e3o de Algoritmos","text":"<p>Foram testados m\u00faltiplos algoritmos para encontrar o melhor desempenho:</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\ndef compare_algorithms(X_train, X_test, y_train, y_test):\n    \"\"\"Compara\u00e7\u00e3o de algoritmos de ML\"\"\"\n\n    algorithms = {\n        'Decision Tree': DecisionTreeClassifier(random_state=42),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n        'Logistic Regression': LogisticRegression(random_state=42),\n        'SVM': SVC(probability=True, random_state=42)\n    }\n\n    results = {}\n\n    for name, algorithm in algorithms.items():\n        logger.info(f\"Treinando {name}...\")\n\n        # Treinar modelo\n        algorithm.fit(X_train, y_train)\n\n        # Fazer predi\u00e7\u00f5es\n        y_pred = algorithm.predict(X_test)\n        y_prob = algorithm.predict_proba(X_test)[:, 1] if hasattr(algorithm, 'predict_proba') else None\n\n        # Calcular m\u00e9tricas\n        accuracy = accuracy_score(y_test, y_pred)\n\n        results[name] = {\n            'model': algorithm,\n            'accuracy': accuracy,\n            'predictions': y_pred,\n            'probabilities': y_prob\n        }\n\n        logger.info(f\"{name} - Acur\u00e1cia: {accuracy:.4f}\")\n\n    return results\n</code></pre> <p>Resultados da Compara\u00e7\u00e3o:</p> Algoritmo Acur\u00e1cia Precis\u00e3o Recall F1-Score Tempo Treino Decision Tree 94.2% 89.5% 87.3% 88.4% 2.3s Random Forest 93.8% 91.2% 85.1% 88.0% 15.7s Gradient Boosting 93.5% 90.8% 84.9% 87.8% 45.2s Logistic Regression 89.1% 78.3% 82.1% 80.2% 1.1s SVM 88.7% 79.1% 80.5% 79.8% 67.3s <p>Modelo Escolhido: Decision Tree  - \u2705 Melhor acur\u00e1cia (94.2%) - \u26a1 Treinamento r\u00e1pido (2.3s) - \ud83d\udd0d Interpretabilidade alta - \ud83d\udcca Boa generaliza\u00e7\u00e3o</p>"},{"location":"architecture/ml-pipeline/#otimizacao-de-hiperparametros","title":"\u2699\ufe0f Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros","text":"<pre><code>from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\ndef optimize_decision_tree(X_train, y_train):\n    \"\"\"Otimiza\u00e7\u00e3o de hiperpar\u00e2metros para Decision Tree\"\"\"\n\n    # Grid de hiperpar\u00e2metros\n    param_grid = {\n        'max_depth': [5, 10, 15, 20, None],\n        'min_samples_split': [50, 100, 200, 500],\n        'min_samples_leaf': [20, 50, 100, 200],\n        'max_features': ['sqrt', 'log2', None],\n        'criterion': ['gini', 'entropy']\n    }\n\n    # Grid Search com Cross Validation\n    dt = DecisionTreeClassifier(random_state=42)\n    grid_search = GridSearchCV(\n        estimator=dt,\n        param_grid=param_grid,\n        cv=5,\n        scoring='accuracy',\n        n_jobs=-1,\n        verbose=1\n    )\n\n    logger.info(\"Iniciando otimiza\u00e7\u00e3o de hiperpar\u00e2metros...\")\n    grid_search.fit(X_train, y_train)\n\n    logger.info(f\"Melhores par\u00e2metros: {grid_search.best_params_}\")\n    logger.info(f\"Melhor score CV: {grid_search.best_score_:.4f}\")\n\n    return grid_search.best_estimator_\n</code></pre> <p>Melhores Hiperpar\u00e2metros Encontrados: <pre><code>{\n    'max_depth': 15,\n    'min_samples_split': 100,\n    'min_samples_leaf': 50,\n    'max_features': 'sqrt',\n    'criterion': 'gini'\n}\n</code></pre></p>"},{"location":"architecture/ml-pipeline/#treinamento-final","title":"\ud83c\udfaf Treinamento Final","text":"<pre><code>def train_final_model(X_train, y_train, best_params):\n    \"\"\"Treina modelo final com melhores par\u00e2metros\"\"\"\n\n    # Criar modelo com hiperpar\u00e2metros otimizados\n    final_model = DecisionTreeClassifier(\n        max_depth=best_params['max_depth'],\n        min_samples_split=best_params['min_samples_split'],\n        min_samples_leaf=best_params['min_samples_leaf'],\n        max_features=best_params['max_features'],\n        criterion=best_params['criterion'],\n        random_state=42\n    )\n\n    # Treinar modelo\n    logger.info(\"Treinando modelo final...\")\n    start_time = time.time()\n\n    final_model.fit(X_train, y_train)\n\n    training_time = time.time() - start_time\n    logger.info(f\"Treinamento conclu\u00eddo em {training_time:.2f}s\")\n\n    return final_model\n</code></pre>"},{"location":"architecture/ml-pipeline/#4-avaliacao-do-modelo","title":"\ud83d\udcc8 4. Avalia\u00e7\u00e3o do Modelo","text":""},{"location":"architecture/ml-pipeline/#metricas-de-performance","title":"\ud83d\udcca M\u00e9tricas de Performance","text":"<pre><code>def evaluate_model(model, X_test, y_test, X_train, y_train):\n    \"\"\"Avalia\u00e7\u00e3o completa do modelo\"\"\"\n\n    # Predi\u00e7\u00f5es\n    y_pred_test = model.predict(X_test)\n    y_prob_test = model.predict_proba(X_test)[:, 1]\n    y_pred_train = model.predict(X_train)\n\n    # M\u00e9tricas b\u00e1sicas\n    test_accuracy = accuracy_score(y_test, y_pred_test)\n    train_accuracy = accuracy_score(y_train, y_pred_train)\n\n    # Relat\u00f3rio de classifica\u00e7\u00e3o\n    classification_rep = classification_report(y_test, y_pred_test, output_dict=True)\n\n    # Matriz de confus\u00e3o\n    cm = confusion_matrix(y_test, y_pred_test)\n\n    # AUC-ROC\n    auc_roc = roc_auc_score(y_test, y_prob_test)\n\n    # Feature importance\n    if hasattr(model, 'feature_importances_'):\n        feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'feature_{i}' for i in range(X_test.shape[1])]\n        feature_importance = pd.DataFrame({\n            'feature': feature_names,\n            'importance': model.feature_importances_\n        }).sort_values('importance', ascending=False)\n\n    results = {\n        'test_accuracy': test_accuracy,\n        'train_accuracy': train_accuracy,\n        'overfitting': train_accuracy - test_accuracy,\n        'classification_report': classification_rep,\n        'confusion_matrix': cm,\n        'auc_roc': auc_roc,\n        'feature_importance': feature_importance\n    }\n\n    return results\n</code></pre> <p>Resultados Finais:</p> <pre><code>{\n    \"test_accuracy\": 0.9421,\n    \"train_accuracy\": 0.9567,\n    \"overfitting\": 0.0146,\n    \"auc_roc\": 0.9153,\n    \"precision\": 0.895,\n    \"recall\": 0.873,\n    \"f1_score\": 0.884\n}\n</code></pre>"},{"location":"architecture/ml-pipeline/#feature-importance","title":"\ud83c\udfaf Feature Importance","text":"Rank Feature Import\u00e2ncia Interpreta\u00e7\u00e3o 1 <code>companhia_encoded</code> 0.234 Companhia a\u00e9rea \u00e9 fator cr\u00edtico 2 <code>hora_partida</code> 0.187 Hor\u00e1rio do voo muito relevante 3 <code>popularidade_rota_norm</code> 0.156 Rotas menos populares mais inst\u00e1veis 4 <code>duracao_planejada</code> 0.123 Voos longos mais propensos a cancelamento 5 <code>mes</code> 0.098 Sazonalidade impacta cancelamentos 6 <code>weather_risk</code> 0.087 Condi\u00e7\u00f5es clim\u00e1ticas s\u00e3o importantes 7 <code>origem_hub_size</code> 0.065 Tamanho do aeroporto de origem 8 <code>dia_semana</code> 0.050 Dia da semana tem influ\u00eancia"},{"location":"architecture/ml-pipeline/#analise-de-erros","title":"\ud83d\udcca An\u00e1lise de Erros","text":"<pre><code>def analyze_prediction_errors(model, X_test, y_test):\n    \"\"\"An\u00e1lise detalhada dos erros de predi\u00e7\u00e3o\"\"\"\n\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1]\n\n    # Identificar erros\n    errors = X_test[y_test != y_pred].copy()\n    errors['true_label'] = y_test[y_test != y_pred]\n    errors['predicted_label'] = y_pred[y_test != y_pred]\n    errors['probability'] = y_prob[y_test != y_pred]\n\n    # An\u00e1lise por tipo de erro\n    false_positives = errors[errors['true_label'] == 0]  # Predito cancelado, mas n\u00e3o cancelou\n    false_negatives = errors[errors['true_label'] == 1]  # Predito n\u00e3o cancelado, mas cancelou\n\n    logger.info(f\"Total de erros: {len(errors)}\")\n    logger.info(f\"Falsos positivos: {len(false_positives)} ({len(false_positives)/len(errors)*100:.1f}%)\")\n    logger.info(f\"Falsos negativos: {len(false_negatives)} ({len(false_negatives)/len(errors)*100:.1f}%)\")\n\n    return {\n        'total_errors': len(errors),\n        'false_positives': len(false_positives),\n        'false_negatives': len(false_negatives),\n        'error_data': errors\n    }\n</code></pre>"},{"location":"architecture/ml-pipeline/#5-persistencia-e-versionamento","title":"\ud83d\udcbe 5. Persist\u00eancia e Versionamento","text":""},{"location":"architecture/ml-pipeline/#salvamento-do-modelo","title":"\ud83d\udcbe Salvamento do Modelo","text":"<pre><code>import joblib\nimport json\nfrom datetime import datetime\n\ndef save_model_with_metadata(model, feature_names, metrics, model_path=\"./model/\"):\n    \"\"\"Salva modelo com metadados completos\"\"\"\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # Salvar modelo\n    model_filename = f\"modelo_arvore_decisao_{timestamp}.pkl\"\n    joblib.dump(model, os.path.join(model_path, model_filename))\n\n    # Salvar link simb\u00f3lico para modelo atual\n    current_model_path = os.path.join(model_path, \"modelo_arvore_decisao.pkl\")\n    if os.path.exists(current_model_path):\n        os.remove(current_model_path)\n    os.symlink(model_filename, current_model_path)\n\n    # Salvar metadados\n    metadata = {\n        \"model_info\": {\n            \"filename\": model_filename,\n            \"algorithm\": \"DecisionTreeClassifier\",\n            \"version\": \"1.0.0\",\n            \"created_at\": datetime.now().isoformat(),\n            \"sklearn_version\": sklearn.__version__\n        },\n        \"features\": {\n            \"count\": len(feature_names),\n            \"names\": feature_names.tolist()\n        },\n        \"performance\": metrics,\n        \"hyperparameters\": model.get_params()\n    }\n\n    metadata_filename = f\"model_metadata_{timestamp}.json\"\n    with open(os.path.join(model_path, metadata_filename), 'w') as f:\n        json.dump(metadata, f, indent=2)\n\n    logger.info(f\"Modelo salvo: {model_filename}\")\n    logger.info(f\"Metadados salvos: {metadata_filename}\")\n\n    return model_filename, metadata_filename\n</code></pre>"},{"location":"architecture/ml-pipeline/#carregamento-para-api","title":"\ud83d\udd04 Carregamento para API","text":"<pre><code>def load_model_for_api(model_path=\"./model/modelo_arvore_decisao.pkl\"):\n    \"\"\"Carrega modelo para uso na API\"\"\"\n\n    try:\n        # Carregar modelo\n        model = joblib.load(model_path)\n\n        # Carregar metadados se dispon\u00edvel\n        metadata_path = model_path.replace('.pkl', '_metadata.json')\n        metadata = {}\n        if os.path.exists(metadata_path):\n            with open(metadata_path, 'r') as f:\n                metadata = json.load(f)\n\n        logger.info(f\"Modelo carregado: {model_path}\")\n        logger.info(f\"Acur\u00e1cia: {metadata.get('performance', {}).get('test_accuracy', 'N/A')}\")\n\n        return {\n            'model': model,\n            'metadata': metadata,\n            'loaded_at': datetime.now().isoformat()\n        }\n\n    except Exception as e:\n        logger.error(f\"Erro ao carregar modelo: {e}\")\n        raise\n</code></pre>"},{"location":"architecture/ml-pipeline/#6-deploy-e-monitoramento","title":"\ud83d\ude80 6. Deploy e Monitoramento","text":""},{"location":"architecture/ml-pipeline/#monitoramento-em-producao","title":"\ud83d\udcca Monitoramento em Produ\u00e7\u00e3o","text":"<pre><code>class ModelMonitor:\n    \"\"\"Monitor de performance do modelo em produ\u00e7\u00e3o\"\"\"\n\n    def __init__(self):\n        self.predictions_log = []\n        self.performance_metrics = {}\n\n    def log_prediction(self, input_features, prediction, probability, actual_result=None):\n        \"\"\"Registra predi\u00e7\u00e3o para monitoramento\"\"\"\n\n        prediction_log = {\n            'timestamp': datetime.now().isoformat(),\n            'input_features': input_features,\n            'prediction': prediction,\n            'probability': probability,\n            'actual_result': actual_result\n        }\n\n        self.predictions_log.append(prediction_log)\n\n        # Calcular m\u00e9tricas se resultado real dispon\u00edvel\n        if actual_result is not None:\n            self._update_performance_metrics(prediction, actual_result)\n\n    def _update_performance_metrics(self, prediction, actual):\n        \"\"\"Atualiza m\u00e9tricas de performance\"\"\"\n\n        if 'total_predictions' not in self.performance_metrics:\n            self.performance_metrics = {\n                'total_predictions': 0,\n                'correct_predictions': 0,\n                'false_positives': 0,\n                'false_negatives': 0\n            }\n\n        self.performance_metrics['total_predictions'] += 1\n\n        if prediction == actual:\n            self.performance_metrics['correct_predictions'] += 1\n        elif prediction == 1 and actual == 0:\n            self.performance_metrics['false_positives'] += 1\n        elif prediction == 0 and actual == 1:\n            self.performance_metrics['false_negatives'] += 1\n\n        # Calcular acur\u00e1cia atual\n        self.performance_metrics['current_accuracy'] = (\n            self.performance_metrics['correct_predictions'] / \n            self.performance_metrics['total_predictions']\n        )\n\n    def get_drift_analysis(self):\n        \"\"\"An\u00e1lise de drift dos dados\"\"\"\n\n        if len(self.predictions_log) &lt; 100:\n            return {\"status\": \"insufficient_data\"}\n\n        # An\u00e1lise simples de drift baseada na distribui\u00e7\u00e3o de probabilidades\n        recent_probs = [p['probability'] for p in self.predictions_log[-100:]]\n        older_probs = [p['probability'] for p in self.predictions_log[-200:-100]]\n\n        if len(older_probs) == 0:\n            return {\"status\": \"insufficient_historical_data\"}\n\n        from scipy import stats\n        statistic, p_value = stats.ks_2samp(recent_probs, older_probs)\n\n        drift_detected = p_value &lt; 0.05\n\n        return {\n            \"drift_detected\": drift_detected,\n            \"p_value\": p_value,\n            \"statistic\": statistic,\n            \"recommendation\": \"retrain_model\" if drift_detected else \"continue_monitoring\"\n        }\n</code></pre>"},{"location":"architecture/ml-pipeline/#pipeline-de-retreinamento","title":"\ud83d\udd04 Pipeline de Retreinamento","text":"<pre><code>def retrain_pipeline(new_data_path, current_model_path):\n    \"\"\"Pipeline autom\u00e1tico de retreinamento\"\"\"\n\n    logger.info(\"Iniciando pipeline de retreinamento...\")\n\n    # 1. Carregar novos dados\n    new_data = pd.read_json(new_data_path)\n    logger.info(f\"Novos dados carregados: {new_data.shape}\")\n\n    # 2. Validar qualidade dos dados\n    validation_report = validate_raw_data(new_data)\n\n    if validation_report['quality_score'] &lt; 0.8:\n        logger.warning(\"Qualidade dos dados abaixo do threshold. Retreinamento cancelado.\")\n        return False\n\n    # 3. Preprocessar dados\n    cleaned_data = clean_flight_data(new_data)\n    engineered_data = engineer_features(cleaned_data)\n\n    # 4. Dividir dados\n    X, y = prepare_features_target(engineered_data)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # 5. Treinar novo modelo\n    new_model = train_final_model(X_train, y_train, best_params)\n\n    # 6. Avaliar novo modelo\n    new_metrics = evaluate_model(new_model, X_test, y_test, X_train, y_train)\n\n    # 7. Comparar com modelo atual\n    current_model = joblib.load(current_model_path)\n    current_metrics = evaluate_model(current_model, X_test, y_test, X_train, y_train)\n\n    # 8. Decidir se deve substituir\n    if new_metrics['test_accuracy'] &gt; current_metrics['test_accuracy'] + 0.01:\n        # Salvar novo modelo\n        save_model_with_metadata(new_model, X.columns, new_metrics)\n        logger.info(f\"Novo modelo salvo. Acur\u00e1cia: {new_metrics['test_accuracy']:.4f}\")\n        return True\n    else:\n        logger.info(\"Novo modelo n\u00e3o apresentou melhoria significativa. Mantendo modelo atual.\")\n        return False\n</code></pre>"},{"location":"architecture/ml-pipeline/#metricas-de-negocio","title":"\ud83d\udcca M\u00e9tricas de Neg\u00f3cio","text":""},{"location":"architecture/ml-pipeline/#impacto-financeiro","title":"\ud83d\udcb0 Impacto Financeiro","text":"<pre><code>def calculate_business_impact(predictions, actual_results, cost_matrix):\n    \"\"\"Calcula impacto financeiro do modelo\"\"\"\n\n    # Matriz de custos\n    # cost_matrix = {\n    #     'true_positive': -50,    # Evitou cancelamento (economia)\n    #     'false_positive': -200,  # Cancelou voo desnecessariamente\n    #     'true_negative': 0,      # Predi\u00e7\u00e3o correta de n\u00e3o cancelamento\n    #     'false_negative': -1000  # N\u00e3o previu cancelamento (preju\u00edzo alto)\n    # }\n\n    impact = {\n        'total_cost': 0,\n        'predictions_analyzed': len(predictions),\n        'cost_breakdown': {\n            'true_positive': 0,\n            'false_positive': 0, \n            'true_negative': 0,\n            'false_negative': 0\n        }\n    }\n\n    for pred, actual in zip(predictions, actual_results):\n        if pred == 1 and actual == 1:  # True Positive\n            impact['total_cost'] += cost_matrix['true_positive']\n            impact['cost_breakdown']['true_positive'] += 1\n\n        elif pred == 1 and actual == 0:  # False Positive\n            impact['total_cost'] += cost_matrix['false_positive']\n            impact['cost_breakdown']['false_positive'] += 1\n\n        elif pred == 0 and actual == 0:  # True Negative\n            impact['total_cost'] += cost_matrix['true_negative']\n            impact['cost_breakdown']['true_negative'] += 1\n\n        elif pred == 0 and actual == 1:  # False Negative\n            impact['total_cost'] += cost_matrix['false_negative']\n            impact['cost_breakdown']['false_negative'] += 1\n\n    impact['average_cost_per_prediction'] = impact['total_cost'] / len(predictions)\n\n    return impact\n</code></pre>"},{"location":"architecture/ml-pipeline/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":""},{"location":"architecture/ml-pipeline/#melhorias-futuras","title":"\ud83d\udd2e Melhorias Futuras","text":"<ol> <li>\ud83e\udd16 Modelos Ensemble: Combinar Decision Tree com Random Forest</li> <li>\ud83e\udde0 Deep Learning: Testar redes neurais para capturar padr\u00f5es complexos</li> <li>\u23f1\ufe0f Time Series: Incorporar an\u00e1lise temporal mais sofisticada</li> <li>\ud83c\udf26\ufe0f Dados Externos: Integrar APIs de clima em tempo real</li> <li>\ud83d\udd04 Online Learning: Implementar aprendizado cont\u00ednuo</li> <li>\ud83c\udfaf Explicabilidade: Adicionar SHAP values para interpreta\u00e7\u00e3o</li> </ol>"},{"location":"architecture/ml-pipeline/#monitoramento-avancado","title":"\ud83d\udcca Monitoramento Avan\u00e7ado","text":"<ol> <li>\ud83d\udcc8 Drift Detection: Implementar detec\u00e7\u00e3o autom\u00e1tica de drift</li> <li>\u26a0\ufe0f Alertas: Sistema de alertas para degrada\u00e7\u00e3o de performance</li> <li>\ud83d\udcca Dashboards: Interface visual para m\u00e9tricas em tempo real</li> <li>\ud83d\udd04 A/B Testing: Testes de modelos em produ\u00e7\u00e3o</li> <li>\ud83d\udcdd Audit Trail: Rastreabilidade completa de mudan\u00e7as</li> </ol>"},{"location":"architecture/ml-pipeline/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83c\udfd7\ufe0f Arquitetura ML - Vis\u00e3o geral da arquitetura</li> <li>\ud83e\udde9 Componentes - Detalhes dos componentes</li> <li>\ud83d\udcca Notebooks - An\u00e1lise explorat\u00f3ria</li> <li>\ud83e\uddea Testes ML - Testes do pipeline</li> <li>\ud83d\udc1b Issues</li> </ul>"},{"location":"architecture/overview/","title":"\ud83c\udfd7\ufe0f Vis\u00e3o Geral da Arquitetura","text":"<p>Este documento apresenta a arquitetura geral do projeto Machine Learning Engineer Challenge, detalhando a organiza\u00e7\u00e3o dos componentes e fluxos de dados.</p>"},{"location":"architecture/overview/#arquitetura-de-alto-nivel","title":"\ud83c\udfaf Arquitetura de Alto N\u00edvel","text":"<pre><code>graph TB\n    subgraph \"\ud83c\udf10 Client Layer\"\n        A[Web Browser]\n        B[HTTP Client]\n        C[Swagger UI]\n    end\n\n    subgraph \"\u26a1 API Layer\"\n        D[FastAPI Application]\n        E[Routers]\n        F[Middlewares]\n    end\n\n    subgraph \"\ud83e\udde0 Business Layer\"\n        G[ML Services]\n        H[Prediction Logic]\n        I[Model Management]\n    end\n\n    subgraph \"\ud83d\uddc4\ufe0f Data Layer\"\n        J[Database Service]\n        K[Model Storage]\n        L[Data Processing]\n    end\n\n    subgraph \"\ud83e\udd16 ML Pipeline\"\n        M[Feature Engineering]\n        N[Model Training]\n        O[Model Evaluation]\n    end\n\n    A --&gt; D\n    B --&gt; D\n    C --&gt; D\n    D --&gt; E\n    E --&gt; G\n    G --&gt; H\n    H --&gt; I\n    I --&gt; K\n    G --&gt; J\n    J --&gt; L\n    M --&gt; N\n    N --&gt; O\n    O --&gt; K\n\n    style A fill:#e3f2fd\n    style D fill:#e8f5e8\n    style G fill:#fff3e0\n    style J fill:#f3e5f5\n    style M fill:#fce4ec</code></pre>"},{"location":"architecture/overview/#organizacao-do-codigo","title":"\ud83d\udcc2 Organiza\u00e7\u00e3o do C\u00f3digo","text":""},{"location":"architecture/overview/#estrutura-hierarquica","title":"\ud83c\udfdb\ufe0f Estrutura Hier\u00e1rquica","text":"<pre><code>machine-learning-engineer/\n\u251c\u2500\u2500 \ud83d\ude80 src/                          # C\u00f3digo fonte principal\n\u2502   \u251c\u2500\u2500 routers/                     # \ud83d\udd17 Camada de API/Rotas\n\u2502   \u2502   \u251c\u2500\u2500 main.py                  # \ud83d\udce1 Aplica\u00e7\u00e3o FastAPI principal\n\u2502   \u2502   \u2514\u2500\u2500 model/                   # \ud83e\udd16 Endpoints de Machine Learning\n\u2502   \u2502       \u251c\u2500\u2500 predict.py           # \ud83c\udfaf Predi\u00e7\u00f5es\n\u2502   \u2502       \u251c\u2500\u2500 load.py              # \ud83d\udce5 Carregamento de modelos\n\u2502   \u2502       \u2514\u2500\u2500 history.py           # \ud83d\udcca Hist\u00f3rico de predi\u00e7\u00f5es\n\u2502   \u2514\u2500\u2500 services/                    # \u2699\ufe0f Camada de Servi\u00e7os/Neg\u00f3cio\n\u2502       \u2514\u2500\u2500 database.py              # \ud83d\uddc4\ufe0f Gerenciamento de dados\n\u251c\u2500\u2500 \ud83e\uddea tests/                       # Testes automatizados\n\u251c\u2500\u2500 \ud83d\udcca data/                        # Datasets e dados processados\n\u251c\u2500\u2500 \ud83e\udd16 model/                       # Modelos treinados e artefatos\n\u251c\u2500\u2500 \ud83d\udcd3 notebook/                    # Jupyter Notebooks (EDA/Experimentos)\n\u251c\u2500\u2500 \ud83d\udcd6 docs/                        # Documenta\u00e7\u00e3o MkDocs\n\u2514\u2500\u2500 \ud83d\udc33 docker/                      # Configura\u00e7\u00f5es de containers\n</code></pre>"},{"location":"architecture/overview/#padroes-arquiteturais","title":"\ud83c\udfa8 Padr\u00f5es Arquiteturais","text":"Padr\u00e3o Implementa\u00e7\u00e3o Benef\u00edcios Layered Architecture API \u2192 Business \u2192 Data Separa\u00e7\u00e3o de responsabilidades Repository Pattern <code>database.py</code> Abstra\u00e7\u00e3o de dados Dependency Injection FastAPI dependencies Testabilidade Factory Pattern Model loading Flexibilidade de modelos"},{"location":"architecture/overview/#camada-de-api-fastapi","title":"\u26a1 Camada de API (FastAPI)","text":""},{"location":"architecture/overview/#estrutura-de-rotas","title":"\ud83d\udd17 Estrutura de Rotas","text":"<pre><code>graph TD\n    A[FastAPI App] --&gt; B[Main Router]\n    B --&gt; C[Health Routes]\n    B --&gt; D[Model Routes]\n    B --&gt; E[User Routes]\n\n    D --&gt; F[Predict Endpoint]\n    D --&gt; G[Load Endpoint]\n    D --&gt; H[History Endpoint]\n\n    style A fill:#e8f5e8\n    style B fill:#e3f2fd\n    style F fill:#fff3e0\n    style G fill:#fff3e0\n    style H fill:#fff3e0</code></pre> <p>Responsabilidades: - \ud83c\udf10 Exposi\u00e7\u00e3o de endpoints REST - \ud83d\udd12 Valida\u00e7\u00e3o de requests (Pydantic) - \ud83d\udccb Serializa\u00e7\u00e3o de responses - \u26a0\ufe0f Tratamento de erros - \ud83d\udcda Documenta\u00e7\u00e3o autom\u00e1tica (Swagger)</p> <p>Tecnologias: - <code>FastAPI</code> - Framework web moderno - <code>Pydantic</code> - Valida\u00e7\u00e3o de dados - <code>Uvicorn</code> - ASGI server</p>"},{"location":"architecture/overview/#endpoints-principais","title":"\ud83d\udce1 Endpoints Principais","text":"<pre><code># src/routers/main.py\n@app.get(\"/\")                    # \u2139\ufe0f Informa\u00e7\u00f5es da API\n@app.get(\"/health\")              # \ud83d\udc9a Health check\n@app.get(\"/docs\")                # \ud83d\udcda Documenta\u00e7\u00e3o Swagger\n\n# src/routers/model/predict.py\n@router.post(\"/model/predict\")   # \ud83c\udfaf Predi\u00e7\u00f5es ML\n\n# src/routers/model/load.py\n@router.get(\"/model/load/default\")   # \ud83d\udce5 Carregar modelo padr\u00e3o\n@router.post(\"/model/load/\")         # \ud83d\udce4 Upload modelo\n\n# src/routers/model/history.py\n@router.get(\"/model/history/\")   # \ud83d\udcca Hist\u00f3rico predi\u00e7\u00f5es\n</code></pre>"},{"location":"architecture/overview/#camada-de-negocio","title":"\ud83e\udde0 Camada de Neg\u00f3cio","text":""},{"location":"architecture/overview/#machine-learning-services","title":"\ud83e\udd16 Machine Learning Services","text":"<pre><code>graph LR\n    A[Request] --&gt; B[Data Validation]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Model Prediction]\n    D --&gt; E[Result Processing]\n    E --&gt; F[Response]\n\n    G[Model Storage] --&gt; D\n    D --&gt; H[History Storage]\n\n    style A fill:#e3f2fd\n    style D fill:#fff3e0\n    style F fill:#e8f5e8\n    style G fill:#f3e5f5\n    style H fill:#fce4ec</code></pre> <p>Componentes: - \ud83c\udfaf Prediction Engine - Core de predi\u00e7\u00e3o - \ud83d\udd27 Feature Engineering - Transforma\u00e7\u00e3o de dados - \ud83d\udce5 Model Loader - Carregamento din\u00e2mico - \ud83d\udcca History Manager - Gest\u00e3o de hist\u00f3rico - \u2705 Validation Layer - Valida\u00e7\u00e3o de dados</p>"},{"location":"architecture/overview/#fluxo-de-predicao","title":"\ud83d\udd04 Fluxo de Predi\u00e7\u00e3o","text":"<pre><code># Fluxo t\u00edpico de predi\u00e7\u00e3o\n1. \ud83d\udce5 Receber dados do cliente\n2. \u2705 Validar entrada (Pydantic)\n3. \ud83d\udd27 Aplicar feature engineering\n4. \ud83e\udd16 Executar predi\u00e7\u00e3o no modelo\n5. \ud83d\udcca Salvar no hist\u00f3rico\n6. \ud83d\udce4 Retornar resultado\n</code></pre>"},{"location":"architecture/overview/#camada-de-dados","title":"\ud83d\uddc4\ufe0f Camada de Dados","text":""},{"location":"architecture/overview/#gerenciamento-de-dados","title":"\ud83d\udcbe Gerenciamento de Dados","text":"<pre><code>graph TB\n    subgraph \"\ud83d\uddc4\ufe0f Data Storage\"\n        A[MongoDB/MockDB]\n        B[Model Files]\n        C[CSV Datasets]\n    end\n\n    subgraph \"\u2699\ufe0f Data Services\"\n        D[Database Service]\n        E[Model Repository]\n        F[Data Processors]\n    end\n\n    subgraph \"\ud83d\udcca Data Operations\"\n        G[CRUD Operations]\n        H[Model Persistence]\n        I[Data Validation]\n    end\n\n    A --&gt; D\n    B --&gt; E\n    C --&gt; F\n    D --&gt; G\n    E --&gt; H\n    F --&gt; I\n\n    style A fill:#f3e5f5\n    style D fill:#e8f5e8\n    style G fill:#fff3e0</code></pre> <p>Responsabilidades: - \ud83d\udcbe Persist\u00eancia de predi\u00e7\u00f5es - \ud83d\udd04 CRUD operations para hist\u00f3rico - \ud83d\udcc1 Gerenciamento de modelos - \ud83d\udd0d Queries otimizadas - \ud83d\udee1\ufe0f Backup e recupera\u00e7\u00e3o</p> <p>Tecnologias: - <code>MongoDB</code> / <code>mongomock</code> - Banco NoSQL - <code>pandas</code> - Manipula\u00e7\u00e3o de dados - <code>pickle</code> / <code>joblib</code> - Serializa\u00e7\u00e3o de modelos</p>"},{"location":"architecture/overview/#pipeline-de-machine-learning","title":"\ud83e\udd16 Pipeline de Machine Learning","text":""},{"location":"architecture/overview/#fluxo-do-ml-pipeline","title":"\ud83d\udd04 Fluxo do ML Pipeline","text":"<pre><code>graph LR\n    subgraph \"\ud83d\udcca Data Processing\"\n        A[Raw Data] --&gt; B[Cleaning]\n        B --&gt; C[Feature Engineering]\n    end\n\n    subgraph \"\ud83c\udfaf Model Development\"\n        C --&gt; D[Model Training]\n        D --&gt; E[Validation]\n        E --&gt; F[Hyperparameter Tuning]\n    end\n\n    subgraph \"\ud83d\ude80 Model Deployment\"\n        F --&gt; G[Model Persistence]\n        G --&gt; H[API Integration]\n        H --&gt; I[Production Serving]\n    end\n\n    subgraph \"\ud83d\udcc8 Monitoring\"\n        I --&gt; J[Performance Tracking]\n        J --&gt; K[Model Retraining]\n        K --&gt; D\n    end\n\n    style A fill:#e3f2fd\n    style D fill:#fff3e0\n    style G fill:#e8f5e8\n    style J fill:#fce4ec</code></pre>"},{"location":"architecture/overview/#componentes-do-pipeline","title":"\ud83e\udde9 Componentes do Pipeline","text":"Componente Responsabilidade Localiza\u00e7\u00e3o \ud83d\udd27 Data Preprocessing Limpeza e transforma\u00e7\u00e3o <code>notebook/Transform.ipynb</code> \ud83d\udcca Feature Engineering Cria\u00e7\u00e3o de features <code>notebook/Model.ipynb</code> \ud83e\udd16 Model Training Treinamento de algoritmos <code>notebook/Model.ipynb</code> \ud83d\udcc8 Model Evaluation M\u00e9tricas e valida\u00e7\u00e3o <code>notebook/Model.ipynb</code> \ud83d\udcbe Model Persistence Salvamento de modelos <code>model/</code> directory \u26a1 Model Serving API de predi\u00e7\u00e3o <code>src/routers/model/</code>"},{"location":"architecture/overview/#fluxos-de-dados","title":"\ud83d\udd04 Fluxos de Dados","text":""},{"location":"architecture/overview/#fluxo-de-treinamento","title":"\ud83d\udcca Fluxo de Treinamento","text":"<pre><code>sequenceDiagram\n    participant D as \ud83d\udcca Data Source\n    participant N as \ud83d\udcd3 Notebook\n    participant M as \ud83e\udd16 Model\n    participant S as \ud83d\udcbe Storage\n\n    D-&gt;&gt;N: Load raw data\n    N-&gt;&gt;N: EDA &amp; preprocessing\n    N-&gt;&gt;N: Feature engineering\n    N-&gt;&gt;M: Train model\n    M-&gt;&gt;N: Return trained model\n    N-&gt;&gt;S: Save model artifact\n    Note over S: modelo_arvore_decisao.pkl</code></pre>"},{"location":"architecture/overview/#fluxo-de-predicao_1","title":"\ud83c\udfaf Fluxo de Predi\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n    participant C as \ud83d\udc64 Client\n    participant A as \u26a1 API\n    participant S as \u2699\ufe0f Service\n    participant M as \ud83e\udd16 Model\n    participant D as \ud83d\uddc4\ufe0f Database\n\n    C-&gt;&gt;A: POST /model/predict\n    A-&gt;&gt;S: Validate &amp; process\n    S-&gt;&gt;M: Execute prediction\n    M-&gt;&gt;S: Return prediction\n    S-&gt;&gt;D: Save to history\n    S-&gt;&gt;A: Format response\n    A-&gt;&gt;C: Return JSON result</code></pre>"},{"location":"architecture/overview/#principios-arquiteturais","title":"\ud83d\udee1\ufe0f Princ\u00edpios Arquiteturais","text":""},{"location":"architecture/overview/#design-principles","title":"\ud83c\udfaf Design Principles","text":"Princ\u00edpio Implementa\u00e7\u00e3o Benef\u00edcio Separation of Concerns Camadas distintas Manutenibilidade Single Responsibility Classes focadas Testabilidade Dependency Inversion Interfaces abstratas Flexibilidade Don't Repeat Yourself Utilit\u00e1rios compartilhados Consist\u00eancia Keep It Simple Solu\u00e7\u00f5es diretas Compreensibilidade"},{"location":"architecture/overview/#escalabilidade","title":"\ud83d\ude80 Escalabilidade","text":"<p>Estrat\u00e9gias de Scaling: - \ud83d\udd04 Horizontal scaling via containers - \u26a1 Load balancing com m\u00faltiplos workers - \ud83d\udcbe Database sharding para grande volume - \ud83c\udfaf Model versioning para A/B testing - \ud83d\udcca Caching de predi\u00e7\u00f5es frequentes</p>"},{"location":"architecture/overview/#confiabilidade","title":"\ud83d\udee1\ufe0f Confiabilidade","text":"<p>Garantias de Qualidade: - \ud83e\uddea Testes automatizados (&gt;95% coverage) - \ud83d\udd0d Valida\u00e7\u00e3o rigorosa de entrada - \u26a0\ufe0f Error handling gracioso - \ud83d\udcca Logging estruturado - \ud83d\udd04 Health checks peri\u00f3dicos</p>"},{"location":"architecture/overview/#configuracao-e-deploy","title":"\ud83d\udd27 Configura\u00e7\u00e3o e Deploy","text":""},{"location":"architecture/overview/#containerizacao","title":"\ud83d\udc33 Containeriza\u00e7\u00e3o","text":"<pre><code>graph TB\n    subgraph \"\ud83c\udfd7\ufe0f Build Stage\"\n        A[Base Image] --&gt; B[Dependencies]\n        B --&gt; C[Source Code]\n    end\n\n    subgraph \"\ud83d\ude80 Runtime Stage\"\n        C --&gt; D[Production Image]\n        D --&gt; E[Container Instance]\n    end\n\n    subgraph \"\ud83d\udd04 Orchestration\"\n        E --&gt; F[Docker Compose]\n        F --&gt; G[Load Balancer]\n        G --&gt; H[Multiple Containers]\n    end\n\n    style A fill:#e3f2fd\n    style D fill:#e8f5e8\n    style F fill:#fff3e0</code></pre> <p>Benef\u00edcios: - \ud83d\udd04 Reprodutibilidade entre ambientes - \u26a1 Deploy r\u00e1pido e consistente - \ud83c\udfaf Isolamento de depend\u00eancias - \ud83d\udcca Escalabilidade horizontal</p>"},{"location":"architecture/overview/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<p>Para entender melhor a arquitetura:</p> <ol> <li>\ud83e\udde9 Componentes Detalhados</li> <li>\ud83e\udd16 Pipeline de ML</li> <li>\u26a1 API Reference</li> <li>\ud83e\uddea Testes de Arquitetura</li> </ol>"},{"location":"architecture/overview/#suporte-tecnico","title":"\ud83d\udcde Suporte T\u00e9cnico","text":"<ul> <li>\ud83c\udfd7\ufe0f Discuss\u00f5es de Arquitetura</li> <li>\ud83d\udc1b Issues T\u00e9cnicas</li> <li>\ud83d\udce7 Contato Direto</li> </ul>"},{"location":"dev/contributing/","title":"\ud83e\udd1d Guia de Contribui\u00e7\u00e3o","text":"<p>Bem-vindo ao projeto Flight Delay Prediction! Este guia cont\u00e9m todas as informa\u00e7\u00f5es necess\u00e1rias para contribuir de forma efetiva, incluindo padr\u00f5es de c\u00f3digo, fluxo de desenvolvimento e boas pr\u00e1ticas.</p>"},{"location":"dev/contributing/#como-contribuir","title":"\ud83c\udfaf Como Contribuir","text":""},{"location":"dev/contributing/#tipos-de-contribuicao","title":"\ud83d\udccb Tipos de Contribui\u00e7\u00e3o","text":"<p>Valorizamos todos os tipos de contribui\u00e7\u00e3o:</p> <ul> <li>\ud83d\udc1b Bug Reports: Identifique e reporte problemas</li> <li>\u2728 Feature Requests: Sugira novas funcionalidades  </li> <li>\ud83d\udd27 Code Contributions: Implemente corre\u00e7\u00f5es e melhorias</li> <li>\ud83d\udcda Documentation: Melhore a documenta\u00e7\u00e3o</li> <li>\ud83e\uddea Testing: Adicione ou melhore testes</li> <li>\ud83c\udfa8 UI/UX: Melhorias na interface e experi\u00eancia</li> <li>\ud83d\udcca Data Science: Novos modelos e an\u00e1lises</li> </ul>"},{"location":"dev/contributing/#primeiros-passos","title":"\ud83d\ude80 Primeiros Passos","text":""},{"location":"dev/contributing/#1-fork-e-clone","title":"1. \ud83d\udce5 Fork e Clone","text":"<pre><code># 1. Fa\u00e7a fork do reposit\u00f3rio no GitHub\n# 2. Clone seu fork localmente\ngit clone https://github.com/SEU_USUARIO/machine-learning-engineer.git\ncd machine-learning-engineer\n\n# 3. Adicione o reposit\u00f3rio original como upstream\ngit remote add upstream https://github.com/ORIGINAL_OWNER/machine-learning-engineer.git\n\n# 4. Verifique os remotes\ngit remote -v\n</code></pre>"},{"location":"dev/contributing/#2-configuracao-do-ambiente","title":"2. \u2699\ufe0f Configura\u00e7\u00e3o do Ambiente","text":"<pre><code># Instalar Poetry (se n\u00e3o tiver)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Instalar depend\u00eancias\npoetry install --with dev,test\n\n# Ativar ambiente virtual\npoetry shell\n\n# Verificar instala\u00e7\u00e3o\npython --version\npoetry --version\n</code></pre>"},{"location":"dev/contributing/#3-setup-de-desenvolvimento","title":"3. \ud83d\udd27 Setup de Desenvolvimento","text":"<pre><code># Instalar pre-commit hooks\npre-commit install\n\n# Executar testes para garantir que tudo funciona\nmake test\n\n# Verificar linting\nmake lint\n\n# Executar formata\u00e7\u00e3o\nmake format\n</code></pre>"},{"location":"dev/contributing/#fluxo-de-desenvolvimento","title":"\ud83c\udf0a Fluxo de Desenvolvimento","text":""},{"location":"dev/contributing/#workflow-padrao","title":"\ud83d\udccb Workflow Padr\u00e3o","text":"<pre><code>graph LR\n    A[Fork Repo] --&gt; B[Create Branch]\n    B --&gt; C[Make Changes]\n    C --&gt; D[Write Tests]\n    D --&gt; E[Run Tests]\n    E --&gt; F[Commit Changes]\n    F --&gt; G[Push Branch]\n    G --&gt; H[Create PR]\n    H --&gt; I[Code Review]\n    I --&gt; J{Approved?}\n    J --&gt;|Yes| K[Merge]\n    J --&gt;|No| L[Address Feedback]\n    L --&gt; C</code></pre>"},{"location":"dev/contributing/#1-criacao-de-branch","title":"1. \ud83c\udf3f Cria\u00e7\u00e3o de Branch","text":"<pre><code># Sempre criar branch a partir da main atualizada\ngit checkout main\ngit pull upstream main\n\n# Criar nova branch com nome descritivo\ngit checkout -b feature/add-weather-integration\ngit checkout -b fix/memory-leak-in-predictor\ngit checkout -b docs/update-api-examples\ngit checkout -b refactor/improve-data-pipeline\n</code></pre> <p>Conven\u00e7\u00e3o de Nomes de Branch: - <code>feature/descricao-da-funcionalidade</code> - <code>fix/descricao-do-bug</code> - <code>docs/descricao-da-documentacao</code> - <code>refactor/descricao-da-refatoracao</code> - <code>test/descricao-do-teste</code> - <code>chore/descricao-da-tarefa</code></p>"},{"location":"dev/contributing/#2-desenvolvimento","title":"2. \ud83d\udcbb Desenvolvimento","text":"<pre><code># Fa\u00e7a suas altera\u00e7\u00f5es\n# Escreva testes\n# Mantenha commits pequenos e focados\n\n# Executar testes frequentemente\nmake test-unit\nmake test-integration\n\n# Verificar qualidade do c\u00f3digo\nmake lint\nmake type-check\n</code></pre>"},{"location":"dev/contributing/#3-commits","title":"3. \ud83d\udcdd Commits","text":"<p>Seguimos a Conventional Commits specification:</p> <pre><code># Estrutura: tipo(escopo): descri\u00e7\u00e3o\ngit commit -m \"feat(api): add weather data integration\"\ngit commit -m \"fix(ml): resolve memory leak in model loading\"\ngit commit -m \"docs(readme): update installation instructions\"\ngit commit -m \"test(services): add unit tests for database service\"\ngit commit -m \"refactor(utils): improve data validation functions\"\n</code></pre> <p>Tipos de Commit: - <code>feat</code>: Nova funcionalidade - <code>fix</code>: Corre\u00e7\u00e3o de bug - <code>docs</code>: Altera\u00e7\u00f5es na documenta\u00e7\u00e3o - <code>style</code>: Formata\u00e7\u00e3o, lint fixes - <code>refactor</code>: Refatora\u00e7\u00e3o de c\u00f3digo - <code>test</code>: Adi\u00e7\u00e3o ou altera\u00e7\u00e3o de testes - <code>chore</code>: Tarefas de manuten\u00e7\u00e3o - <code>perf</code>: Melhoria de performance - <code>ci</code>: Altera\u00e7\u00f5es no CI/CD</p>"},{"location":"dev/contributing/#4-pull-request","title":"4. \ud83d\udd04 Pull Request","text":"<pre><code># Antes de criar PR, sincronize com upstream\ngit fetch upstream\ngit rebase upstream/main\n\n# Push da branch\ngit push origin feature/add-weather-integration\n\n# Criar Pull Request no GitHub\n</code></pre>"},{"location":"dev/contributing/#padroes-de-codigo","title":"\ud83d\udccf Padr\u00f5es de C\u00f3digo","text":""},{"location":"dev/contributing/#python-standards","title":"\ud83d\udc0d Python Standards","text":""},{"location":"dev/contributing/#formatacao","title":"Formata\u00e7\u00e3o","text":"<p>Usamos Black e isort para formata\u00e7\u00e3o:</p> <pre><code># .pyproject.toml configura\u00e7\u00e3o\n[tool.black]\nline-length = 88\ntarget-version = ['py312']\ninclude = '\\.pyi?$'\nextend-exclude = '''\n/(\n  migrations\n  | .venv\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nmulti_line_output = 3\nline_length = 88\n</code></pre>"},{"location":"dev/contributing/#linting","title":"Linting","text":"<p>Usamos flake8 e pylint:</p> <pre><code># setup.cfg\n[flake8]\nmax-line-length = 88\nextend-ignore = E203, W503, E501\nexclude = \n    .git,\n    __pycache__,\n    .venv,\n    migrations,\n    build,\n    dist\n\n[pylint]\nmax-line-length = 88\ndisable = \n    missing-docstring,\n    too-few-public-methods,\n    import-error\n</code></pre>"},{"location":"dev/contributing/#type-hints","title":"Type Hints","text":"<p>Use type hints sempre que poss\u00edvel:</p> <pre><code>from typing import List, Dict, Optional, Union, Any\nfrom datetime import datetime\nimport pandas as pd\n\ndef predict_flight_delay(\n    flight_data: Dict[str, Any],\n    model_version: str = \"latest\"\n) -&gt; Dict[str, Union[float, str]]:\n    \"\"\"\n    Prediz atraso de voo baseado nos dados fornecidos.\n\n    Args:\n        flight_data: Dados do voo para predi\u00e7\u00e3o\n        model_version: Vers\u00e3o do modelo a usar\n\n    Returns:\n        Dicion\u00e1rio com predi\u00e7\u00e3o e metadados\n\n    Raises:\n        ValueError: Se dados inv\u00e1lidos forem fornecidos\n        ModelNotFoundError: Se vers\u00e3o do modelo n\u00e3o existir\n    \"\"\"\n    # Implementa\u00e7\u00e3o aqui\n    pass\n\nclass FlightDelayPredictor:\n    \"\"\"Classe para predi\u00e7\u00e3o de atrasos de voo.\"\"\"\n\n    def __init__(self, model_path: str) -&gt; None:\n        self.model_path = model_path\n        self._model: Optional[Any] = None\n\n    def load_model(self) -&gt; None:\n        \"\"\"Carrega modelo do disco.\"\"\"\n        pass\n\n    def predict(\n        self, \n        features: pd.DataFrame\n    ) -&gt; List[float]:\n        \"\"\"Realiza predi\u00e7\u00f5es em lote.\"\"\"\n        pass\n</code></pre>"},{"location":"dev/contributing/#padroes-de-teste","title":"\ud83e\uddea Padr\u00f5es de Teste","text":""},{"location":"dev/contributing/#estrutura-de-testes","title":"Estrutura de Testes","text":"<pre><code># tests/test_predictor.py\nimport pytest\nfrom unittest.mock import Mock, patch\nimport pandas as pd\n\nfrom src.ml.predictor import FlightDelayPredictor\nfrom src.exceptions import ModelNotFoundError\n\n\nclass TestFlightDelayPredictor:\n    \"\"\"Testes para a classe FlightDelayPredictor.\"\"\"\n\n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Dados de exemplo para testes.\"\"\"\n        return pd.DataFrame({\n            'hour': [10, 14, 18],\n            'day_of_week': [1, 3, 5],\n            'weather_score': [0.8, 0.6, 0.9]\n        })\n\n    @pytest.fixture\n    def predictor(self, tmp_path):\n        \"\"\"Inst\u00e2ncia do preditor para testes.\"\"\"\n        model_path = tmp_path / \"test_model.pkl\"\n        return FlightDelayPredictor(str(model_path))\n\n    def test_init(self, predictor):\n        \"\"\"Testa inicializa\u00e7\u00e3o do preditor.\"\"\"\n        assert predictor.model_path is not None\n        assert predictor._model is None\n\n    @patch('pickle.load')\n    @patch('builtins.open')\n    def test_load_model_success(self, mock_open, mock_pickle, predictor):\n        \"\"\"Testa carregamento bem-sucedido do modelo.\"\"\"\n        # Arrange\n        mock_model = Mock()\n        mock_pickle.return_value = mock_model\n\n        # Act\n        predictor.load_model()\n\n        # Assert\n        assert predictor._model == mock_model\n        mock_open.assert_called_once()\n\n    def test_load_model_file_not_found(self, predictor):\n        \"\"\"Testa erro quando arquivo do modelo n\u00e3o existe.\"\"\"\n        # Act &amp; Assert\n        with pytest.raises(ModelNotFoundError):\n            predictor.load_model()\n\n    def test_predict(self, predictor, sample_data):\n        \"\"\"Testa predi\u00e7\u00e3o com dados v\u00e1lidos.\"\"\"\n        # Arrange\n        predictor._model = Mock()\n        predictor._model.predict.return_value = [10.5, 15.2, 5.0]\n\n        # Act\n        result = predictor.predict(sample_data)\n\n        # Assert\n        assert len(result) == 3\n        assert all(isinstance(pred, float) for pred in result)\n        predictor._model.predict.assert_called_once()\n\n    @pytest.mark.parametrize(\"invalid_data\", [\n        None,\n        pd.DataFrame(),  # DataFrame vazio\n        \"not_a_dataframe\"\n    ])\n    def test_predict_invalid_data(self, predictor, invalid_data):\n        \"\"\"Testa predi\u00e7\u00e3o com dados inv\u00e1lidos.\"\"\"\n        predictor._model = Mock()\n\n        with pytest.raises(ValueError):\n            predictor.predict(invalid_data)\n\n\n# Testes de integra\u00e7\u00e3o\nclass TestFlightDelayPredictorIntegration:\n    \"\"\"Testes de integra\u00e7\u00e3o para FlightDelayPredictor.\"\"\"\n\n    def test_full_prediction_pipeline(self, real_model_path, real_flight_data):\n        \"\"\"Testa pipeline completo de predi\u00e7\u00e3o.\"\"\"\n        # Arrange\n        predictor = FlightDelayPredictor(real_model_path)\n        predictor.load_model()\n\n        # Act\n        predictions = predictor.predict(real_flight_data)\n\n        # Assert\n        assert len(predictions) == len(real_flight_data)\n        assert all(pred &gt;= 0 for pred in predictions)  # Atrasos n\u00e3o negativos\n</code></pre>"},{"location":"dev/contributing/#convencoes-de-teste","title":"Conven\u00e7\u00f5es de Teste","text":"<ol> <li>Nomes descritivos: <code>test_should_return_error_when_model_not_found</code></li> <li>Arrange-Act-Assert: Estrutura clara de prepara\u00e7\u00e3o, execu\u00e7\u00e3o e verifica\u00e7\u00e3o</li> <li>Fixtures: Use para dados e setup reutiliz\u00e1veis</li> <li>Mocking: Mock depend\u00eancias externas</li> <li>Parametriza\u00e7\u00e3o: Use <code>@pytest.mark.parametrize</code> para m\u00faltiplos casos</li> </ol>"},{"location":"dev/contributing/#estrutura-de-arquivos","title":"\ud83d\udcc1 Estrutura de Arquivos","text":"<pre><code>src/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py                     # FastAPI application\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 settings.py            # Configura\u00e7\u00f5es da aplica\u00e7\u00e3o\n\u251c\u2500\u2500 routers/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 health.py             # Health checks\n\u2502   \u251c\u2500\u2500 prediction.py         # Endpoints de predi\u00e7\u00e3o\n\u2502   \u2514\u2500\u2500 model.py              # Endpoints do modelo\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 database.py           # Servi\u00e7os de banco de dados\n\u2502   \u251c\u2500\u2500 external_apis.py      # Integra\u00e7\u00f5es externas\n\u2502   \u2514\u2500\u2500 cache.py              # Servi\u00e7os de cache\n\u251c\u2500\u2500 ml/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 predictor.py          # Classe principal de predi\u00e7\u00e3o\n\u2502   \u251c\u2500\u2500 preprocessor.py       # Pipeline de preprocessamento\n\u2502   \u251c\u2500\u2500 feature_engineer.py   # Engenharia de features\n\u2502   \u2514\u2500\u2500 model_loader.py       # Carregamento de modelos\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 validators.py         # Validadores de dados\n\u2502   \u251c\u2500\u2500 exceptions.py         # Exce\u00e7\u00f5es customizadas\n\u2502   \u2514\u2500\u2500 logging.py            # Configura\u00e7\u00e3o de logs\n\u2514\u2500\u2500 models/\n    \u2514\u2500\u2500 schemas.py            # Pydantic models\n</code></pre>"},{"location":"dev/contributing/#ferramentas-de-qualidade","title":"\ud83d\udd27 Ferramentas de Qualidade","text":""},{"location":"dev/contributing/#pre-commit-configuration","title":"Pre-commit Configuration","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n      - id: check-json\n      - id: check-merge-conflict\n\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n        language_version: python3.12\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.0.0\n    hooks:\n      - id: flake8\n        additional_dependencies: [flake8-docstrings]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.3.0\n    hooks:\n      - id: mypy\n        additional_dependencies: [types-all]\n</code></pre>"},{"location":"dev/contributing/#makefile-commands","title":"Makefile Commands","text":"<pre><code># Makefile\n.PHONY: install test lint format type-check clean\n\ninstall:\n    poetry install --with dev,test\n\ntest:\n    poetry run pytest tests/ -v --cov=src --cov-report=html\n\ntest-unit:\n    poetry run pytest tests/unit/ -v\n\ntest-integration:\n    poetry run pytest tests/integration/ -v\n\nlint:\n    poetry run flake8 src/ tests/\n    poetry run pylint src/\n\nformat:\n    poetry run black src/ tests/\n    poetry run isort src/ tests/\n\ntype-check:\n    poetry run mypy src/\n\nclean:\n    find . -type d -name \"__pycache__\" -delete\n    find . -type f -name \"*.pyc\" -delete\n    rm -rf .coverage htmlcov/ .pytest_cache/\n\npre-commit:\n    pre-commit run --all-files\n\nci: install lint type-check test\n    @echo \"\u2705 All CI checks passed!\"\n</code></pre>"},{"location":"dev/contributing/#padroes-de-data-science","title":"\ud83d\udcca Padr\u00f5es de Data Science","text":""},{"location":"dev/contributing/#notebooks","title":"\ud83d\udd2c Notebooks","text":""},{"location":"dev/contributing/#estrutura-padrao","title":"Estrutura Padr\u00e3o","text":"<pre><code># =============================================================================\n# NOTEBOOK: An\u00e1lise Explorat\u00f3ria de Dados - Flight Delays\n# AUTOR: Seu Nome\n# DATA: 2024-01-20\n# VERS\u00c3O: 1.0\n# =============================================================================\n\n# %% [markdown]\n# # An\u00e1lise Explorat\u00f3ria de Dados - Flight Delays\n# \n# **Objetivo:** Analisar padr\u00f5es nos dados de atraso de voos\n# **Dataset:** flight_delays_2024.csv\n# **Per\u00edodo:** Janeiro 2024\n\n# %% [markdown]\n# ## 1. Setup e Imports\n\n# %%\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\n\n# Configura\u00e7\u00f5es\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# %%\n# Configura\u00e7\u00f5es globais\nRANDOM_STATE = 42\nDATA_PATH = Path(\"../data/input/\")\nOUTPUT_PATH = Path(\"../data/output/\")\n\n# %% [markdown]\n# ## 2. Carregamento de Dados\n\n# %%\ndef load_flight_data(filepath: Path) -&gt; pd.DataFrame:\n    \"\"\"Carrega e faz limpeza inicial dos dados de voo.\"\"\"\n    df = pd.read_csv(filepath)\n\n    # Convers\u00f5es de tipo\n    df['scheduled_departure'] = pd.to_datetime(df['scheduled_departure'])\n    df['actual_departure'] = pd.to_datetime(df['actual_departure'])\n\n    # Feature engineering b\u00e1sica\n    df['delay_minutes'] = (\n        df['actual_departure'] - df['scheduled_departure']\n    ).dt.total_seconds() / 60\n\n    return df\n\ndf = load_flight_data(DATA_PATH / \"flight_data.csv\")\nprint(f\"\ud83d\udcca Dados carregados: {df.shape[0]:,} registros, {df.shape[1]} colunas\")\n</code></pre>"},{"location":"dev/contributing/#convencoes-para-notebooks","title":"Conven\u00e7\u00f5es para Notebooks","text":"<ol> <li>Cabe\u00e7alho padr\u00e3o com metadados</li> <li>Se\u00e7\u00f5es organizadas com markdown</li> <li>C\u00e9lulas pequenas e focadas</li> <li>Fun\u00e7\u00e3o auxiliares bem documentadas</li> <li>Visualiza\u00e7\u00f5es explicativas</li> <li>Conclus\u00f5es claras ao final</li> </ol>"},{"location":"dev/contributing/#experimentos-ml","title":"\ud83e\udd16 Experimentos ML","text":""},{"location":"dev/contributing/#template-de-experimento","title":"Template de Experimento","text":"<pre><code># experiments/experiment_template.py\nfrom dataclasses import dataclass\nfrom typing import Dict, Any, Optional\nimport mlflow\nimport json\nfrom datetime import datetime\n\n@dataclass\nclass ExperimentConfig:\n    \"\"\"Configura\u00e7\u00e3o de experimento ML.\"\"\"\n    experiment_name: str\n    model_name: str\n    model_params: Dict[str, Any]\n    features: List[str]\n    preprocessing_steps: List[str]\n    validation_strategy: str\n    random_state: int = 42\n\nclass MLExperiment:\n    \"\"\"Classe base para experimentos de ML.\"\"\"\n\n    def __init__(self, config: ExperimentConfig):\n        self.config = config\n        self.results = {}\n\n    def run_experiment(\n        self, \n        X_train, y_train, \n        X_test, y_test\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Executa experimento completo.\"\"\"\n\n        with mlflow.start_run(run_name=self.config.experiment_name):\n            # Log da configura\u00e7\u00e3o\n            mlflow.log_params(self.config.model_params)\n            mlflow.log_param(\"features_count\", len(self.config.features))\n            mlflow.log_param(\"validation_strategy\", self.config.validation_strategy)\n\n            # Treinar modelo\n            model = self._train_model(X_train, y_train)\n\n            # Avaliar modelo\n            metrics = self._evaluate_model(model, X_test, y_test)\n\n            # Log das m\u00e9tricas\n            for metric_name, value in metrics.items():\n                mlflow.log_metric(metric_name, value)\n\n            # Salvar modelo\n            mlflow.sklearn.log_model(model, \"model\")\n\n            # Preparar resultados\n            self.results = {\n                \"experiment_id\": mlflow.active_run().info.run_id,\n                \"config\": self.config,\n                \"metrics\": metrics,\n                \"timestamp\": datetime.now().isoformat()\n            }\n\n        return self.results\n\n    def _train_model(self, X_train, y_train):\n        \"\"\"Treina o modelo.\"\"\"\n        # Implementa\u00e7\u00e3o espec\u00edfica do modelo\n        raise NotImplementedError\n\n    def _evaluate_model(self, model, X_test, y_test) -&gt; Dict[str, float]:\n        \"\"\"Avalia o modelo.\"\"\"\n        # Implementa\u00e7\u00e3o das m\u00e9tricas\n        raise NotImplementedError\n</code></pre>"},{"location":"dev/contributing/#padroes-de-documentacao","title":"\ud83d\udcd6 Padr\u00f5es de Documenta\u00e7\u00e3o","text":""},{"location":"dev/contributing/#docstrings","title":"\ud83d\udcdd Docstrings","text":"<p>Use o formato Google Style:</p> <pre><code>def predict_flight_delay(\n    flight_data: Dict[str, Any],\n    model_version: str = \"latest\",\n    include_confidence: bool = True\n) -&gt; Dict[str, Union[float, str]]:\n    \"\"\"\n    Prediz atraso de voo baseado nos dados fornecidos.\n\n    Esta fun\u00e7\u00e3o utiliza o modelo de machine learning treinado para predizer\n    o atraso esperado de um voo com base em suas caracter\u00edsticas.\n\n    Args:\n        flight_data: Dicion\u00e1rio contendo dados do voo. Deve incluir:\n            - flight_number (str): N\u00famero do voo\n            - origin_airport (str): Aeroporto de origem (c\u00f3digo IATA)\n            - destination_airport (str): Aeroporto de destino (c\u00f3digo IATA)\n            - scheduled_departure (str): Hor\u00e1rio programado (ISO format)\n            - weather_conditions (dict): Condi\u00e7\u00f5es clim\u00e1ticas\n        model_version: Vers\u00e3o do modelo a utilizar. Padr\u00e3o \u00e9 \"latest\".\n        include_confidence: Se deve incluir score de confian\u00e7a na resposta.\n\n    Returns:\n        Dicion\u00e1rio contendo:\n            - delay_minutes (float): Atraso predito em minutos\n            - probability_delayed (float): Probabilidade de atraso &gt; 15min\n            - confidence (float): Score de confian\u00e7a (se include_confidence=True)\n            - model_version (str): Vers\u00e3o do modelo utilizada\n\n    Raises:\n        ValueError: Se flight_data cont\u00e9m campos inv\u00e1lidos ou ausentes\n        ModelNotFoundError: Se a vers\u00e3o do modelo especificada n\u00e3o existe\n        PredictionError: Se ocorrer erro durante a predi\u00e7\u00e3o\n\n    Example:\n        &gt;&gt;&gt; flight_info = {\n        ...     \"flight_number\": \"AA123\",\n        ...     \"origin_airport\": \"JFK\", \n        ...     \"destination_airport\": \"LAX\",\n        ...     \"scheduled_departure\": \"2024-01-15T10:00:00\",\n        ...     \"weather_conditions\": {\"temperature\": 25.0, \"wind_speed\": 10}\n        ... }\n        &gt;&gt;&gt; result = predict_flight_delay(flight_info)\n        &gt;&gt;&gt; print(f\"Atraso esperado: {result['delay_minutes']:.1f} minutos\")\n        Atraso esperado: 12.3 minutos\n\n    Note:\n        Esta fun\u00e7\u00e3o requer que o modelo esteja previamente carregado na mem\u00f3ria.\n        Para melhor performance, mantenha a inst\u00e2ncia do preditor ativa.\n    \"\"\"\n    # Implementa\u00e7\u00e3o aqui\n    pass\n</code></pre>"},{"location":"dev/contributing/#readme-guidelines","title":"\ud83d\udccb README Guidelines","text":""},{"location":"dev/contributing/#estrutura-padrao_1","title":"Estrutura Padr\u00e3o","text":"<pre><code># \ud83d\udcca Flight Delay Prediction\n\n[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://python.org)\n[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com)\n[![Tests](https://github.com/user/repo/workflows/Tests/badge.svg)](https://github.com/user/repo/actions)\n[![Coverage](https://codecov.io/gh/user/repo/branch/main/graph/badge.svg)](https://codecov.io/gh/user/repo)\n\n&gt; Sistema de predi\u00e7\u00e3o de atrasos de voos usando Machine Learning\n\n## \u2728 Features\n\n- \ud83c\udfaf **Predi\u00e7\u00e3o Precisa**: MAE de 10.8 minutos\n- \u26a1 **API R\u00e1pida**: Resposta &lt; 100ms\n- \ud83d\udd04 **Real-time**: Integra\u00e7\u00e3o com APIs clim\u00e1ticas\n- \ud83d\udcca **Monitoramento**: Dashboard de performance\n- \ud83d\udc33 **Docker Ready**: Deploy simplificado\n\n## \ud83d\ude80 Quick Start\n\n### Prerequisites\n\n- Python 3.12+\n- Poetry\n- Docker (opcional)\n\n### Installation\n\n```bash\n# Clone repository\ngit clone https://github.com/user/flight-delay-prediction.git\ncd flight-delay-prediction\n\n# Install dependencies\npoetry install\n\n# Run application\npoetry run python -m src.main\n</code></pre>"},{"location":"dev/contributing/#usage","title":"Usage","text":"<pre><code>import requests\n\n# Predict flight delay\nresponse = requests.post(\"http://localhost:8000/predict\", json={\n    \"flight_number\": \"AA123\",\n    \"origin_airport\": \"JFK\",\n    \"destination_airport\": \"LAX\",\n    \"scheduled_departure\": \"2024-01-15T10:00:00\"\n})\n\nresult = response.json()\nprint(f\"Expected delay: {result['delay_minutes']:.1f} minutes\")\n</code></pre>"},{"location":"dev/contributing/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>\ud83d\udcd6 Full Documentation - Complete documentation</li> <li>\ud83d\ude80 Quick Start - Get started quickly  </li> <li>\ud83c\udfd7\ufe0f Architecture - System design</li> <li>\ud83e\uddea API Reference - Endpoint documentation</li> </ul>"},{"location":"dev/contributing/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"dev/contributing/#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the project LICENSE file. <pre><code>## \ud83d\udd04 Code Review Guidelines\n\n### \ud83d\udcdd Review Checklist\n\n#### Funcionalidade\n- [ ] O c\u00f3digo faz o que deveria fazer?\n- [ ] A l\u00f3gica est\u00e1 correta?\n- [ ] Casos extremos s\u00e3o tratados?\n- [ ] Tratamento de erros \u00e9 adequado?\n\n#### Qualidade do C\u00f3digo\n- [ ] C\u00f3digo est\u00e1 leg\u00edvel e bem organizado?\n- [ ] Nomes de vari\u00e1veis/fun\u00e7\u00f5es s\u00e3o descritivos?\n- [ ] N\u00e3o h\u00e1 c\u00f3digo duplicado?\n- [ ] Complexidade est\u00e1 apropriada?\n\n#### Testes\n- [ ] Novos testes foram adicionados?\n- [ ] Testes cobrem casos importantes?\n- [ ] Todos os testes passam?\n- [ ] Coverage n\u00e3o diminuiu significativamente?\n\n#### Documenta\u00e7\u00e3o\n- [ ] C\u00f3digo est\u00e1 documentado adequadamente?\n- [ ] README foi atualizado se necess\u00e1rio?\n- [ ] Docstrings est\u00e3o presentes e corretas?\n\n#### Performance\n- [ ] N\u00e3o h\u00e1 vazamentos de mem\u00f3ria?\n- [ ] Performance \u00e9 aceit\u00e1vel?\n- [ ] Otimiza\u00e7\u00f5es desnecess\u00e1rias foram evitadas?\n\n### \ud83d\udcac Coment\u00e1rios de Review\n\n**Bons exemplos:**\n</code></pre> \u2705 \"Boa implementa\u00e7\u00e3o! Considere adicionar tratamento para o caso em que     weather_data seja None.\"</p> <p>\u2705 \"O algoritmo est\u00e1 correto, mas podemos melhorar a legibilidade extraindo     esta l\u00f3gica para uma fun\u00e7\u00e3o separada.\"</p> <p>\u2705 \"Excelente cobertura de testes! Seria interessante adicionar um teste     para o caso de timeout da API.\" <pre><code>**Evitar:**\n</code></pre> \u274c \"Este c\u00f3digo est\u00e1 ruim.\"</p> <p>\u274c \"N\u00e3o funciona.\"</p> <p>\u274c \"Reescreva tudo.\" <pre><code>### \ud83c\udff7\ufe0f Labels de PR\n\n- `\ud83d\udc1b bug`: Corre\u00e7\u00e3o de bug\n- `\u2728 enhancement`: Nova funcionalidade\n- `\ud83d\udcda documentation`: Melhoria na documenta\u00e7\u00e3o\n- `\ud83e\uddea tests`: Relacionado a testes\n- `\ud83d\udd27 refactor`: Refatora\u00e7\u00e3o\n- `\u26a1 performance`: Melhoria de performance\n- `\ud83d\udea8 breaking-change`: Mudan\u00e7a que quebra compatibilidade\n- `\ud83d\ude80 ready-for-review`: PR pronto para revis\u00e3o\n- `\u23f3 work-in-progress`: Ainda em desenvolvimento\n\n## \ud83d\ude80 Release Process\n\n### \ud83d\udce6 Versionamento\n\nSeguimos [Semantic Versioning](https://semver.org/):\n- **MAJOR**: Mudan\u00e7as que quebram compatibilidade\n- **MINOR**: Novas funcionalidades mantendo compatibilidade\n- **PATCH**: Corre\u00e7\u00f5es de bugs\n\n### \ud83c\udff7\ufe0f Cria\u00e7\u00e3o de Release\n\n```bash\n# 1. Atualizar vers\u00e3o\npoetry version patch  # ou minor, major\n\n# 2. Atualizar CHANGELOG\n# Adicionar entrada para nova vers\u00e3o\n\n# 3. Commit e tag\ngit add pyproject.toml CHANGELOG.md\ngit commit -m \"chore: release v1.2.3\"\ngit tag v1.2.3\n\n# 4. Push\ngit push origin main --tags\n</code></pre></p>"},{"location":"dev/contributing/#checklist-de-release","title":"\ud83d\udccb Checklist de Release","text":"<ul> <li>[ ] Todos os testes passam</li> <li>[ ] Documenta\u00e7\u00e3o est\u00e1 atualizada</li> <li>[ ] CHANGELOG foi atualizado</li> <li>[ ] Vers\u00e3o foi incrementada</li> <li>[ ] Tag foi criada</li> <li>[ ] Release notes foram escritas</li> </ul>"},{"location":"dev/contributing/#ajuda-e-suporte","title":"\ud83c\udd98 Ajuda e Suporte","text":""},{"location":"dev/contributing/#onde-buscar-ajuda","title":"\ud83d\udcde Onde Buscar Ajuda","text":"<ol> <li>\ud83d\udcd6 Documenta\u00e7\u00e3o: Verifique a documenta\u00e7\u00e3o completa</li> <li>\ud83d\udc1b Issues: Busque por issues similares no GitHub</li> <li>\ud83d\udcac Discussions: Participe das discuss\u00f5es da comunidade</li> <li>\ud83d\udce7 Contato: Entre em contato com maintainers</li> </ol>"},{"location":"dev/contributing/#como-fazer-boas-perguntas","title":"\u2753 Como Fazer Boas Perguntas","text":"<ol> <li>Seja espec\u00edfico: Descreva exatamente o que est\u00e1 tentando fazer</li> <li>Contexto: Forne\u00e7a informa\u00e7\u00f5es sobre o ambiente</li> <li>C\u00f3digo: Inclua c\u00f3digo relevante (use c\u00f3digo minimalreprodut\u00edvel)</li> <li>Erro: Cole a mensagem de erro completa</li> <li>Tentativas: Descreva o que j\u00e1 tentou</li> </ol> <p>Exemplo de boa pergunta:</p> <pre><code>## Problema\nEstou tentando fazer uma predi\u00e7\u00e3o usando a API, mas recebo erro 422.\n\n## Ambiente\n- Python 3.12\n- FastAPI 0.104\n- OS: Ubuntu 22.04\n\n## C\u00f3digo\n```python\nimport requests\n\nresponse = requests.post(\"http://localhost:8000/api/v1/predict\", json={\n    \"flight_number\": \"AA123\",\n    \"origin_airport\": \"JFK\"\n})\nprint(response.status_code)  # 422\n</code></pre>"},{"location":"dev/contributing/#erro","title":"Erro","text":"<pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"destination_airport\"],\n      \"msg\": \"field required\",\n      \"type\": \"value_error.missing\"\n    }\n  ]\n}\n</code></pre>"},{"location":"dev/contributing/#o-que-ja-tentei","title":"O que j\u00e1 tentei","text":"<ul> <li>Verifiquei a documenta\u00e7\u00e3o da API</li> <li>Testei com outros dados</li> <li>Validei o JSON enviado</li> </ul> <p>Como posso resolver esse erro de campo obrigat\u00f3rio? ```</p>"},{"location":"dev/contributing/#agradecimentos","title":"\ud83d\ude4f Agradecimentos","text":"<p>Obrigado por contribuir para o projeto Flight Delay Prediction! Sua participa\u00e7\u00e3o ajuda a tornar este projeto melhor para todos. </p>"},{"location":"dev/contributing/#contributors","title":"\ud83c\udf1f Contributors","text":"<p>\ud83d\udcde Contato - \ud83d\udce7 Email: maintainer@project.com - \ud83d\udcac Discord: Convite do servidor - \ud83d\udc26 Twitter: @project_ml</p>"},{"location":"dev/environment/","title":"\ud83d\udcbb Ambiente de Desenvolvimento","text":"<p>Guia completo para configurar um ambiente de desenvolvimento produtivo para o projeto Machine Learning Engineer Challenge.</p>"},{"location":"dev/environment/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Este guia apresenta as melhores pr\u00e1ticas para configurar um ambiente de desenvolvimento completo, incluindo ferramentas, configura\u00e7\u00f5es e workflows recomendados.</p>"},{"location":"dev/environment/#stack-de-desenvolvimento","title":"\ud83d\udee0\ufe0f Stack de Desenvolvimento","text":""},{"location":"dev/environment/#ferramentas-essenciais","title":"\ud83d\udccb Ferramentas Essenciais","text":"Categoria Ferramenta Vers\u00e3o Prop\u00f3sito \ud83d\udcbb Editor VS Code Latest IDE principal \ud83d\udc0d Python Python 3.12.7 Linguagem base \ud83d\udce6 Deps Poetry 1.7+ Gerenciamento de depend\u00eancias \ud83d\udd27 Git Git 2.40+ Controle de vers\u00e3o \ud83d\udc33 Container Docker 24.0+ Containeriza\u00e7\u00e3o \ud83d\udcd3 Notebook Jupyter Latest An\u00e1lise de dados"},{"location":"dev/environment/#extensoes-vs-code-recomendadas","title":"\ud83c\udfa8 Extens\u00f5es VS Code Recomendadas","text":"<pre><code>// .vscode/extensions.json\n{\n  \"recommendations\": [\n    // Python essentials\n    \"ms-python.python\",\n    \"ms-python.black-formatter\",\n    \"ms-python.isort\",\n    \"charliermarsh.ruff\",\n    \"ms-python.pylint\",\n\n    // Jupyter\n    \"ms-toolsai.jupyter\",\n    \"ms-toolsai.jupyter-keymap\",\n\n    // Git\n    \"eamodio.gitlens\",\n    \"github.vscode-pull-request-github\",\n\n    // Docker\n    \"ms-azuretools.vscode-docker\",\n\n    // Markdown\n    \"yzhang.markdown-all-in-one\",\n    \"shd101wyy.markdown-preview-enhanced\",\n\n    // \u00dateis\n    \"ms-vscode.errorlens\",\n    \"wayou.vscode-todo-highlight\",\n    \"streetsidesoftware.code-spell-checker\"\n  ]\n}\n</code></pre>"},{"location":"dev/environment/#configuracoes-vs-code","title":"\u2699\ufe0f Configura\u00e7\u00f5es VS Code","text":"<pre><code>// .vscode/settings.json\n{\n  // Python\n  \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n  \"python.formatting.provider\": \"none\",\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": true,\n  \"python.testing.pytestEnabled\": true,\n  \"python.testing.unittestEnabled\": false,\n\n  // Formata\u00e7\u00e3o\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n      \"source.organizeImports\": true\n    }\n  },\n\n  // Black formatter\n  \"black-formatter.args\": [\"--line-length=88\"],\n\n  // isort\n  \"isort.args\": [\"--profile\", \"black\"],\n\n  // Ruff\n  \"ruff.args\": [\"--line-length=88\"],\n\n  // Files\n  \"files.exclude\": {\n    \"**/__pycache__\": true,\n    \"**/*.pyc\": true,\n    \".pytest_cache\": true,\n    \".coverage\": true,\n    \"htmlcov\": true,\n    \".ruff_cache\": true\n  },\n\n  // Editor\n  \"editor.rulers\": [88],\n  \"editor.wordWrap\": \"bounded\",\n  \"editor.wordWrapColumn\": 88,\n\n  // Terminal\n  \"terminal.integrated.defaultProfile.windows\": \"PowerShell\",\n  \"terminal.integrated.defaultProfile.linux\": \"bash\",\n  \"terminal.integrated.defaultProfile.osx\": \"zsh\"\n}\n</code></pre>"},{"location":"dev/environment/#configuracao-python-avancada","title":"\ud83d\udc0d Configura\u00e7\u00e3o Python Avan\u00e7ada","text":""},{"location":"dev/environment/#pyprojecttoml-completo","title":"\ud83c\udfaf pyproject.toml Completo","text":"<pre><code>[tool.poetry]\nname = \"machine-learning-engineer\"\nversion = \"1.0.0\"\ndescription = \"Flight delay prediction API with ML pipeline\"\nauthors = [\"Ulisses Bomjardim &lt;ulisses.bomjardim@gmail.com&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"src\"}]\n\n[tool.poetry.dependencies]\npython = \"&gt;=3.12.0,&lt;4.0\"\nfastapi = \"^0.104.1\"\nuvicorn = \"^0.24.0\"\npandas = \"^2.1.4\"\nscikit-learn = \"^1.3.2\"\npydantic = \"^2.5.1\"\npython-multipart = \"^0.0.6\"\npymongo = {version = \"^4.6.0\", optional = true}\njoblib = \"^1.3.2\"\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^7.4.3\"\npytest-cov = \"^4.1.0\"\npytest-xdist = \"^3.5.0\"\npytest-mock = \"^3.12.0\"\nblack = \"^23.11.0\"\nisort = \"^5.12.0\"\nruff = \"^0.1.7\"\npylint = \"^3.0.3\"\nmypy = \"^1.7.1\"\nhttpx = \"^0.25.2\"\njupyter = \"^1.0.0\"\nnotebook = \"^7.0.6\"\nipykernel = \"^6.27.1\"\nmkdocs = \"^1.5.3\"\nmkdocs-material = \"^9.4.10\"\npre-commit = \"^3.6.0\"\n\n[tool.poetry.group.extras.dependencies]\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.0\"\nplotly = \"^5.17.0\"\nydata-profiling = \"^4.6.4\"\n\n[tool.poetry.extras]\nmongodb = [\"pymongo\"]\nviz = [\"matplotlib\", \"seaborn\", \"plotly\"]\nprofiling = [\"ydata-profiling\"]\n\n[tool.taskipy.tasks]\ntest = \"pytest tests/ -v\"\ntest-cov = \"pytest tests/ --cov=src --cov-report=term-missing --cov-report=html\"\nformat = \"black src/ tests/ &amp;&amp; isort src/ tests/\"\nlint = \"ruff check src/ tests/ &amp;&amp; pylint src/ tests/\"\ntype-check = \"mypy src/ tests/\"\ndocs = \"mkdocs serve\"\napi = \"uvicorn src.routers.main:app --reload\"\nclean = \"find . -type d -name '__pycache__' -exec rm -rf {} + 2&gt;/dev/null || true\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n# Configura\u00e7\u00f5es de ferramentas\n[tool.black]\nline-length = 88\ntarget-version = ['py312']\ninclude = '\\.pyi?$'\nextend-exclude = '''\n/(\n  \\.git\n  | \\.mypy_cache\n  | \\.pytest_cache\n  | \\.ruff_cache\n  | __pycache__\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nline_length = 88\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\n\n[tool.ruff]\nline-length = 88\ntarget-version = \"py312\"\nselect = [\n  \"E\",   # pycodestyle errors\n  \"W\",   # pycodestyle warnings  \n  \"F\",   # pyflakes\n  \"I\",   # isort\n  \"B\",   # flake8-bugbear\n  \"C4\",  # flake8-comprehensions\n  \"UP\",  # pyupgrade\n]\nignore = [\n  \"E501\",  # line too long (handled by black)\n  \"B008\",  # do not perform function calls in argument defaults\n]\n\n[tool.mypy]\npython_version = \"3.12\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\ndisallow_incomplete_defs = true\ncheck_untyped_defs = true\ndisallow_untyped_decorators = true\nno_implicit_optional = true\nwarn_redundant_casts = true\nwarn_unused_ignores = true\nwarn_no_return = true\nwarn_unreachable = true\nstrict_equality = true\n\n[[tool.mypy.overrides]]\nmodule = [\n  \"sklearn.*\",\n  \"pandas.*\",\n  \"numpy.*\",\n  \"matplotlib.*\",\n  \"seaborn.*\",\n]\nignore_missing_imports = true\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\npython_classes = [\"Test*\"]\npython_functions = [\"test_*\"]\naddopts = [\n  \"-v\",\n  \"--strict-markers\",\n  \"--tb=short\",\n  \"--cov=src\",\n  \"--cov-report=term-missing\",\n  \"--cov-fail-under=85\",\n]\nmarkers = [\n  \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\",\n  \"integration: marks tests as integration tests\",\n  \"unit: marks tests as unit tests\",\n  \"api: marks tests as API tests\",\n]\nfilterwarnings = [\n  \"ignore::UserWarning\",\n  \"ignore::DeprecationWarning\",\n]\n\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\n  \"*/tests/*\",\n  \"*/test_*\",\n  \"*/__pycache__/*\",\n  \"*/venv/*\",\n  \"*/.venv/*\",\n]\n\n[tool.coverage.report]\nexclude_lines = [\n  \"pragma: no cover\",\n  \"def __repr__\",\n  \"raise AssertionError\", \n  \"raise NotImplementedError\",\n  \"if __name__ == .__main__.:\",\n]\n</code></pre>"},{"location":"dev/environment/#pre-commit-configuration","title":"\ud83d\udd27 Pre-commit Configuration","text":"<pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.4.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n      - id: check-merge-conflict\n      - id: debug-statements\n\n  - repo: https://github.com/psf/black\n    rev: 23.11.0\n    hooks:\n      - id: black\n        language_version: python3.12\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n        args: [\"--profile\", \"black\"]\n\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: v0.1.7\n    hooks:\n      - id: ruff\n        args: [--fix, --exit-non-zero-on-fix]\n\n  - repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v1.7.1\n    hooks:\n      - id: mypy\n        additional_dependencies: [types-requests]\n</code></pre> <p>Instalar pre-commit: <pre><code># Instalar hooks\npoetry run pre-commit install\n\n# Executar em todos os arquivos\npoetry run pre-commit run --all-files\n\n# Atualizar hooks\npoetry run pre-commit autoupdate\n</code></pre></p>"},{"location":"dev/environment/#configuracao-de-testes","title":"\ud83e\uddea Configura\u00e7\u00e3o de Testes","text":""},{"location":"dev/environment/#pytestini-avancado","title":"\ud83d\udccb pytest.ini Avan\u00e7ado","text":"<pre><code>[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = \n    -v\n    --strict-markers\n    --tb=short\n    --cov=src\n    --cov-report=term-missing\n    --cov-report=html\n    --cov-fail-under=85\n    --maxfail=5\n    --disable-warnings\n\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests  \n    unit: marks tests as unit tests\n    api: marks tests as API tests\n    ml: marks tests as ML tests\n    database: marks tests requiring database\n\nfilterwarnings =\n    ignore::UserWarning\n    ignore::DeprecationWarning\n    ignore::PendingDeprecationWarning\n\nlog_cli = true\nlog_cli_level = INFO\nlog_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s\nlog_cli_date_format = %Y-%m-%d %H:%M:%S\n</code></pre>"},{"location":"dev/environment/#configuracao-de-coverage","title":"\ud83d\udd27 Configura\u00e7\u00e3o de Coverage","text":"<pre><code># .coveragerc\n[run]\nsource = src\nomit = \n    */tests/*\n    */test_*\n    */__pycache__/*\n    */venv/*\n    */.venv/*\n    */migrations/*\n    */settings/*\n\n[report]\nexclude_lines =\n    pragma: no cover\n    def __repr__\n    raise AssertionError\n    raise NotImplementedError\n    if __name__ == .__main__.:\n    @abstract\n    @abstractmethod\n\nprecision = 2\nshow_missing = true\nskip_covered = false\n\n[html]\ndirectory = htmlcov\ntitle = Machine Learning Engineer Coverage Report\n</code></pre>"},{"location":"dev/environment/#docker-para-desenvolvimento","title":"\ud83d\udc33 Docker para Desenvolvimento","text":""},{"location":"dev/environment/#dockerfiledev","title":"\ud83d\udccb Dockerfile.dev","text":"<pre><code># Dockerfile.dev - Otimizado para desenvolvimento\nFROM python:3.12-slim\n\n# Instalar depend\u00eancias do sistema\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    git \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Configurar diret\u00f3rio de trabalho\nWORKDIR /app\n\n# Instalar Poetry\nRUN pip install poetry\n\n# Configurar Poetry para n\u00e3o criar venv (usar\u00e1 container)\nRUN poetry config virtualenvs.create false\n\n# Copiar arquivos de depend\u00eancias\nCOPY pyproject.toml poetry.lock ./\n\n# Instalar depend\u00eancias (incluindo dev)\nRUN poetry install\n\n# Copiar c\u00f3digo fonte\nCOPY . .\n\n# Expor porta\nEXPOSE 8000\n\n# Comando padr\u00e3o para desenvolvimento\nCMD [\"uvicorn\", \"src.routers.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n</code></pre>"},{"location":"dev/environment/#docker-composedevyml","title":"\ud83d\udd04 docker-compose.dev.yml","text":"<pre><code># docker-compose.dev.yml - Setup de desenvolvimento\nversion: '3.8'\n\nservices:\n  api:\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"8000:8000\"\n    volumes:\n      # Hot reload - c\u00f3digo em tempo real\n      - .:/app\n      # Cache Poetry\n      - poetry-cache:/root/.cache/pypoetry\n    environment:\n      - ENVIRONMENT=development\n      - LOG_LEVEL=DEBUG\n      - DATABASE_URL=mongodb://mongodb:27017/flight_predictions\n    depends_on:\n      - mongodb\n    networks:\n      - ml-dev-network\n\n  mongodb:\n    image: mongo:7.0\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongodb-dev-data:/data/db\n      - ./docker/mongo-init:/docker-entrypoint-initdb.d:ro\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=devuser\n      - MONGO_INITDB_ROOT_PASSWORD=devpass\n      - MONGO_INITDB_DATABASE=flight_predictions\n    networks:\n      - ml-dev-network\n\n  jupyter:\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - .:/app\n      - jupyter-data:/root/.jupyter\n    command: &gt;\n      bash -c \"\n        pip install jupyterlab &amp;&amp;\n        jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n          --NotebookApp.token='' --NotebookApp.password=''\n      \"\n    networks:\n      - ml-dev-network\n\nvolumes:\n  mongodb-dev-data:\n  poetry-cache:\n  jupyter-data:\n\nnetworks:\n  ml-dev-network:\n    driver: bridge\n</code></pre> <p>Comandos de desenvolvimento: <pre><code># Iniciar ambiente completo de dev\ndocker-compose -f docker-compose.dev.yml up --build\n\n# Apenas API\ndocker-compose -f docker-compose.dev.yml up api\n\n# Apenas Jupyter\ndocker-compose -f docker-compose.dev.yml up jupyter\n\n# Logs em tempo real\ndocker-compose -f docker-compose.dev.yml logs -f api\n</code></pre></p>"},{"location":"dev/environment/#debugging-e-profiling","title":"\ud83d\udcca Debugging e Profiling","text":""},{"location":"dev/environment/#configuracao-de-debug","title":"\ud83d\udd0d Configura\u00e7\u00e3o de Debug","text":"<pre><code># debug_config.py\nimport logging\nimport sys\nfrom typing import Any, Dict\n\ndef setup_debug_logging():\n    \"\"\"Configura\u00e7\u00e3o de logging para debug\"\"\"\n\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',\n        handlers=[\n            logging.StreamHandler(sys.stdout),\n            logging.FileHandler('debug.log')\n        ]\n    )\n\n    # Configurar loggers espec\u00edficos\n    logging.getLogger('uvicorn').setLevel(logging.INFO)\n    logging.getLogger('fastapi').setLevel(logging.DEBUG)\n    logging.getLogger('sklearn').setLevel(logging.WARNING)\n\ndef debug_request(request_data: Dict[str, Any]) -&gt; None:\n    \"\"\"Helper para debug de requests\"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(f\"Request received: {request_data}\")\n\n    # Validar estrutura\n    if isinstance(request_data, dict):\n        logger.debug(f\"Request keys: {list(request_data.keys())}\")\n\n        if 'features' in request_data:\n            features = request_data['features']\n            logger.debug(f\"Features type: {type(features)}\")\n            logger.debug(f\"Features content: {features}\")\n\nclass PerformanceProfiler:\n    \"\"\"Profiler para an\u00e1lise de performance\"\"\"\n\n    def __init__(self):\n        self.timings = {}\n\n    def __enter__(self):\n        import time\n        self.start_time = time.time()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        import time\n        self.end_time = time.time()\n        self.duration = self.end_time - self.start_time\n\n    def time_function(self, func_name: str, func, *args, **kwargs):\n        \"\"\"Cronometrar execu\u00e7\u00e3o de fun\u00e7\u00e3o\"\"\"\n        import time\n\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n\n        self.timings[func_name] = end - start\n\n        logger = logging.getLogger(__name__)\n        logger.debug(f\"{func_name} executado em {end - start:.4f}s\")\n\n        return result\n\n# Uso:\n# with PerformanceProfiler() as profiler:\n#     # c\u00f3digo a ser medido\n#     pass\n# print(f\"Execu\u00e7\u00e3o levou {profiler.duration:.4f}s\")\n</code></pre>"},{"location":"dev/environment/#memory-profiling","title":"\ud83d\udcca Memory Profiling","text":"<pre><code># memory_profiler.py\nimport psutil\nimport tracemalloc\nfrom typing import Dict, Any\nimport logging\n\nclass MemoryProfiler:\n    \"\"\"Profiler de mem\u00f3ria para debug\"\"\"\n\n    def __init__(self):\n        self.process = psutil.Process()\n        self.initial_memory = None\n\n    def start_profiling(self):\n        \"\"\"Inicia profiling de mem\u00f3ria\"\"\"\n        tracemalloc.start()\n        self.initial_memory = self.process.memory_info().rss\n\n    def get_memory_usage(self) -&gt; Dict[str, Any]:\n        \"\"\"Retorna uso atual de mem\u00f3ria\"\"\"\n        current, peak = tracemalloc.get_traced_memory()\n        process_memory = self.process.memory_info().rss\n\n        return {\n            \"current_tracemalloc_mb\": current / 1024 / 1024,\n            \"peak_tracemalloc_mb\": peak / 1024 / 1024,\n            \"process_memory_mb\": process_memory / 1024 / 1024,\n            \"memory_increase_mb\": (process_memory - self.initial_memory) / 1024 / 1024 if self.initial_memory else 0\n        }\n\n    def log_top_memory_usage(self, limit: int = 10):\n        \"\"\"Log dos maiores consumidores de mem\u00f3ria\"\"\"\n        snapshot = tracemalloc.take_snapshot()\n        top_stats = snapshot.statistics('lineno')\n\n        logger = logging.getLogger(__name__)\n        logger.info(f\"Top {limit} memory consumers:\")\n\n        for index, stat in enumerate(top_stats[:limit], 1):\n            logger.info(f\"{index}. {stat}\")\n\n# Decorator para profiling autom\u00e1tico\ndef profile_memory(func):\n    \"\"\"Decorator para profiling de mem\u00f3ria\"\"\"\n    def wrapper(*args, **kwargs):\n        profiler = MemoryProfiler()\n        profiler.start_profiling()\n\n        try:\n            result = func(*args, **kwargs)\n            return result\n        finally:\n            memory_info = profiler.get_memory_usage()\n            logger = logging.getLogger(__name__)\n            logger.info(f\"{func.__name__} memory usage: {memory_info}\")\n\n    return wrapper\n</code></pre>"},{"location":"dev/environment/#workflow-de-desenvolvimento","title":"\ud83d\udd04 Workflow de Desenvolvimento","text":""},{"location":"dev/environment/#gitflow-simplificado","title":"\ud83d\udccb Gitflow Simplificado","text":"<pre><code># 1. Configurar reposit\u00f3rio\ngit config user.name \"Seu Nome\"\ngit config user.email \"seu.email@exemplo.com\"\n\n# 2. Criar branch para feature\ngit checkout -b feature/nova-funcionalidade\n\n# 3. Fazer altera\u00e7\u00f5es e commits frequentes\ngit add .\ngit commit -m \"feat: adiciona nova funcionalidade\"\n\n# 4. Push da branch\ngit push -u origin feature/nova-funcionalidade\n\n# 5. Criar Pull Request no GitHub\n# 6. Ap\u00f3s aprova\u00e7\u00e3o, merge via GitHub\n# 7. Limpar branch local\ngit checkout main\ngit pull origin main\ngit branch -d feature/nova-funcionalidade\n</code></pre>"},{"location":"dev/environment/#conventional-commits","title":"\ud83c\udfaf Conventional Commits","text":"<p>Padr\u00e3o de mensagens de commit:</p> <pre><code># Tipos de commit\nfeat: nova funcionalidade\nfix: corre\u00e7\u00e3o de bug  \ndocs: documenta\u00e7\u00e3o\nstyle: formata\u00e7\u00e3o\nrefactor: refatora\u00e7\u00e3o\ntest: testes\nchore: tarefas de manuten\u00e7\u00e3o\n\n# Exemplos\ngit commit -m \"feat: adiciona endpoint de predi\u00e7\u00e3o em lote\"\ngit commit -m \"fix: corrige valida\u00e7\u00e3o de datas no modelo\"\ngit commit -m \"docs: atualiza documenta\u00e7\u00e3o da API\"\ngit commit -m \"test: adiciona testes para servi\u00e7o de ML\"\n</code></pre>"},{"location":"dev/environment/#desenvolvimento-iterativo","title":"\ud83d\udd04 Desenvolvimento Iterativo","text":"<pre><code># Ciclo t\u00edpico de desenvolvimento\npoetry shell                    # Ativar ambiente\ntask test                      # Executar testes\n# Fazer altera\u00e7\u00f5es no c\u00f3digo\ntask format                    # Formatar c\u00f3digo\ntask lint                      # Verificar qualidade\ntask test                      # Testar novamente\nuvicorn src.routers.main:app --reload  # Testar API\ngit add . &amp;&amp; git commit -m \"...\"       # Commit\n</code></pre>"},{"location":"dev/environment/#monitoramento-local","title":"\ud83d\udcca Monitoramento Local","text":""},{"location":"dev/environment/#metricas-de-desenvolvimento","title":"\ud83d\udcc8 M\u00e9tricas de Desenvolvimento","text":"<pre><code># dev_metrics.py\nimport time\nimport psutil\nfrom typing import Dict, Any\nfrom datetime import datetime\n\nclass DevMetrics:\n    \"\"\"M\u00e9tricas para desenvolvimento local\"\"\"\n\n    def __init__(self):\n        self.start_time = time.time()\n        self.request_count = 0\n        self.error_count = 0\n\n    def record_request(self, processing_time: float):\n        \"\"\"Registra m\u00e9trica de request\"\"\"\n        self.request_count += 1\n\n        if hasattr(self, 'processing_times'):\n            self.processing_times.append(processing_time)\n        else:\n            self.processing_times = [processing_time]\n\n    def record_error(self):\n        \"\"\"Registra erro\"\"\"\n        self.error_count += 1\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Retorna sum\u00e1rio das m\u00e9tricas\"\"\"\n        uptime = time.time() - self.start_time\n\n        summary = {\n            \"uptime_seconds\": uptime,\n            \"total_requests\": self.request_count,\n            \"total_errors\": self.error_count,\n            \"error_rate\": self.error_count / max(self.request_count, 1),\n            \"requests_per_minute\": self.request_count / (uptime / 60) if uptime &gt; 0 else 0\n        }\n\n        if hasattr(self, 'processing_times') and self.processing_times:\n            summary.update({\n                \"avg_processing_time\": sum(self.processing_times) / len(self.processing_times),\n                \"min_processing_time\": min(self.processing_times),\n                \"max_processing_time\": max(self.processing_times)\n            })\n\n        # M\u00e9tricas do sistema\n        summary.update({\n            \"cpu_percent\": psutil.cpu_percent(),\n            \"memory_percent\": psutil.virtual_memory().percent,\n            \"disk_percent\": psutil.disk_usage('/').percent\n        })\n\n        return summary\n\n# Inst\u00e2ncia global\ndev_metrics = DevMetrics()\n</code></pre>"},{"location":"dev/environment/#scripts-uteis","title":"\ud83d\ude80 Scripts \u00dateis","text":""},{"location":"dev/environment/#setup_devsh","title":"\ud83d\udccb setup_dev.sh","text":"<pre><code>#!/bin/bash\n# setup_dev.sh - Script de setup completo\n\nset -e\n\necho \"\ud83d\ude80 Configurando ambiente de desenvolvimento...\"\n\n# Verificar Python\npython_version=$(python3 --version 2&gt;&amp;1 | awk '{print $2}')\necho \"Python version: $python_version\"\n\n# Instalar Poetry se n\u00e3o existir\nif ! command -v poetry &amp;&gt; /dev/null; then\n    echo \"\ud83d\udce6 Instalando Poetry...\"\n    curl -sSL https://install.python-poetry.org | python3 -\nfi\n\n# Configurar Poetry\necho \"\u2699\ufe0f Configurando Poetry...\"\npoetry config virtualenvs.in-project true\n\n# Instalar depend\u00eancias\necho \"\ud83d\udcda Instalando depend\u00eancias...\"\npoetry install\n\n# Configurar pre-commit\necho \"\ud83d\udd27 Configurando pre-commit...\"\npoetry run pre-commit install\n\n# Executar testes iniciais\necho \"\ud83e\uddea Executando testes...\"\npoetry run task test\n\n# Verificar formata\u00e7\u00e3o\necho \"\ud83c\udfa8 Verificando formata\u00e7\u00e3o...\"\npoetry run task format\n\necho \"\u2705 Ambiente configurado com sucesso!\"\necho \"\"\necho \"\ud83d\udccb Pr\u00f3ximos passos:\"\necho \"1. poetry shell (ativar ambiente)\"\necho \"2. task api (iniciar API)\"\necho \"3. task docs (visualizar docs)\"\n</code></pre>"},{"location":"dev/environment/#check_healthpy","title":"\ud83d\udcca check_health.py","text":"<pre><code>#!/usr/bin/env python3\n# check_health.py - Script de verifica\u00e7\u00e3o de sa\u00fade\n\nimport requests\nimport json\nimport sys\nfrom datetime import datetime\n\ndef check_api_health():\n    \"\"\"Verifica sa\u00fade da API\"\"\"\n    try:\n        response = requests.get(\"http://localhost:8000/health\", timeout=5)\n\n        if response.status_code == 200:\n            data = response.json()\n            print(\"\u2705 API est\u00e1 funcionando\")\n            print(f\"Status: {data.get('status', 'unknown')}\")\n            return True\n        else:\n            print(f\"\u274c API retornou status {response.status_code}\")\n            return False\n\n    except requests.exceptions.ConnectionError:\n        print(\"\u274c N\u00e3o foi poss\u00edvel conectar \u00e0 API\")\n        return False\n    except Exception as e:\n        print(f\"\u274c Erro ao verificar API: {e}\")\n        return False\n\ndef check_dependencies():\n    \"\"\"Verifica depend\u00eancias Python\"\"\"\n    try:\n        import fastapi\n        import pandas\n        import sklearn\n        print(\"\u2705 Depend\u00eancias principais OK\")\n        return True\n    except ImportError as e:\n        print(f\"\u274c Depend\u00eancia faltando: {e}\")\n        return False\n\ndef main():\n    \"\"\"Verifica\u00e7\u00e3o completa de sa\u00fade\"\"\"\n    print(f\"\ud83d\udd0d Verifica\u00e7\u00e3o de sa\u00fade - {datetime.now()}\")\n    print(\"=\" * 50)\n\n    checks = [\n        (\"Depend\u00eancias Python\", check_dependencies),\n        (\"API Health\", check_api_health)\n    ]\n\n    all_passed = True\n\n    for check_name, check_func in checks:\n        print(f\"\\n{check_name}:\")\n        if not check_func():\n            all_passed = False\n\n    print(\"\\n\" + \"=\" * 50)\n    if all_passed:\n        print(\"\u2705 Todas as verifica\u00e7\u00f5es passaram!\")\n        sys.exit(0)\n    else:\n        print(\"\u274c Algumas verifica\u00e7\u00f5es falharam!\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"dev/environment/#recursos-adicionais","title":"\ud83d\udcda Recursos Adicionais","text":""},{"location":"dev/environment/#documentacao-util","title":"\ud83d\udcd6 Documenta\u00e7\u00e3o \u00datil","text":"<ul> <li>FastAPI: https://fastapi.tiangolo.com/</li> <li>Poetry: https://python-poetry.org/docs/</li> <li>Pytest: https://docs.pytest.org/</li> <li>Black: https://black.readthedocs.io/</li> <li>Ruff: https://docs.astral.sh/ruff/</li> </ul>"},{"location":"dev/environment/#shortcuts-uteis","title":"\ud83c\udfaf Shortcuts \u00dateis","text":"<pre><code># Aliases \u00fateis para .bashrc/.zshrc\nalias pshell=\"poetry shell\"\nalias ptest=\"poetry run task test\"\nalias pformat=\"poetry run task format\"\nalias papi=\"poetry run task api\"\nalias pdocs=\"poetry run task docs\"\n\n# Git aliases\nalias gst=\"git status\"\nalias gco=\"git checkout\"\nalias gcb=\"git checkout -b\"\nalias gp=\"git push\"\nalias gl=\"git pull\"\n</code></pre>"},{"location":"dev/environment/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udd27 Troubleshooting - Solu\u00e7\u00e3o de problemas</li> <li>\ud83d\ude80 Quick Start - Configura\u00e7\u00e3o inicial</li> <li>\ud83c\udfd7\ufe0f Arquitetura - Vis\u00e3o geral do sistema</li> <li>\ud83d\udc1b Issues - Reportar problemas</li> </ul>"},{"location":"dev/troubleshooting/","title":"\ud83d\udd27 Troubleshooting","text":"<p>Guia completo para resolu\u00e7\u00e3o de problemas comuns do projeto Machine Learning Engineer Challenge.</p>"},{"location":"dev/troubleshooting/#problemas-mais-comuns","title":"\ud83d\udea8 Problemas Mais Comuns","text":""},{"location":"dev/troubleshooting/#problemas-com-python","title":"\ud83d\udc0d Problemas com Python","text":""},{"location":"dev/troubleshooting/#versao-incorreta-do-python","title":"\u274c Vers\u00e3o incorreta do Python","text":"<p>Sintoma: <pre><code>python --version\n# Python 3.11.x ou outra vers\u00e3o\n</code></pre></p> <p>Solu\u00e7\u00f5es:</p> Pyenv (Recomendado)PoetryManual <pre><code># Instalar Python 3.12.7\npyenv install 3.12.7\npyenv local 3.12.7\n\n# Verificar\npython --version\n# Esperado: Python 3.12.7\n</code></pre> <pre><code># For\u00e7ar Poetry a usar vers\u00e3o correta\npoetry env use 3.12.7\npoetry env info\n</code></pre> <pre><code># Windows: baixar de python.org\n# Linux: usar apt/yum\n# macOS: usar homebrew\n</code></pre>"},{"location":"dev/troubleshooting/#poetry-nao-reconhece-python","title":"\u274c Poetry n\u00e3o reconhece Python","text":"<p>Sintoma: <pre><code>poetry env use 3.12.7\n# The specified Python version is not available\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Verificar caminhos dispon\u00edveis\nwhich python3.12\nwhere python  # Windows\n\n# 2. Usar caminho completo\npoetry env use /usr/bin/python3.12  # Linux/macOS\npoetry env use C:\\Python312\\python.exe  # Windows\n\n# 3. Reinstalar Poetry\npip uninstall poetry\ncurl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-com-poetry","title":"\ud83d\udce6 Problemas com Poetry","text":""},{"location":"dev/troubleshooting/#conflitos-de-dependencias","title":"\u274c Conflitos de depend\u00eancias","text":"<p>Sintoma: <pre><code>poetry install\n# Solving dependencies... (this may take a minute)\n# Because project depends on package A (^1.0.0) and package B (^2.0.0),\n# version solving failed.\n</code></pre></p> <p>Solu\u00e7\u00f5es:</p> Limpar CacheAtualizar Depend\u00eanciasLock File <pre><code># Limpar cache do Poetry\npoetry cache clear pypi --all\n\n# Remover ambiente virtual\npoetry env remove python\n\n# Reinstalar\npoetry install\n</code></pre> <pre><code># Atualizar pyproject.toml\npoetry update\n\n# Ou atualizar package espec\u00edfico\npoetry update fastapi\n</code></pre> <pre><code># Deletar lock file e recriar\nrm poetry.lock\npoetry install\n</code></pre>"},{"location":"dev/troubleshooting/#ambiente-virtual-corrompido","title":"\u274c Ambiente virtual corrompido","text":"<p>Sintoma: <pre><code>poetry shell\n# Virtual environment is corrupted\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Remover ambiente completamente\npoetry env remove --all\n\n# 2. Recriar ambiente\npoetry env use 3.12.7\npoetry install\n\n# 3. Verificar\npoetry env info\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-com-a-api","title":"\u26a1 Problemas com a API","text":""},{"location":"dev/troubleshooting/#api-nao-inicia","title":"\u274c API n\u00e3o inicia","text":"<p>Sintomas: <pre><code>uvicorn src.routers.main:app --reload\n# ModuleNotFoundError: No module named 'src'\n# OU\n# Error loading ASGI app\n</code></pre></p> <p>Solu\u00e7\u00f5es:</p> PYTHONPATHImportsPorta Ocupada <pre><code># Executar do diret\u00f3rio raiz\ncd machine_learning_engineer\n\n# Verificar estrutura\nls src/\n\n# Executar\npoetry run uvicorn src.routers.main:app --reload\n</code></pre> <pre><code># Verificar se __init__.py existe\ntouch src/__init__.py\ntouch src/routers/__init__.py\n\n# Testar imports\npoetry run python -c \"from src.routers.main import app; print('OK')\"\n</code></pre> <pre><code># Verificar porta 8000\nnetstat -an | grep :8000  # Linux/macOS\nnetstat -an | findstr :8000  # Windows\n\n# Usar porta diferente\nuvicorn src.routers.main:app --port 8001 --reload\n</code></pre>"},{"location":"dev/troubleshooting/#erro-500-na-api","title":"\u274c Erro 500 na API","text":"<p>Sintomas: <pre><code>curl http://localhost:8000/health\n# {\"detail\": \"Internal Server Error\"}\n</code></pre></p> <p>Diagn\u00f3stico: <pre><code># 1. Verificar logs do uvicorn\n# Os logs aparecem no terminal onde uvicorn est\u00e1 executando\n\n# 2. Testar imports manualmente\npoetry run python -c \"\nfrom src.services.database import get_database\ndb = get_database()\nprint('Database OK')\n\"\n\n# 3. Verificar modelo\nls -la model/\npoetry run python -c \"\nimport pickle\nwith open('model/modelo_arvore_decisao.pkl', 'rb') as f:\n    model = pickle.load(f)\nprint('Model OK')\n\"\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-com-testes","title":"\ud83e\uddea Problemas com Testes","text":""},{"location":"dev/troubleshooting/#testes-falham-por-imports","title":"\u274c Testes falham por imports","text":"<p>Sintoma: <pre><code>pytest\n# ModuleNotFoundError: No module named 'src'\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Executar do diret\u00f3rio raiz\ncd machine_learning_engineer\n\n# 2. Usar Poetry\npoetry run pytest\n\n# 3. Verificar PYTHONPATH no pytest.ini\ncat pytest.ini\n# testpaths = tests\n# python_paths = .\n</code></pre></p>"},{"location":"dev/troubleshooting/#testes-falham-por-dependencias","title":"\u274c Testes falham por depend\u00eancias","text":"<p>Sintoma: <pre><code>pytest tests/test_ml_pipeline.py\n# ImportError: No module named 'sklearn'\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Instalar depend\u00eancias de teste\npoetry install\n\n# 2. Verificar se depend\u00eancias est\u00e3o instaladas\npoetry run pip list | grep scikit-learn\n\n# 3. Testes com skip condicional\npytest tests/test_ml_pipeline.py -v\n# Deve mostrar SKIPPED para m\u00f3dulos n\u00e3o dispon\u00edveis\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-com-docker","title":"\ud83d\udc33 Problemas com Docker","text":""},{"location":"dev/troubleshooting/#docker-build-falha","title":"\u274c Docker build falha","text":"<p>Sintomas: <pre><code>docker build -t ml-engineer-api .\n# Error: failed to solve: process \"/bin/sh -c pip install poetry\" did not complete successfully\n</code></pre></p> <p>Solu\u00e7\u00f5es:</p> Cache e NetworkMulti-stage DebugDockerfile Simplificado <pre><code># Build sem cache\ndocker build --no-cache -t ml-engineer-api .\n\n# Verificar rede\ndocker run --rm alpine ping google.com\n</code></pre> <pre><code># Build at\u00e9 est\u00e1gio espec\u00edfico\ndocker build --target base -t ml-engineer-base .\n\n# Entrar no container para debug\ndocker run -it ml-engineer-base bash\n</code></pre> <pre><code># Dockerfile.simple\nFROM python:3.12-slim\n\nWORKDIR /app\n\n# Instalar depend\u00eancias diretamente\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"src.routers.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"dev/troubleshooting/#docker-compose-falha","title":"\u274c Docker compose falha","text":"<p>Sintomas: <pre><code>docker-compose up\n# Error: service 'api' failed to build\n</code></pre></p> <p>Diagn\u00f3stico: <pre><code># 1. Verificar compose file\ndocker-compose config\n\n# 2. Build individual\ndocker-compose build api\n\n# 3. Logs detalhados\ndocker-compose up --build --verbose\n\n# 4. Verificar volumes\ndocker volume ls\ndocker volume inspect ml_mongodb_data\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-com-dados","title":"\ud83d\udcca Problemas com Dados","text":""},{"location":"dev/troubleshooting/#arquivo-de-dados-nao-encontrado","title":"\u274c Arquivo de dados n\u00e3o encontrado","text":"<p>Sintomas: <pre><code>pd.read_json('data/input/voos.json')\n# FileNotFoundError: [Errno 2] No such file or directory\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Verificar estrutura de dados\nls -la data/input/\n\n# 2. Verificar diret\u00f3rio atual\npwd\n\n# 3. Usar caminhos absolutos\nimport os\ndata_path = os.path.join(os.getcwd(), 'data', 'input', 'voos.json')\n</code></pre></p>"},{"location":"dev/troubleshooting/#modelo-nao-carrega","title":"\u274c Modelo n\u00e3o carrega","text":"<p>Sintomas: <pre><code>with open('model/modelo_arvore_decisao.pkl', 'rb') as f:\n    model = pickle.load(f)\n# FileNotFoundError\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Verificar se modelo existe\nls -la model/\n\n# 2. Se n\u00e3o existe, treinar modelo\njupyter lab notebook/Model.ipynb\n\n# 3. Verificar compatibilidade de vers\u00e3o\npoetry run python -c \"\nimport sklearn\nprint(f'Scikit-learn version: {sklearn.__version__}')\n\"\n</code></pre></p>"},{"location":"dev/troubleshooting/#problemas-de-rede","title":"\ud83c\udf10 Problemas de Rede","text":""},{"location":"dev/troubleshooting/#mongodb-nao-conecta","title":"\u274c MongoDB n\u00e3o conecta","text":"<p>Sintomas: <pre><code>docker-compose up\n# api_1      | pymongo.errors.ServerSelectionTimeoutError: mongodb:27017: [Errno -2] Name or service not known\n</code></pre></p> <p>Solu\u00e7\u00f5es:</p> Network DebugCompose OrderHealth Check <pre><code># Verificar rede Docker\ndocker network ls\ndocker network inspect ml_default\n\n# Testar conectividade\ndocker-compose exec api ping mongodb\n</code></pre> <pre><code># docker-compose.yml\nservices:\n  api:\n    depends_on:\n      - mongodb\n    # ...\n  mongodb:\n    # ...\n</code></pre> <pre><code># Adicionar health check ao MongoDB\nmongodb:\n  image: mongo:7.0\n  healthcheck:\n    test: [\"CMD\", \"mongo\", \"--eval\", \"db.adminCommand('ping')\"]\n    interval: 10s\n    timeout: 5s\n    retries: 5\n</code></pre>"},{"location":"dev/troubleshooting/#problemas-com-notebooks","title":"\ud83d\udcdd Problemas com Notebooks","text":""},{"location":"dev/troubleshooting/#kernel-nao-encontrado","title":"\u274c Kernel n\u00e3o encontrado","text":"<p>Sintomas: <pre><code>jupyter lab\n# No kernel available for Python 3.12\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Instalar ipykernel no ambiente Poetry\npoetry add ipykernel\n\n# 2. Registrar kernel\npoetry run python -m ipykernel install --user --name ml-engineer\n\n# 3. Selecionar kernel correto no Jupyter\n# Kernel &gt; Change Kernel &gt; ml-engineer\n</code></pre></p>"},{"location":"dev/troubleshooting/#imports-falham-no-notebook","title":"\u274c Imports falham no notebook","text":"<p>Sintomas: <pre><code>from src.services.database import get_database\n# ModuleNotFoundError: No module named 'src'\n</code></pre></p> <p>Solu\u00e7\u00f5es: <pre><code># 1. Adicionar path no notebook\nimport sys\nimport os\nsys.path.append(os.path.abspath('..'))\n\n# 2. Ou usar PYTHONPATH\nimport os\nos.chdir('..')  # Se notebook est\u00e1 em subpasta\n</code></pre></p>"},{"location":"dev/troubleshooting/#comandos-de-diagnostico","title":"\ud83d\udd0d Comandos de Diagn\u00f3stico","text":""},{"location":"dev/troubleshooting/#verificacao-do-ambiente-python","title":"\ud83d\udc0d Verifica\u00e7\u00e3o do Ambiente Python","text":"<pre><code># Informa\u00e7\u00f5es completas do ambiente\npoetry env info\n\n# Vers\u00e3o do Python\npython --version\n\n# Localiza\u00e7\u00e3o do execut\u00e1vel\nwhich python  # Linux/macOS\nwhere python  # Windows\n\n# Pacotes instalados\npoetry show\n\n# Verificar imports cr\u00edticos\npoetry run python -c \"\nimport sys\nprint(f'Python: {sys.version}')\n\ntry:\n    import fastapi\n    print(f'FastAPI: {fastapi.__version__}')\nexcept ImportError as e:\n    print(f'FastAPI Error: {e}')\n\ntry:\n    import pandas\n    print(f'Pandas: {pandas.__version__}')\nexcept ImportError as e:\n    print(f'Pandas Error: {e}')\n\ntry:\n    import sklearn\n    print(f'Sklearn: {sklearn.__version__}')\nexcept ImportError as e:\n    print(f'Sklearn Error: {e}')\n\"\n</code></pre>"},{"location":"dev/troubleshooting/#verificacao-de-sistema","title":"\ud83d\udd27 Verifica\u00e7\u00e3o de Sistema","text":"<pre><code># Informa\u00e7\u00f5es do sistema\nuname -a  # Linux/macOS\nsysteminfo  # Windows\n\n# Espa\u00e7o em disco\ndf -h  # Linux/macOS\ndir  # Windows\n\n# Mem\u00f3ria dispon\u00edvel\nfree -h  # Linux\ntop  # macOS\ntasklist  # Windows\n\n# Processos Python\nps aux | grep python  # Linux/macOS\ntasklist | findstr python  # Windows\n</code></pre>"},{"location":"dev/troubleshooting/#verificacao-de-rede","title":"\ud83c\udf10 Verifica\u00e7\u00e3o de Rede","text":"<pre><code># Testar conectividade\nping google.com\n\n# Portas em uso\nnetstat -tuln  # Linux\nnetstat -an  # Windows/macOS\n\n# Testar porta espec\u00edfica\ntelnet localhost 8000\n\n# Processos usando porta\nlsof -i :8000  # Linux/macOS\nnetstat -ano | findstr :8000  # Windows\n</code></pre>"},{"location":"dev/troubleshooting/#solucoes-rapidas","title":"\ud83d\udea8 Solu\u00e7\u00f5es R\u00e1pidas","text":""},{"location":"dev/troubleshooting/#reset-completo-do-ambiente","title":"\u26a1 Reset Completo do Ambiente","text":"<pre><code># 1. Limpar tudo\npoetry env remove --all\nrm -rf .venv/\nrm poetry.lock\n\n# 2. Recriar ambiente\npoetry env use 3.12.7\npoetry install\n\n# 3. Testar\npoetry run python -c \"print('Environment OK')\"\npoetry run task test\n</code></pre>"},{"location":"dev/troubleshooting/#reset-completo-do-docker","title":"\ud83d\udc33 Reset Completo do Docker","text":"<pre><code># 1. Parar tudo\ndocker-compose down --volumes --remove-orphans\n\n# 2. Limpar sistema\ndocker system prune -a\ndocker volume prune\n\n# 3. Rebuild\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre>"},{"location":"dev/troubleshooting/#verificacao-completa-do-projeto","title":"\ud83d\udcca Verifica\u00e7\u00e3o Completa do Projeto","text":"<pre><code>#!/bin/bash\n# check_project.sh\n\necho \"\ud83d\udd0d Verificando projeto...\"\n\n# Python e Poetry\necho \"1. Python e Poetry:\"\npoetry --version\npoetry env info\n\n# Depend\u00eancias\necho \"2. Depend\u00eancias cr\u00edticas:\"\npoetry run python -c \"\nimport fastapi, pandas, sklearn\nprint('\u2705 Depend\u00eancias OK')\n\"\n\n# Estrutura de arquivos\necho \"3. Estrutura de arquivos:\"\nls -la src/ data/ tests/ model/\n\n# Testes\necho \"4. Testes r\u00e1pidos:\"\npoetry run pytest tests/ -x --tb=short\n\n# API\necho \"5. Testando API:\"\npoetry run python -c \"\nfrom fastapi.testclient import TestClient\nfrom src.routers.main import app\nclient = TestClient(app)\nresponse = client.get('/health')\nprint(f'Health check: {response.status_code}')\n\"\n\necho \"\u2705 Verifica\u00e7\u00e3o completa!\"\n</code></pre>"},{"location":"dev/troubleshooting/#logs-e-debugging","title":"\ud83d\udcda Logs e Debugging","text":""},{"location":"dev/troubleshooting/#configuracao-de-logs","title":"\ud83d\udccb Configura\u00e7\u00e3o de Logs","text":"<pre><code># logging_config.py\nimport logging\nimport sys\n\ndef setup_logging(level=logging.INFO):\n    \"\"\"Configurar logs para debugging\"\"\"\n\n    logging.basicConfig(\n        level=level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.FileHandler('app.log'),\n            logging.StreamHandler(sys.stdout)\n        ]\n    )\n\n    # Logs espec\u00edficos\n    logging.getLogger('uvicorn').setLevel(logging.INFO)\n    logging.getLogger('fastapi').setLevel(logging.DEBUG)\n\n# Uso nos m\u00f3dulos\nimport logging\nlogger = logging.getLogger(__name__)\nlogger.info(\"Debugging info here\")\n</code></pre>"},{"location":"dev/troubleshooting/#debug-da-api","title":"\ud83d\udd0d Debug da API","text":"<pre><code># src/routers/main.py\nimport logging\nfrom fastapi import FastAPI, Request\n\napp = FastAPI()\n\n@app.middleware(\"http\")\nasync def log_requests(request: Request, call_next):\n    \"\"\"Log de todas as requests\"\"\"\n    start_time = time.time()\n\n    logger.info(f\"Request: {request.method} {request.url}\")\n\n    response = await call_next(request)\n\n    process_time = time.time() - start_time\n    logger.info(f\"Response: {response.status_code} ({process_time:.3f}s)\")\n\n    return response\n</code></pre>"},{"location":"dev/troubleshooting/#quando-pedir-ajuda","title":"\ud83d\udcde Quando Pedir Ajuda","text":""},{"location":"dev/troubleshooting/#criando-issues-efetivas","title":"\ud83d\udc1b Criando Issues Efetivas","text":"<p>Template de Issue: <pre><code>## \ud83d\udc1b Descri\u00e7\u00e3o do Problema\n[Descreva o problema claramente]\n\n## \ud83d\udd04 Passos para Reproduzir\n1. Primeiro passo\n2. Segundo passo\n3. Erro ocorre aqui\n\n## \ud83d\udccb Informa\u00e7\u00f5es do Ambiente\n- OS: [Windows/Linux/macOS]\n- Python: [vers\u00e3o]\n- Poetry: [vers\u00e3o]\n- Docker: [vers\u00e3o se aplic\u00e1vel]\n\n## \ud83d\udcca Logs/Screenshots\n</code></pre> [Cole logs ou screenshots relevantes] <pre><code>## \ud83d\udca1 Tentativas de Solu\u00e7\u00e3o\n[O que j\u00e1 foi tentado]\n</code></pre></p>"},{"location":"dev/troubleshooting/#contato-direto","title":"\ud83d\udce7 Contato Direto","text":"<ul> <li>\ud83d\udce7 Email: ulisses.bomjardim@gmail.com</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udcac Discuss\u00f5es: GitHub Discussions</li> </ul>"},{"location":"docker/compose/","title":"\ud83d\udc33 Docker Compose","text":"<p>Guia completo para orquestra\u00e7\u00e3o de containers usando Docker Compose, incluindo configura\u00e7\u00e3o de m\u00faltiplos servi\u00e7os, redes, volumes e ambientes de desenvolvimento/produ\u00e7\u00e3o.</p>"},{"location":"docker/compose/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Docker Compose permite definir e executar aplica\u00e7\u00f5es Docker com m\u00faltiplos containers de forma simples e reproduz\u00edvel. Esta se\u00e7\u00e3o detalha como configurar todo o stack da aplica\u00e7\u00e3o.</p>"},{"location":"docker/compose/#estrutura-dos-arquivos","title":"\ud83d\udcc1 Estrutura dos Arquivos","text":"<pre><code>docker/\n\u251c\u2500\u2500 docker-compose.yml          # Produ\u00e7\u00e3o\n\u251c\u2500\u2500 docker-compose.dev.yml      # Desenvolvimento  \n\u251c\u2500\u2500 docker-compose.test.yml     # Testes\n\u251c\u2500\u2500 .env.example               # Vari\u00e1veis de ambiente\n\u251c\u2500\u2500 nginx/\n\u2502   \u2514\u2500\u2500 nginx.conf            # Configura\u00e7\u00e3o Nginx\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 wait-for-it.sh        # Script para aguardar servi\u00e7os\n    \u2514\u2500\u2500 init-db.sh           # Inicializa\u00e7\u00e3o do banco\n</code></pre>"},{"location":"docker/compose/#docker-compose-principal","title":"\ud83d\ude80 Docker Compose Principal","text":""},{"location":"docker/compose/#docker-composeyml-producao","title":"\ud83d\udccb docker-compose.yml (Produ\u00e7\u00e3o)","text":"<pre><code>version: '3.8'\n\nservices:\n  # API Principal\n  api:\n    build:\n      context: .\n      dockerfile: docker/api.Dockerfile\n    container_name: flight_delay_api\n    restart: unless-stopped\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ENVIRONMENT=production\n      - DATABASE_URL=mongodb://mongodb:27017/flight_predictions\n      - REDIS_URL=redis://redis:6379/0\n      - MODEL_PATH=/app/model/\n      - LOG_LEVEL=INFO\n    volumes:\n      - ./model:/app/model:ro\n      - ./logs:/app/logs\n    depends_on:\n      - mongodb\n      - redis\n    networks:\n      - flight-network\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 40s\n\n  # Banco de Dados MongoDB\n  mongodb:\n    image: mongo:7.0\n    container_name: flight_delay_mongodb\n    restart: unless-stopped\n    ports:\n      - \"27017:27017\"\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=${MONGO_USERNAME:-admin}\n      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_PASSWORD:-password123}\n      - MONGO_INITDB_DATABASE=flight_predictions\n    volumes:\n      - mongodb_data:/data/db\n      - ./docker/scripts/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro\n    networks:\n      - flight-network\n    healthcheck:\n      test: echo 'db.runCommand(\"ping\").ok' | mongosh localhost:27017/flight_predictions --quiet\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # Cache Redis\n  redis:\n    image: redis:7-alpine\n    container_name: flight_delay_redis\n    restart: unless-stopped\n    ports:\n      - \"6379:6379\"\n    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-redis123}\n    volumes:\n      - redis_data:/data\n    networks:\n      - flight-network\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # Reverse Proxy Nginx\n  nginx:\n    image: nginx:alpine\n    container_name: flight_delay_nginx\n    restart: unless-stopped\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./docker/nginx/ssl:/etc/nginx/ssl:ro\n      - nginx_logs:/var/log/nginx\n    depends_on:\n      - api\n    networks:\n      - flight-network\n    healthcheck:\n      test: [\"CMD\", \"wget\", \"--quiet\", \"--tries=1\", \"--spider\", \"http://localhost/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # Worker para Processamento Ass\u00edncrono\n  worker:\n    build:\n      context: .\n      dockerfile: docker/api.Dockerfile\n    container_name: flight_delay_worker\n    restart: unless-stopped\n    command: celery -A src.worker worker --loglevel=info --concurrency=4\n    environment:\n      - ENVIRONMENT=production\n      - DATABASE_URL=mongodb://mongodb:27017/flight_predictions\n      - REDIS_URL=redis://redis:6379/0\n      - CELERY_BROKER_URL=redis://redis:6379/1\n      - CELERY_RESULT_BACKEND=redis://redis:6379/1\n    volumes:\n      - ./model:/app/model:ro\n      - ./logs:/app/logs\n    depends_on:\n      - mongodb\n      - redis\n    networks:\n      - flight-network\n\n  # Monitor Celery\n  flower:\n    build:\n      context: .\n      dockerfile: docker/api.Dockerfile\n    container_name: flight_delay_flower\n    restart: unless-stopped\n    command: celery -A src.worker flower --port=5555\n    ports:\n      - \"5555:5555\"\n    environment:\n      - CELERY_BROKER_URL=redis://redis:6379/1\n      - CELERY_RESULT_BACKEND=redis://redis:6379/1\n    depends_on:\n      - redis\n      - worker\n    networks:\n      - flight-network\n\n  # Monitoramento Prometheus\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: flight_delay_prometheus\n    restart: unless-stopped\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - prometheus_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n    networks:\n      - flight-network\n\n  # Dashboard Grafana\n  grafana:\n    image: grafana/grafana:latest\n    container_name: flight_delay_grafana\n    restart: unless-stopped\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./docker/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro\n      - ./docker/grafana/datasources:/etc/grafana/provisioning/datasources:ro\n    depends_on:\n      - prometheus\n    networks:\n      - flight-network\n\n# Volumes persistentes\nvolumes:\n  mongodb_data:\n    driver: local\n  redis_data:\n    driver: local\n  prometheus_data:\n    driver: local\n  grafana_data:\n    driver: local\n  nginx_logs:\n    driver: local\n\n# Redes\nnetworks:\n  flight-network:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 172.20.0.0/16\n</code></pre>"},{"location":"docker/compose/#ambiente-de-desenvolvimento","title":"\ud83d\udd27 Ambiente de Desenvolvimento","text":""},{"location":"docker/compose/#docker-composedevyml","title":"\ud83d\udccb docker-compose.dev.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  # API com hot reload\n  api-dev:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.dev\n    container_name: flight_delay_api_dev\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ENVIRONMENT=development\n      - DATABASE_URL=mongodb://mongodb-dev:27017/flight_predictions_dev\n      - REDIS_URL=redis://redis-dev:6379/0\n      - DEBUG=True\n      - LOG_LEVEL=DEBUG\n    volumes:\n      - .:/app:delegated\n      - /app/venv\n      - dev_cache:/root/.cache\n    depends_on:\n      - mongodb-dev\n      - redis-dev\n    networks:\n      - flight-dev-network\n    command: uvicorn src.routers.main:app --host 0.0.0.0 --port 8000 --reload --log-level debug\n\n  # MongoDB Desenvolvimento\n  mongodb-dev:\n    image: mongo:7.0\n    container_name: flight_delay_mongodb_dev\n    ports:\n      - \"27018:27017\"\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=devuser\n      - MONGO_INITDB_ROOT_PASSWORD=devpass\n      - MONGO_INITDB_DATABASE=flight_predictions_dev\n    volumes:\n      - mongodb_dev_data:/data/db\n      - ./data/sample:/docker-entrypoint-initdb.d:ro\n    networks:\n      - flight-dev-network\n\n  # Redis Desenvolvimento\n  redis-dev:\n    image: redis:7-alpine\n    container_name: flight_delay_redis_dev\n    ports:\n      - \"6380:6379\"\n    command: redis-server --appendonly yes\n    volumes:\n      - redis_dev_data:/data\n    networks:\n      - flight-dev-network\n\n  # Jupyter Lab para an\u00e1lise\n  jupyter:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.jupyter\n    container_name: flight_delay_jupyter\n    ports:\n      - \"8888:8888\"\n    environment:\n      - JUPYTER_ENABLE_LAB=yes\n      - JUPYTER_TOKEN=\n      - DATABASE_URL=mongodb://mongodb-dev:27017/flight_predictions_dev\n    volumes:\n      - .:/app:delegated\n      - jupyter_data:/home/jovyan/.jupyter\n    depends_on:\n      - mongodb-dev\n    networks:\n      - flight-dev-network\n    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''\n\n  # Adminer para gerenciar MongoDB\n  mongo-express:\n    image: mongo-express:latest\n    container_name: flight_delay_mongo_express\n    ports:\n      - \"8081:8081\"\n    environment:\n      - ME_CONFIG_MONGODB_SERVER=mongodb-dev\n      - ME_CONFIG_MONGODB_ADMINUSERNAME=devuser\n      - ME_CONFIG_MONGODB_ADMINPASSWORD=devpass\n      - ME_CONFIG_BASICAUTH_USERNAME=admin\n      - ME_CONFIG_BASICAUTH_PASSWORD=admin123\n    depends_on:\n      - mongodb-dev\n    networks:\n      - flight-dev-network\n\n  # Mailhog para testes de email\n  mailhog:\n    image: mailhog/mailhog:latest\n    container_name: flight_delay_mailhog\n    ports:\n      - \"1025:1025\"  # SMTP\n      - \"8025:8025\"  # Web UI\n    networks:\n      - flight-dev-network\n\nvolumes:\n  mongodb_dev_data:\n  redis_dev_data:\n  jupyter_data:\n  dev_cache:\n\nnetworks:\n  flight-dev-network:\n    driver: bridge\n</code></pre>"},{"location":"docker/compose/#ambiente-de-testes","title":"\ud83e\uddea Ambiente de Testes","text":""},{"location":"docker/compose/#docker-composetestyml","title":"\ud83d\udccb docker-compose.test.yml","text":"<pre><code>version: '3.8'\n\nservices:\n  # API para testes\n  api-test:\n    build:\n      context: .\n      dockerfile: docker/api.Dockerfile\n      target: test\n    container_name: flight_delay_api_test\n    environment:\n      - ENVIRONMENT=testing\n      - DATABASE_URL=mongodb://mongodb-test:27017/flight_predictions_test\n      - REDIS_URL=redis://redis-test:6379/0\n      - TESTING=True\n    volumes:\n      - .:/app:delegated\n      - test_coverage:/app/htmlcov\n    depends_on:\n      - mongodb-test\n      - redis-test\n    networks:\n      - flight-test-network\n    command: &gt;\n      bash -c \"\n        python -m pytest tests/ -v --cov=src --cov-report=html --cov-report=term-missing --junit-xml=test-results.xml &amp;&amp;\n        python -m pytest tests/integration/ -v --integration\n      \"\n\n  # MongoDB Testes (em mem\u00f3ria)\n  mongodb-test:\n    image: mongo:7.0\n    container_name: flight_delay_mongodb_test\n    environment:\n      - MONGO_INITDB_DATABASE=flight_predictions_test\n    tmpfs:\n      - /data/db\n    networks:\n      - flight-test-network\n\n  # Redis Testes\n  redis-test:\n    image: redis:7-alpine\n    container_name: flight_delay_redis_test\n    command: redis-server --save \"\"\n    tmpfs:\n      - /data\n    networks:\n      - flight-test-network\n\n  # Testes de performance\n  locust:\n    build:\n      context: .\n      dockerfile: docker/Dockerfile.locust\n    container_name: flight_delay_locust\n    ports:\n      - \"8089:8089\"\n    environment:\n      - LOCUST_HOST=http://api-test:8000\n    volumes:\n      - ./tests/performance:/app/tests\n    depends_on:\n      - api-test\n    networks:\n      - flight-test-network\n    command: locust -f /app/tests/locustfile.py --host=http://api-test:8000\n\nvolumes:\n  test_coverage:\n\nnetworks:\n  flight-test-network:\n    driver: bridge\n</code></pre>"},{"location":"docker/compose/#configuracoes-auxiliares","title":"\u2699\ufe0f Configura\u00e7\u00f5es Auxiliares","text":""},{"location":"docker/compose/#nginx-configuration","title":"\ud83c\udf10 Nginx Configuration","text":"<pre><code># docker/nginx/nginx.conf\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    upstream api_backend {\n        server api:8000;\n    }\n\n    # Rate Limiting\n    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;\n    limit_req_zone $binary_remote_addr zone=predict_limit:10m rate=2r/s;\n\n    # Logging\n    log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n                   '$status $body_bytes_sent \"$http_referer\" '\n                   '\"$http_user_agent\" \"$http_x_forwarded_for\" '\n                   'rt=$request_time uct=\"$upstream_connect_time\" '\n                   'uht=\"$upstream_header_time\" urt=\"$upstream_response_time\"';\n\n    access_log /var/log/nginx/access.log main;\n    error_log /var/log/nginx/error.log warn;\n\n    server {\n        listen 80;\n        server_name localhost;\n\n        # Security headers\n        add_header X-Frame-Options DENY;\n        add_header X-Content-Type-Options nosniff;\n        add_header X-XSS-Protection \"1; mode=block\";\n        add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\" always;\n\n        # Gzip compression\n        gzip on;\n        gzip_vary on;\n        gzip_min_length 1024;\n        gzip_types text/plain text/css application/json application/javascript text/xml application/xml;\n\n        # Health check\n        location /health {\n            proxy_pass http://api_backend/health;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n\n        # API endpoints with rate limiting\n        location /api/ {\n            limit_req zone=api_limit burst=20 nodelay;\n\n            proxy_pass http://api_backend/;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # Timeouts\n            proxy_connect_timeout 5s;\n            proxy_send_timeout 60s;\n            proxy_read_timeout 60s;\n        }\n\n        # Predict endpoint with stricter rate limiting\n        location /predict {\n            limit_req zone=predict_limit burst=5 nodelay;\n\n            proxy_pass http://api_backend/predict;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n\n            # Larger body size for batch predictions\n            client_max_body_size 10M;\n        }\n\n        # Static files (documenta\u00e7\u00e3o)\n        location /docs {\n            proxy_pass http://api_backend/docs;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n        }\n    }\n}\n</code></pre>"},{"location":"docker/compose/#prometheus-configuration","title":"\ud83d\udcca Prometheus Configuration","text":"<pre><code># docker/prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nrule_files:\n  - \"rules/*.yml\"\n\nscrape_configs:\n  # API Metrics\n  - job_name: 'flight-delay-api'\n    static_configs:\n      - targets: ['api:8000']\n    metrics_path: '/metrics'\n    scrape_interval: 10s\n\n  # System Metrics\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  # MongoDB Metrics\n  - job_name: 'mongodb-exporter'\n    static_configs:\n      - targets: ['mongodb-exporter:9216']\n\n  # Redis Metrics\n  - job_name: 'redis-exporter'\n    static_configs:\n      - targets: ['redis-exporter:9121']\n\n  # Nginx Metrics\n  - job_name: 'nginx-exporter'\n    static_configs:\n      - targets: ['nginx-exporter:9113']\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          - alertmanager:9093\n</code></pre>"},{"location":"docker/compose/#variaveis-de-ambiente","title":"\ud83d\udcdd Vari\u00e1veis de Ambiente","text":""},{"location":"docker/compose/#envexample","title":"\ud83d\udd27 .env.example","text":"<pre><code># Ambiente\nENVIRONMENT=production\n\n# Banco de Dados\nMONGO_USERNAME=admin\nMONGO_PASSWORD=secure_password_here\nDATABASE_URL=mongodb://mongodb:27017/flight_predictions\n\n# Cache\nREDIS_PASSWORD=redis_secure_password\nREDIS_URL=redis://redis:6379/0\n\n# Celery\nCELERY_BROKER_URL=redis://redis:6379/1\nCELERY_RESULT_BACKEND=redis://redis:6379/1\n\n# Monitoring\nGRAFANA_PASSWORD=grafana_admin_password\n\n# API Keys (se necess\u00e1rio)\nWEATHER_API_KEY=your_weather_api_key_here\nSENTRY_DSN=your_sentry_dsn_here\n\n# SSL (se usando HTTPS)\nSSL_CERT_PATH=/etc/nginx/ssl/cert.pem\nSSL_KEY_PATH=/etc/nginx/ssl/key.pem\n\n# Recursos\nAPI_WORKERS=4\nAPI_MAX_REQUESTS=1000\nAPI_MAX_REQUESTS_JITTER=100\n</code></pre>"},{"location":"docker/compose/#scripts-de-gerenciamento","title":"\ud83d\ude80 Scripts de Gerenciamento","text":""},{"location":"docker/compose/#makefile","title":"\ud83d\udccb Makefile","text":"<pre><code># Makefile para gerenciar Docker Compose\n\n.PHONY: help build up down logs ps clean test dev prod\n\n# Vari\u00e1veis\nCOMPOSE_FILE = docker-compose.yml\nCOMPOSE_DEV_FILE = docker-compose.dev.yml\nCOMPOSE_TEST_FILE = docker-compose.test.yml\n\nhelp: ## Mostra esta ajuda\n    @grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\n# Comandos de produ\u00e7\u00e3o\nbuild: ## Build das imagens para produ\u00e7\u00e3o\n    docker-compose -f $(COMPOSE_FILE) build --no-cache\n\nup: ## Inicia todos os servi\u00e7os em produ\u00e7\u00e3o\n    docker-compose -f $(COMPOSE_FILE) up -d\n\ndown: ## Para todos os servi\u00e7os\n    docker-compose -f $(COMPOSE_FILE) down\n\nrestart: ## Reinicia todos os servi\u00e7os\n    docker-compose -f $(COMPOSE_FILE) restart\n\nlogs: ## Mostra logs de todos os servi\u00e7os\n    docker-compose -f $(COMPOSE_FILE) logs -f\n\nps: ## Mostra status dos containers\n    docker-compose -f $(COMPOSE_FILE) ps\n\n# Comandos de desenvolvimento\ndev-build: ## Build para desenvolvimento\n    docker-compose -f $(COMPOSE_DEV_FILE) build\n\ndev-up: ## Inicia ambiente de desenvolvimento\n    docker-compose -f $(COMPOSE_DEV_FILE) up -d\n\ndev-down: ## Para ambiente de desenvolvimento\n    docker-compose -f $(COMPOSE_DEV_FILE) down\n\ndev-logs: ## Logs do ambiente de desenvolvimento\n    docker-compose -f $(COMPOSE_DEV_FILE) logs -f\n\n# Comandos de teste\ntest-build: ## Build para testes\n    docker-compose -f $(COMPOSE_TEST_FILE) build\n\ntest-up: ## Executa testes\n    docker-compose -f $(COMPOSE_TEST_FILE) up --abort-on-container-exit\n\ntest-down: ## Para ambiente de teste\n    docker-compose -f $(COMPOSE_TEST_FILE) down -v\n\n# Comandos de limpeza\nclean: ## Remove containers, networks e volumes n\u00e3o utilizados\n    docker system prune -f\n    docker volume prune -f\n    docker network prune -f\n\nclean-all: ## Remove tudo (CUIDADO!)\n    docker-compose -f $(COMPOSE_FILE) down -v --rmi all\n    docker-compose -f $(COMPOSE_DEV_FILE) down -v --rmi all\n    docker-compose -f $(COMPOSE_TEST_FILE) down -v --rmi all\n\n# Comandos de utilidade\nshell-api: ## Acessa shell do container da API\n    docker-compose -f $(COMPOSE_FILE) exec api bash\n\nshell-db: ## Acessa shell do MongoDB\n    docker-compose -f $(COMPOSE_FILE) exec mongodb mongosh\n\nbackup-db: ## Backup do banco de dados\n    docker-compose -f $(COMPOSE_FILE) exec mongodb mongodump --host localhost --port 27017 --out /data/backup\n\nrestore-db: ## Restaura backup do banco\n    docker-compose -f $(COMPOSE_FILE) exec mongodb mongorestore --host localhost --port 27017 /data/backup\n\n# Monitoramento\nstats: ## Estat\u00edsticas dos containers\n    docker stats $(shell docker-compose -f $(COMPOSE_FILE) ps -q)\n\nhealth: ## Verifica sa\u00fade dos servi\u00e7os\n    @echo \"=== API Health ===\"\n    @curl -s http://localhost/health | jq .\n    @echo \"\\n=== MongoDB Status ===\"\n    @docker-compose -f $(COMPOSE_FILE) exec mongodb mongosh --eval \"db.adminCommand('ping')\"\n    @echo \"\\n=== Redis Status ===\"\n    @docker-compose -f $(COMPOSE_FILE) exec redis redis-cli ping\n</code></pre>"},{"location":"docker/compose/#scripts-de-inicializacao","title":"\ud83d\udd27 Scripts de Inicializa\u00e7\u00e3o","text":"<pre><code>#!/bin/bash\n# docker/scripts/init-db.sh\n\nset -e\n\necho \"Inicializando banco de dados...\"\n\n# Aguardar MongoDB estar pronto\nuntil mongosh --host localhost --port 27017 --eval \"print(\\\"MongoDB is ready\\\")\"; do\n  echo \"Aguardando MongoDB...\"\n  sleep 2\ndone\n\n# Criar usu\u00e1rio da aplica\u00e7\u00e3o\nmongosh --host localhost --port 27017 &lt;&lt;EOF\nuse flight_predictions;\n\n// Criar usu\u00e1rio da aplica\u00e7\u00e3o\ndb.createUser({\n  user: \"app_user\",\n  pwd: \"app_password_123\",\n  roles: [\n    { role: \"readWrite\", db: \"flight_predictions\" }\n  ]\n});\n\n// Criar \u00edndices\ndb.predictions.createIndex({ \"flight_id\": 1 });\ndb.predictions.createIndex({ \"created_at\": 1 });\ndb.predictions.createIndex({ \"departure_airport\": 1, \"arrival_airport\": 1 });\n\n// Inserir dados de exemplo (opcional)\ndb.airports.insertMany([\n  {\n    \"icao_code\": \"SBGR\",\n    \"name\": \"S\u00e3o Paulo/Guarulhos\",\n    \"city\": \"S\u00e3o Paulo\",\n    \"country\": \"Brazil\",\n    \"latitude\": -23.4356,\n    \"longitude\": -46.4731,\n    \"altitude\": 750\n  },\n  {\n    \"icao_code\": \"SBRJ\", \n    \"name\": \"Rio de Janeiro/Santos Dumont\",\n    \"city\": \"Rio de Janeiro\",\n    \"country\": \"Brazil\",\n    \"latitude\": -22.9110,\n    \"longitude\": -43.1631,\n    \"altitude\": 3\n  }\n]);\n\nprint(\"Inicializa\u00e7\u00e3o do banco conclu\u00edda!\");\nEOF\n\necho \"Banco de dados inicializado com sucesso!\"\n</code></pre>"},{"location":"docker/compose/#comandos-uteis","title":"\ud83d\udcca Comandos \u00dateis","text":""},{"location":"docker/compose/#comandos-de-producao","title":"\ud83d\ude80 Comandos de Produ\u00e7\u00e3o","text":"<pre><code># Inicializar stack completo\ndocker-compose up -d\n\n# Verificar status\ndocker-compose ps\ndocker-compose logs -f api\n\n# Escalar servi\u00e7os\ndocker-compose up -d --scale worker=3\n\n# Atualizar apenas a API\ndocker-compose up -d --build api\n\n# Backup completo\ndocker-compose exec mongodb mongodump --archive | gzip &gt; backup_$(date +%Y%m%d).gz\n\n# Monitorar recursos\ndocker stats $(docker-compose ps -q)\n</code></pre>"},{"location":"docker/compose/#comandos-de-desenvolvimento","title":"\ud83d\udd27 Comandos de Desenvolvimento","text":"<pre><code># Ambiente de desenvolvimento\ndocker-compose -f docker-compose.dev.yml up -d\n\n# Logs em tempo real\ndocker-compose -f docker-compose.dev.yml logs -f api-dev\n\n# Executar testes\ndocker-compose -f docker-compose.test.yml up --abort-on-container-exit\n\n# Acessar Jupyter\nopen http://localhost:8888\n\n# Gerenciar MongoDB\nopen http://localhost:8081\n</code></pre>"},{"location":"docker/compose/#comandos-de-limpeza","title":"\ud83e\uddf9 Comandos de Limpeza","text":"<pre><code># Limpeza b\u00e1sica\ndocker-compose down --volumes\ndocker system prune -f\n\n# Limpeza completa (CUIDADO!)\ndocker-compose down --rmi all --volumes --remove-orphans\ndocker system prune -a -f --volumes\n</code></pre>"},{"location":"docker/compose/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udc33 Setup Docker - Configura\u00e7\u00e3o b\u00e1sica do Docker</li> <li>\ud83d\ude80 Deployment - Deploy em produ\u00e7\u00e3o</li> <li>\ud83e\uddea Testes - Testes de integra\u00e7\u00e3o</li> </ol>"},{"location":"docker/compose/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83c\udfd7\ufe0f Arquitetura - Vis\u00e3o geral do sistema</li> <li>\u26a1 API - Endpoints da aplica\u00e7\u00e3o</li> <li>\ud83d\udd27 Troubleshooting - Solu\u00e7\u00e3o de problemas</li> </ul>"},{"location":"docker/deployment/","title":"\ud83d\ude80 Deployment","text":"<p>Guia completo para deploy da aplica\u00e7\u00e3o em diferentes ambientes de produ\u00e7\u00e3o, incluindo AWS, Google Cloud, Azure e infraestrutura on-premise.</p>"},{"location":"docker/deployment/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Esta se\u00e7\u00e3o documenta as estrat\u00e9gias de deployment para a aplica\u00e7\u00e3o de predi\u00e7\u00e3o de atrasos de voos, cobrindo desde deployment local at\u00e9 infraestrutura em nuvem com alta disponibilidade.</p>"},{"location":"docker/deployment/#estrategias-de-deployment","title":"\u2601\ufe0f Estrat\u00e9gias de Deployment","text":"<pre><code>graph TD\n    A[C\u00f3digo Fonte] --&gt; B[CI/CD Pipeline]\n    B --&gt; C{Ambiente}\n\n    C --&gt;|Staging| D[AWS ECS Staging]\n    C --&gt;|Production| E[AWS ECS Production]\n    C --&gt;|Development| F[Docker Local]\n    C --&gt;|On-Premise| G[Kubernetes Cluster]\n\n    D --&gt; H[Load Balancer]\n    E --&gt; H\n    G --&gt; I[Ingress Controller]\n\n    H --&gt; J[Monitoring &amp; Logging]\n    I --&gt; J\n\n    style A fill:#e3f2fd\n    style J fill:#c8e6c9</code></pre>"},{"location":"docker/deployment/#aws-ecs-deployment","title":"\ud83d\udc33 AWS ECS Deployment","text":""},{"location":"docker/deployment/#1-task-definition","title":"1. \ud83d\udccb Task Definition","text":"<pre><code>{\n  \"family\": \"flight-delay-api\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"1024\",\n  \"memory\": \"2048\",\n  \"executionRoleArn\": \"arn:aws:iam::ACCOUNT:role/ecsTaskExecutionRole\",\n  \"taskRoleArn\": \"arn:aws:iam::ACCOUNT:role/flightDelayTaskRole\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"api\",\n      \"image\": \"YOUR_ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/flight-delay-api:latest\",\n      \"essential\": true,\n      \"portMappings\": [\n        {\n          \"containerPort\": 8000,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"ENVIRONMENT\",\n          \"value\": \"production\"\n        },\n        {\n          \"name\": \"LOG_LEVEL\",\n          \"value\": \"INFO\"\n        }\n      ],\n      \"secrets\": [\n        {\n          \"name\": \"DATABASE_URL\",\n          \"valueFrom\": \"arn:aws:secretsmanager:us-east-1:ACCOUNT:secret:flight-delay/database-url\"\n        },\n        {\n          \"name\": \"REDIS_URL\",\n          \"valueFrom\": \"arn:aws:secretsmanager:us-east-1:ACCOUNT:secret:flight-delay/redis-url\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/flight-delay-api\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"api\"\n        }\n      },\n      \"healthCheck\": {\n        \"command\": [\"CMD-SHELL\", \"curl -f http://localhost:8000/health || exit 1\"],\n        \"interval\": 30,\n        \"timeout\": 5,\n        \"retries\": 3,\n        \"startPeriod\": 60\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"docker/deployment/#2-ecs-service-configuration","title":"2. \ud83d\udd27 ECS Service Configuration","text":"<pre><code># ecs-service.yml\nResources:\n  FlightDelayService:\n    Type: AWS::ECS::Service\n    Properties:\n      ServiceName: flight-delay-api\n      Cluster: !Ref ECSCluster\n      TaskDefinition: !Ref TaskDefinition\n      DesiredCount: 3\n      LaunchType: FARGATE\n      NetworkConfiguration:\n        AwsvpcConfiguration:\n          SecurityGroups:\n            - !Ref APISecurityGroup\n          Subnets:\n            - !Ref PrivateSubnet1\n            - !Ref PrivateSubnet2\n          AssignPublicIp: DISABLED\n      LoadBalancers:\n        - ContainerName: api\n          ContainerPort: 8000\n          TargetGroupArn: !Ref APITargetGroup\n      ServiceRegistries:\n        - RegistryArn: !GetAtt ServiceDiscovery.Arn\n      DeploymentConfiguration:\n        MaximumPercent: 200\n        MinimumHealthyPercent: 50\n        DeploymentCircuitBreaker:\n          Enable: true\n          Rollback: true\n      EnableExecuteCommand: true\n\n  # Auto Scaling\n  ServiceAutoScalingTarget:\n    Type: AWS::ApplicationAutoScaling::ScalableTarget\n    Properties:\n      MaxCapacity: 10\n      MinCapacity: 2\n      ResourceId: !Sub \"service/${ECSCluster}/${FlightDelayService.Name}\"\n      RoleARN: !GetAtt AutoScalingRole.Arn\n      ScalableDimension: ecs:service:DesiredCount\n      ServiceNamespace: ecs\n\n  ServiceAutoScalingPolicy:\n    Type: AWS::ApplicationAutoScaling::ScalingPolicy\n    Properties:\n      PolicyName: FlightDelayAPIScalingPolicy\n      PolicyType: TargetTrackingScaling\n      ResourceId: !Ref ServiceAutoScalingTarget\n      ScalableDimension: ecs:service:DesiredCount\n      ServiceNamespace: ecs\n      TargetTrackingScalingPolicyConfiguration:\n        PredefinedMetricSpecification:\n          PredefinedMetricType: ECSServiceAverageCPUUtilization\n        TargetValue: 70.0\n        ScaleOutCooldown: 300\n        ScaleInCooldown: 300\n</code></pre>"},{"location":"docker/deployment/#3-infrastructure-as-code-terraform","title":"3. \ud83c\udfd7\ufe0f Infrastructure as Code (Terraform)","text":"<pre><code># main.tf\nterraform {\n  required_version = \"&gt;= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket = \"flight-delay-terraform-state\"\n    key    = \"production/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n\n  default_tags {\n    tags = {\n      Project     = \"FlightDelayPrediction\"\n      Environment = var.environment\n      ManagedBy   = \"Terraform\"\n    }\n  }\n}\n\n# VPC\nmodule \"vpc\" {\n  source = \"terraform-aws-modules/vpc/aws\"\n\n  name = \"flight-delay-vpc\"\n  cidr = \"10.0.0.0/16\"\n\n  azs             = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n\n  enable_nat_gateway = true\n  enable_vpn_gateway = false\n\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"flight-delay-vpc\"\n  }\n}\n\n# ECS Cluster\nresource \"aws_ecs_cluster\" \"main\" {\n  name = \"flight-delay-cluster\"\n\n  configuration {\n    execute_command_configuration {\n      logging = \"OVERRIDE\"\n\n      log_configuration {\n        cloud_watch_log_group_name = aws_cloudwatch_log_group.ecs.name\n      }\n    }\n  }\n\n  setting {\n    name  = \"containerInsights\"\n    value = \"enabled\"\n  }\n}\n\n# Application Load Balancer\nresource \"aws_lb\" \"main\" {\n  name               = \"flight-delay-alb\"\n  internal           = false\n  load_balancer_type = \"application\"\n  security_groups    = [aws_security_group.alb.id]\n  subnets           = module.vpc.public_subnets\n\n  enable_deletion_protection = true\n\n  access_logs {\n    bucket  = aws_s3_bucket.alb_logs.bucket\n    prefix  = \"flight-delay-alb\"\n    enabled = true\n  }\n}\n\n# RDS PostgreSQL (alternativa ao MongoDB)\nresource \"aws_db_instance\" \"main\" {\n  identifier = \"flight-delay-db\"\n\n  engine         = \"postgres\"\n  engine_version = \"15.4\"\n  instance_class = \"db.t3.medium\"\n\n  allocated_storage     = 20\n  max_allocated_storage = 100\n  storage_encrypted     = true\n\n  db_name  = \"flight_predictions\"\n  username = \"postgres\"\n  password = var.db_password\n\n  vpc_security_group_ids = [aws_security_group.rds.id]\n  db_subnet_group_name   = aws_db_subnet_group.main.name\n\n  backup_retention_period = 7\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"sun:04:00-sun:05:00\"\n\n  skip_final_snapshot = false\n  final_snapshot_identifier = \"flight-delay-db-final-snapshot-${formatdate(\"YYYY-MM-DD-hhmm\", timestamp())}\"\n\n  performance_insights_enabled = true\n  monitoring_interval         = 60\n  monitoring_role_arn        = aws_iam_role.rds_monitoring.arn\n\n  tags = {\n    Name = \"flight-delay-database\"\n  }\n}\n\n# ElastiCache Redis\nresource \"aws_elasticache_subnet_group\" \"main\" {\n  name       = \"flight-delay-cache-subnet\"\n  subnet_ids = module.vpc.private_subnets\n}\n\nresource \"aws_elasticache_replication_group\" \"main\" {\n  replication_group_id       = \"flight-delay-redis\"\n  description               = \"Redis cluster for flight delay prediction\"\n\n  node_type                 = \"cache.t3.micro\"\n  port                      = 6379\n  parameter_group_name      = \"default.redis7\"\n\n  num_cache_clusters        = 2\n  automatic_failover_enabled = true\n  multi_az_enabled          = true\n\n  subnet_group_name = aws_elasticache_subnet_group.main.name\n  security_group_ids = [aws_security_group.redis.id]\n\n  at_rest_encryption_enabled = true\n  transit_encryption_enabled = true\n  auth_token                = var.redis_auth_token\n\n  log_delivery_configuration {\n    destination      = aws_cloudwatch_log_group.redis.name\n    destination_type = \"cloudwatch-logs\"\n    log_format      = \"text\"\n    log_type        = \"slow-log\"\n  }\n}\n</code></pre>"},{"location":"docker/deployment/#google-cloud-platform-deployment","title":"\ud83c\udf10 Google Cloud Platform Deployment","text":""},{"location":"docker/deployment/#1-cloud-run-configuration","title":"1. \ud83d\udccb Cloud Run Configuration","text":"<pre><code># cloudrun.yml\napiVersion: serving.knative.dev/v1\nkind: Service\nmetadata:\n  name: flight-delay-api\n  annotations:\n    run.googleapis.com/ingress: all\n    run.googleapis.com/execution-environment: gen2\nspec:\n  template:\n    metadata:\n      annotations:\n        autoscaling.knative.dev/maxScale: \"100\"\n        autoscaling.knative.dev/minScale: \"2\"\n        run.googleapis.com/cpu-throttling: \"false\"\n        run.googleapis.com/memory: \"2Gi\"\n        run.googleapis.com/cpu: \"1\"\n    spec:\n      containerConcurrency: 80\n      timeoutSeconds: 300\n      containers:\n      - image: gcr.io/PROJECT_ID/flight-delay-api:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: ENVIRONMENT\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: flight-delay-secrets\n              key: database-url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: flight-delay-secrets\n              key: redis-url\n        resources:\n          limits:\n            cpu: \"1\"\n            memory: \"2Gi\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n</code></pre>"},{"location":"docker/deployment/#2-cloud-build-pipeline","title":"2. \ud83d\udd27 Cloud Build Pipeline","text":"<pre><code># cloudbuild.yml\nsteps:\n  # Build da imagem\n  - name: 'gcr.io/cloud-builders/docker'\n    args:\n      - 'build'\n      - '-t'\n      - 'gcr.io/$PROJECT_ID/flight-delay-api:$COMMIT_SHA'\n      - '-t' \n      - 'gcr.io/$PROJECT_ID/flight-delay-api:latest'\n      - '.'\n\n  # Push da imagem\n  - name: 'gcr.io/cloud-builders/docker'\n    args:\n      - 'push'\n      - 'gcr.io/$PROJECT_ID/flight-delay-api:$COMMIT_SHA'\n\n  - name: 'gcr.io/cloud-builders/docker'\n    args:\n      - 'push'\n      - 'gcr.io/$PROJECT_ID/flight-delay-api:latest'\n\n  # Testes\n  - name: 'gcr.io/$PROJECT_ID/flight-delay-api:$COMMIT_SHA'\n    entrypoint: 'python'\n    args: ['-m', 'pytest', 'tests/', '-v']\n    env:\n      - 'TESTING=true'\n\n  # Deploy para Cloud Run\n  - name: 'gcr.io/cloud-builders/gcloud'\n    args:\n      - 'run'\n      - 'deploy'\n      - 'flight-delay-api'\n      - '--image=gcr.io/$PROJECT_ID/flight-delay-api:$COMMIT_SHA'\n      - '--region=us-central1'\n      - '--platform=managed'\n      - '--allow-unauthenticated'\n      - '--max-instances=100'\n      - '--min-instances=2'\n      - '--cpu=1'\n      - '--memory=2Gi'\n      - '--concurrency=80'\n      - '--timeout=300'\n\noptions:\n  logging: CLOUD_LOGGING_ONLY\n  machineType: 'E2_HIGHCPU_8'\n\ntimeout: '1200s'\n</code></pre>"},{"location":"docker/deployment/#kubernetes-deployment","title":"\u2693 Kubernetes Deployment","text":""},{"location":"docker/deployment/#1-deployment-manifest","title":"1. \ud83d\udccb Deployment Manifest","text":"<pre><code># k8s/deployment.yml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flight-delay-api\n  labels:\n    app: flight-delay-api\n    version: v1\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n      maxSurge: 1\n  selector:\n    matchLabels:\n      app: flight-delay-api\n  template:\n    metadata:\n      labels:\n        app: flight-delay-api\n        version: v1\n    spec:\n      serviceAccountName: flight-delay-service-account\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n      containers:\n      - name: api\n        image: flight-delay-api:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        env:\n        - name: ENVIRONMENT\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: flight-delay-secrets\n              key: database-url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: flight-delay-secrets\n              key: redis-url\n        resources:\n          requests:\n            cpu: 100m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: tmp-volume\n          mountPath: /tmp\n      volumes:\n      - name: tmp-volume\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: \"kubernetes.io/arch\"\n        operator: \"Equal\"\n        value: \"amd64\"\n        effect: \"NoSchedule\"\n\n---\n# Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: flight-delay-api-service\n  labels:\n    app: flight-delay-api\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8000\n    protocol: TCP\n    name: http\n  selector:\n    app: flight-delay-api\n\n---\n# HorizontalPodAutoscaler\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: flight-delay-api-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: flight-delay-api\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 10\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n      - type: Pods\n        value: 4\n        periodSeconds: 15\n      selectPolicy: Max\n</code></pre>"},{"location":"docker/deployment/#2-ingress-configuration","title":"2. \ud83c\udf10 Ingress Configuration","text":"<pre><code># k8s/ingress.yml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: flight-delay-ingress\n  annotations:\n    kubernetes.io/ingress.class: \"nginx\"\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\n    nginx.ingress.kubernetes.io/rate-limit: \"10\"\n    nginx.ingress.kubernetes.io/rate-limit-window: \"1m\"\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\nspec:\n  tls:\n  - hosts:\n    - api.flightdelay.com\n    secretName: flight-delay-tls\n  rules:\n  - host: api.flightdelay.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: flight-delay-api-service\n            port:\n              number: 80\n</code></pre>"},{"location":"docker/deployment/#cicd-pipelines","title":"\ud83d\udd04 CI/CD Pipelines","text":""},{"location":"docker/deployment/#1-github-actions","title":"1. \ud83d\udc19 GitHub Actions","text":"<pre><code># .github/workflows/deploy.yml\nname: Deploy to Production\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      mongodb:\n        image: mongo:7.0\n        ports:\n          - 27017:27017\n      redis:\n        image: redis:7-alpine\n        ports:\n          - 6379:6379\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies\n      run: |\n        pip install poetry\n        poetry install\n\n    - name: Run tests\n      run: |\n        poetry run pytest tests/ -v --cov=src --cov-report=xml\n      env:\n        DATABASE_URL: mongodb://localhost:27017/test_db\n        REDIS_URL: redis://localhost:6379/0\n        TESTING: true\n\n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n\n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'\n\n    outputs:\n      image: ${{ steps.image.outputs.image }}\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    - name: Log in to Container Registry\n      uses: docker/login-action@v3\n      with:\n        registry: ${{ env.REGISTRY }}\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=sha,prefix={{branch}}-\n          type=raw,value=latest,enable={{is_default_branch}}\n\n    - name: Build and push\n      id: build\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        platforms: linux/amd64,linux/arm64\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        cache-from: type=gha\n        cache-to: type=gha,mode=max\n\n    - name: Output image\n      id: image\n      run: echo \"image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest\" &gt;&gt; $GITHUB_OUTPUT\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    environment: production\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-east-1\n\n    - name: Update ECS task definition\n      id: task-def\n      uses: aws-actions/amazon-ecs-render-task-definition@v1\n      with:\n        task-definition: .aws/task-definition.json\n        container-name: api\n        image: ${{ needs.build.outputs.image }}\n\n    - name: Deploy to ECS\n      uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n      with:\n        task-definition: ${{ steps.task-def.outputs.task-definition }}\n        service: flight-delay-api\n        cluster: flight-delay-cluster\n        wait-for-service-stability: true\n\n    - name: Verify deployment\n      run: |\n        sleep 60\n        curl -f ${{ secrets.PRODUCTION_URL }}/health\n</code></pre>"},{"location":"docker/deployment/#2-gitlab-cicd","title":"2. \ud83e\udd8a GitLab CI/CD","text":"<pre><code># .gitlab-ci.yml\nstages:\n  - test\n  - build\n  - deploy\n\nvariables:\n  DOCKER_IMAGE: $CI_REGISTRY_IMAGE\n  DOCKER_TAG: $CI_COMMIT_SHA\n\n.docker_template: &amp;docker_template\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  before_script:\n    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY\n\ntest:\n  stage: test\n  image: python:3.12\n  services:\n    - name: mongo:7.0\n      alias: mongodb\n    - name: redis:7-alpine\n      alias: redis\n  variables:\n    DATABASE_URL: \"mongodb://mongodb:27017/test_db\"\n    REDIS_URL: \"redis://redis:6379/0\"\n    TESTING: \"true\"\n  before_script:\n    - pip install poetry\n    - poetry install\n  script:\n    - poetry run pytest tests/ -v --cov=src --cov-report=term-missing --cov-report=xml\n    - poetry run ruff check src/ tests/\n    - poetry run black --check src/ tests/\n  coverage: '/TOTAL.+ ([0-9]{1,3}%)/'\n  artifacts:\n    reports:\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage.xml\n    paths:\n      - coverage.xml\n    expire_in: 1 week\n\nbuild:\n  stage: build\n  &lt;&lt;: *docker_template\n  script:\n    - docker build -t $DOCKER_IMAGE:$DOCKER_TAG -t $DOCKER_IMAGE:latest .\n    - docker push $DOCKER_IMAGE:$DOCKER_TAG\n    - docker push $DOCKER_IMAGE:latest\n  only:\n    - main\n\ndeploy_staging:\n  stage: deploy\n  &lt;&lt;: *docker_template\n  environment:\n    name: staging\n    url: https://staging-api.flightdelay.com\n  script:\n    - echo \"Deploying to staging...\"\n    - |\n      kubectl set image deployment/flight-delay-api-staging \\\n        api=$DOCKER_IMAGE:$DOCKER_TAG \\\n        --namespace=staging\n    - kubectl rollout status deployment/flight-delay-api-staging --namespace=staging\n  only:\n    - main\n\ndeploy_production:\n  stage: deploy\n  &lt;&lt;: *docker_template\n  environment:\n    name: production\n    url: https://api.flightdelay.com\n  script:\n    - echo \"Deploying to production...\"\n    - |\n      kubectl set image deployment/flight-delay-api \\\n        api=$DOCKER_IMAGE:$DOCKER_TAG \\\n        --namespace=production\n    - kubectl rollout status deployment/flight-delay-api --namespace=production\n  when: manual\n  only:\n    - main\n</code></pre>"},{"location":"docker/deployment/#monitoramento-e-observabilidade","title":"\ud83d\udcca Monitoramento e Observabilidade","text":""},{"location":"docker/deployment/#1-prometheus-grafana","title":"1. \ud83d\udd0d Prometheus &amp; Grafana","text":"<pre><code># monitoring/prometheus-config.yml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-config\ndata:\n  prometheus.yml: |\n    global:\n      scrape_interval: 15s\n      evaluation_interval: 15s\n\n    rule_files:\n      - \"/etc/prometheus/rules/*.yml\"\n\n    scrape_configs:\n      - job_name: 'flight-delay-api'\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n            action: replace\n            regex: ([^:]+)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n            target_label: __address__\n\n    alerting:\n      alertmanagers:\n        - kubernetes_sd_configs:\n            - role: pod\n          relabel_configs:\n            - source_labels: [__meta_kubernetes_pod_label_app]\n              action: keep\n              regex: alertmanager\n\n---\n# Alerting Rules\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: prometheus-rules\ndata:\n  flight-delay.yml: |\n    groups:\n      - name: flight-delay-api\n        rules:\n          - alert: APIHighErrorRate\n            expr: rate(http_requests_total{status=~\"5..\"}[5m]) &gt; 0.1\n            for: 5m\n            labels:\n              severity: critical\n            annotations:\n              summary: \"High error rate detected\"\n              description: \"API error rate is {{ $value }} errors per second\"\n\n          - alert: APIHighLatency\n            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) &gt; 1.0\n            for: 5m\n            labels:\n              severity: warning\n            annotations:\n              summary: \"High API latency\"\n              description: \"95th percentile latency is {{ $value }} seconds\"\n\n          - alert: PodCrashLooping\n            expr: rate(kube_pod_container_status_restarts_total[15m]) &gt; 0\n            for: 5m\n            labels:\n              severity: critical\n            annotations:\n              summary: \"Pod is crash looping\"\n              description: \"Pod {{ $labels.pod }} is restarting frequently\"\n</code></pre>"},{"location":"docker/deployment/#2-application-metrics","title":"2. \ud83d\udcca Application Metrics","text":"<pre><code># src/monitoring/metrics.py\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nimport time\nfrom functools import wraps\n\n# M\u00e9tricas customizadas\nREQUEST_COUNT = Counter(\n    'http_requests_total',\n    'Total HTTP requests',\n    ['method', 'endpoint', 'status']\n)\n\nREQUEST_LATENCY = Histogram(\n    'http_request_duration_seconds',\n    'HTTP request latency',\n    ['method', 'endpoint']\n)\n\nACTIVE_CONNECTIONS = Gauge(\n    'http_active_connections',\n    'Active HTTP connections'\n)\n\nPREDICTION_COUNT = Counter(\n    'predictions_total',\n    'Total predictions made',\n    ['model_type', 'result']\n)\n\nMODEL_INFERENCE_TIME = Histogram(\n    'model_inference_duration_seconds',\n    'Model inference time',\n    ['model_type']\n)\n\ndef monitor_requests(f):\n    \"\"\"Decorator para monitorar requests\"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        start_time = time.time()\n\n        try:\n            result = f(*args, **kwargs)\n            status = '200'\n            return result\n        except Exception as e:\n            status = '500'\n            raise\n        finally:\n            REQUEST_COUNT.labels(\n                method='POST', \n                endpoint='/predict', \n                status=status\n            ).inc()\n\n            REQUEST_LATENCY.labels(\n                method='POST', \n                endpoint='/predict'\n            ).observe(time.time() - start_time)\n\n    return decorated_function\n\ndef monitor_predictions(f):\n    \"\"\"Decorator para monitorar predi\u00e7\u00f5es\"\"\"\n    @wraps(f)\n    def decorated_function(*args, **kwargs):\n        start_time = time.time()\n\n        result = f(*args, **kwargs)\n\n        # Registrar m\u00e9trica\n        PREDICTION_COUNT.labels(\n            model_type='xgboost',\n            result='delayed' if result['is_delayed'] else 'on_time'\n        ).inc()\n\n        MODEL_INFERENCE_TIME.labels(\n            model_type='xgboost'\n        ).observe(time.time() - start_time)\n\n        return result\n\n    return decorated_function\n</code></pre>"},{"location":"docker/deployment/#security-compliance","title":"\ud83d\udee1\ufe0f Security &amp; Compliance","text":""},{"location":"docker/deployment/#1-security-scanning","title":"1. \ud83d\udd10 Security Scanning","text":"<pre><code># .github/workflows/security.yml\nname: Security Scan\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n  schedule:\n    - cron: '0 2 * * 1'  # Weekly scan\n\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    # Dependency scanning\n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: 'fs'\n        scan-ref: '.'\n        format: 'sarif'\n        output: 'trivy-results.sarif'\n\n    - name: Upload Trivy scan results\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: 'trivy-results.sarif'\n\n    # Container scanning\n    - name: Build image for scanning\n      run: docker build -t security-scan .\n\n    - name: Run Trivy image scan\n      uses: aquasecurity/trivy-action@master\n      with:\n        image-ref: 'security-scan'\n        format: 'sarif'\n        output: 'trivy-image-results.sarif'\n\n    # SAST scanning\n    - name: Initialize CodeQL\n      uses: github/codeql-action/init@v2\n      with:\n        languages: python\n\n    - name: Perform CodeQL Analysis\n      uses: github/codeql-action/analyze@v2\n\n    # Secrets scanning\n    - name: Run GitLeaks\n      uses: zricethezav/gitleaks-action@master\n</code></pre>"},{"location":"docker/deployment/#2-network-policies","title":"2. \ud83d\udd12 Network Policies","text":"<pre><code># k8s/network-policy.yml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: flight-delay-api-netpol\nspec:\n  podSelector:\n    matchLabels:\n      app: flight-delay-api\n  policyTypes:\n  - Ingress\n  - Egress\n\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: ingress-nginx\n    - podSelector:\n        matchLabels:\n          app: prometheus\n    ports:\n    - protocol: TCP\n      port: 8000\n\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: database\n    ports:\n    - protocol: TCP\n      port: 27017\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: cache\n    ports:\n    - protocol: TCP\n      port: 6379\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 53\n    - protocol: UDP\n      port: 53\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>"},{"location":"docker/deployment/#checklist-de-deployment","title":"\ud83d\udccb Checklist de Deployment","text":""},{"location":"docker/deployment/#pre-deploy","title":"\u2705 Pr\u00e9-Deploy","text":"<ul> <li>[ ] Testes passando</li> <li>[ ] Testes unit\u00e1rios</li> <li>[ ] Testes de integra\u00e7\u00e3o</li> <li>[ ] Testes de seguran\u00e7a</li> <li> <p>[ ] Testes de performance</p> </li> <li> <p>[ ] Configura\u00e7\u00e3o validada</p> </li> <li>[ ] Vari\u00e1veis de ambiente</li> <li>[ ] Secrets configurados</li> <li>[ ] Recursos adequados</li> <li> <p>[ ] Health checks funcionando</p> </li> <li> <p>[ ] Infraestrutura preparada</p> </li> <li>[ ] Banco de dados migrado</li> <li>[ ] Cache configurado</li> <li>[ ] Load balancer configurado</li> <li>[ ] Monitoramento ativo</li> </ul>"},{"location":"docker/deployment/#deploy","title":"\u2705 Deploy","text":"<ul> <li>[ ] Deployment executado</li> <li>[ ] Imagem constru\u00edda e pushed</li> <li>[ ] Containers atualizados</li> <li>[ ] Rolling update conclu\u00eddo</li> <li> <p>[ ] Health checks passando</p> </li> <li> <p>[ ] Valida\u00e7\u00e3o p\u00f3s-deploy</p> </li> <li>[ ] Endpoints respondendo</li> <li>[ ] M\u00e9tricas normais</li> <li>[ ] Logs sem erros</li> <li>[ ] Smoke tests passando</li> </ul>"},{"location":"docker/deployment/#pos-deploy","title":"\u2705 P\u00f3s-Deploy","text":"<ul> <li>[ ] Monitoramento ativo</li> <li>[ ] Alertas configurados</li> <li>[ ] Dashboards atualizados</li> <li> <p>[ ] SLAs monitorados</p> </li> <li> <p>[ ] Rollback preparado</p> </li> <li>[ ] Vers\u00e3o anterior dispon\u00edvel</li> <li>[ ] Procedimento de rollback testado</li> <li>[ ] Equipe notificada</li> </ul>"},{"location":"docker/deployment/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udc33 Setup Docker - Configura\u00e7\u00e3o inicial</li> <li>\ud83d\udd27 Docker Compose - Orquestra\u00e7\u00e3o local</li> <li>\ud83d\udcca Monitoramento - Observabilidade</li> </ol>"},{"location":"docker/deployment/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\u2601\ufe0f AWS ECS - Documenta\u00e7\u00e3o oficial</li> <li>\u2693 Kubernetes - Guias de deployment</li> <li>\ud83d\udd0d Prometheus - Monitoramento</li> </ul>"},{"location":"docker/setup/","title":"\ud83d\udc33 Setup Docker","text":"<p>Este guia apresenta como configurar e usar Docker para executar o projeto Machine Learning Engineer Challenge.</p>"},{"location":"docker/setup/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>O projeto oferece duas op\u00e7\u00f5es de containeriza\u00e7\u00e3o: - \ud83d\ude80 Docker simples - Container \u00fanico da API - \ud83d\udd04 Docker Compose - Orquestra\u00e7\u00e3o completa (API + MongoDB)</p>"},{"location":"docker/setup/#pre-requisitos","title":"\ud83d\udee0\ufe0f Pr\u00e9-requisitos","text":""},{"location":"docker/setup/#instalacao-do-docker","title":"\ud83d\udce6 Instala\u00e7\u00e3o do Docker","text":""},{"location":"docker/setup/#windows","title":"Windows","text":"<p>Op\u00e7\u00e3o 1: Docker Desktop (Recomendado) <pre><code># Baixar: https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe\n# Instalar e reiniciar o sistema\n</code></pre></p> <p>Op\u00e7\u00e3o 2: Chocolatey <pre><code>choco install docker-desktop\n</code></pre></p> <p>Op\u00e7\u00e3o 3: Winget <pre><code>winget install Docker.DockerDesktop\n</code></pre></p>"},{"location":"docker/setup/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Instalar Docker\nsudo apt update\nsudo apt install -y docker.io docker-compose\n\n# Iniciar servi\u00e7o\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Adicionar usu\u00e1rio ao grupo docker\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# Verificar instala\u00e7\u00e3o\ndocker --version\ndocker-compose --version\n</code></pre>"},{"location":"docker/setup/#macos","title":"macOS","text":"<p>Homebrew: <pre><code>brew install --cask docker\n</code></pre></p> <p>Manual: <pre><code># Baixar: https://desktop.docker.com/mac/main/amd64/Docker.dmg\n# Instalar via interface gr\u00e1fica\n</code></pre></p>"},{"location":"docker/setup/#verificar-instalacao","title":"\u2705 Verificar Instala\u00e7\u00e3o","text":"<pre><code># Verificar Docker\ndocker --version\n# Esperado: Docker version 20.x.x\n\n# Verificar Docker Compose\ndocker-compose --version\n# Esperado: docker-compose version 1.x.x\n\n# Teste b\u00e1sico\ndocker run hello-world\n</code></pre>"},{"location":"docker/setup/#estrutura-docker","title":"\ud83d\udcc1 Estrutura Docker","text":""},{"location":"docker/setup/#arquivos-de-configuracao","title":"\ud83d\uddc2\ufe0f Arquivos de Configura\u00e7\u00e3o","text":"<pre><code>machine-learning-engineer/\n\u251c\u2500\u2500 \ud83d\udc33 Dockerfile                    # Imagem da API\n\u251c\u2500\u2500 \ud83d\udd04 docker-compose.yml           # Orquestra\u00e7\u00e3o completa\n\u251c\u2500\u2500 \ud83d\ude80 docker-compose.simple.yml    # Apenas API\n\u251c\u2500\u2500 \ud83d\udccb .dockerignore                # Arquivos ignorados\n\u2514\u2500\u2500 \ud83d\udd27 docker/                      # Configs espec\u00edficas\n    \u251c\u2500\u2500 api.Dockerfile              # Dockerfile otimizado\n    \u251c\u2500\u2500 requirements.txt            # Depend\u00eancias Docker\n    \u2514\u2500\u2500 mongo-init/\n        \u2514\u2500\u2500 init-db.js              # Setup MongoDB\n</code></pre>"},{"location":"docker/setup/#dockerfile-principal","title":"\ud83d\udccb Dockerfile Principal","text":"<pre><code># Dockerfile - Multi-stage build\nFROM python:3.12-slim as base\n\n# Instalar depend\u00eancias do sistema\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    gcc \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Configurar diret\u00f3rio de trabalho\nWORKDIR /app\n\n# Copiar e instalar depend\u00eancias Python\nCOPY pyproject.toml poetry.lock ./\nRUN pip install poetry &amp;&amp; \\\n    poetry config virtualenvs.create false &amp;&amp; \\\n    poetry install --without dev\n\n# Copiar c\u00f3digo fonte\nCOPY src/ ./src/\nCOPY model/ ./model/\n\n# Expor porta\nEXPOSE 8000\n\n# Comando de inicializa\u00e7\u00e3o\nCMD [\"uvicorn\", \"src.routers.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"docker/setup/#executando-com-docker","title":"\ud83d\ude80 Executando com Docker","text":""},{"location":"docker/setup/#1-build-da-imagem","title":"1. \ud83c\udfd7\ufe0f Build da Imagem","text":"<pre><code># Build da imagem principal\ndocker build -t ml-engineer-api .\n\n# Ou com tag espec\u00edfica\ndocker build -t ml-engineer-api:v1.0.0 .\n\n# Build com cache limpo (se necess\u00e1rio)\ndocker build --no-cache -t ml-engineer-api .\n</code></pre>"},{"location":"docker/setup/#2-executar-container","title":"2. \u26a1 Executar Container","text":"<p>Execu\u00e7\u00e3o b\u00e1sica: <pre><code># Executar em foreground\ndocker run -p 8000:8000 ml-engineer-api\n\n# Executar em background\ndocker run -d -p 8000:8000 --name ml-api ml-engineer-api\n\n# Com montagem de volume para modelos\ndocker run -d \\\n  -p 8000:8000 \\\n  -v $(pwd)/model:/app/model \\\n  --name ml-api \\\n  ml-engineer-api\n</code></pre></p> <p>Com vari\u00e1veis de ambiente: <pre><code>docker run -d \\\n  -p 8000:8000 \\\n  -e ENVIRONMENT=production \\\n  -e LOG_LEVEL=INFO \\\n  -v $(pwd)/model:/app/model \\\n  --name ml-api \\\n  ml-engineer-api\n</code></pre></p>"},{"location":"docker/setup/#3-verificar-execucao","title":"3. \ud83d\udd0d Verificar Execu\u00e7\u00e3o","text":"<pre><code># Verificar containers rodando\ndocker ps\n\n# Verificar logs\ndocker logs ml-api\n\n# Logs em tempo real\ndocker logs -f ml-api\n\n# Entrar no container\ndocker exec -it ml-api bash\n</code></pre>"},{"location":"docker/setup/#docker-compose","title":"\ud83d\udd04 Docker Compose","text":""},{"location":"docker/setup/#configuracao-completa","title":"\ud83d\udccb Configura\u00e7\u00e3o Completa","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  api:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./model:/app/model:ro\n    environment:\n      - ENVIRONMENT=development\n      - DATABASE_URL=mongodb://mongodb:27017/flight_predictions\n    depends_on:\n      - mongodb\n    networks:\n      - ml-network\n\n  mongodb:\n    image: mongo:7.0\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongodb_data:/data/db\n      - ./docker/mongo-init:/docker-entrypoint-initdb.d:ro\n    environment:\n      - MONGO_INITDB_ROOT_USERNAME=admin\n      - MONGO_INITDB_ROOT_PASSWORD=password\n      - MONGO_INITDB_DATABASE=flight_predictions\n    networks:\n      - ml-network\n\nvolumes:\n  mongodb_data:\n\nnetworks:\n  ml-network:\n    driver: bridge\n</code></pre>"},{"location":"docker/setup/#comandos-docker-compose","title":"\ud83d\ude80 Comandos Docker Compose","text":"<p>Execu\u00e7\u00e3o completa: <pre><code># Build e executar todos os servi\u00e7os\ndocker-compose up --build\n\n# Executar em background\ndocker-compose up -d\n\n# Apenas build (sem executar)\ndocker-compose build\n\n# Executar servi\u00e7o espec\u00edfico\ndocker-compose up api\n\n# Parar todos os servi\u00e7os\ndocker-compose down\n\n# Parar e remover volumes\ndocker-compose down --volumes\n</code></pre></p> <p>Gerenciamento: <pre><code># Ver status dos servi\u00e7os\ndocker-compose ps\n\n# Ver logs de todos os servi\u00e7os\ndocker-compose logs\n\n# Logs de servi\u00e7o espec\u00edfico\ndocker-compose logs api\n\n# Logs em tempo real\ndocker-compose logs -f api\n\n# Executar comando no container\ndocker-compose exec api bash\ndocker-compose exec mongodb mongo\n</code></pre></p>"},{"location":"docker/setup/#compose-simplificado","title":"\ud83c\udfaf Compose Simplificado","text":"<p>Para usar apenas a API (sem MongoDB):</p> <pre><code># Usar compose simplificado\ndocker-compose -f docker-compose.simple.yml up --build\n</code></pre> <p>docker-compose.simple.yml: <pre><code>version: '3.8'\n\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./model:/app/model:ro\n    environment:\n      - ENVIRONMENT=development\n      - USE_MOCK_DB=true\n</code></pre></p>"},{"location":"docker/setup/#configuracoes-avancadas","title":"\u2699\ufe0f Configura\u00e7\u00f5es Avan\u00e7adas","text":""},{"location":"docker/setup/#variaveis-de-ambiente","title":"\ud83d\udd27 Vari\u00e1veis de Ambiente","text":"<pre><code># .env (criar na raiz do projeto)\nENVIRONMENT=production\nLOG_LEVEL=INFO\nDATABASE_URL=mongodb://mongodb:27017/flight_predictions\nMODEL_PATH=/app/model/modelo_arvore_decisao.pkl\nAPI_HOST=0.0.0.0\nAPI_PORT=8000\nWORKERS=4\n</code></pre>"},{"location":"docker/setup/#build-multi-stage","title":"\ud83c\udfd7\ufe0f Build Multi-stage","text":"<pre><code># Dockerfile.optimized\nFROM python:3.12-slim as builder\n\n# Instalar depend\u00eancias de build\nRUN apt-get update &amp;&amp; apt-get install -y gcc\n\n# Instalar Poetry e depend\u00eancias\nCOPY pyproject.toml poetry.lock ./\nRUN pip install poetry &amp;&amp; \\\n    poetry export -f requirements.txt --output requirements.txt --without dev\n\n# Stage de produ\u00e7\u00e3o\nFROM python:3.12-slim as production\n\n# Copiar apenas requirements\nCOPY --from=builder requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiar c\u00f3digo\nCOPY src/ ./src/\nCOPY model/ ./model/\n\n# Usu\u00e1rio n\u00e3o-root para seguran\u00e7a\nRUN useradd -m -u 1000 appuser &amp;&amp; chown -R appuser:appuser /app\nUSER appuser\n\nCMD [\"uvicorn\", \"src.routers.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"docker/setup/#health-checks","title":"\ud83d\udee1\ufe0f Health Checks","text":"<pre><code># Dockerfile com health check\nFROM python:3.12-slim\n\n# ... configura\u00e7\u00f5es anteriores ...\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\nCMD [\"uvicorn\", \"src.routers.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"docker/setup/#monitoramento","title":"\ud83d\udcca Monitoramento","text":""},{"location":"docker/setup/#logs-estruturados","title":"\ud83d\udd0d Logs Estruturados","text":"<pre><code># Ver logs com timestamp\ndocker-compose logs -t api\n\n# Filtrar logs por n\u00edvel\ndocker-compose logs api | grep ERROR\n\n# Salvar logs em arquivo\ndocker-compose logs api &gt; api.log\n</code></pre>"},{"location":"docker/setup/#metricas-de-container","title":"\ud83d\udcc8 M\u00e9tricas de Container","text":"<pre><code># Estat\u00edsticas em tempo real\ndocker stats\n\n# Uso de recursos do container espec\u00edfico\ndocker stats ml-api\n\n# Informa\u00e7\u00f5es detalhadas\ndocker inspect ml-api\n</code></pre>"},{"location":"docker/setup/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"docker/setup/#problemas-comuns","title":"\u274c Problemas Comuns","text":"<p>Container n\u00e3o inicia: <pre><code># Verificar logs\ndocker logs ml-api\n\n# Verificar porta ocupada\nnetstat -an | grep :8000  # Linux/macOS\nnetstat -an | findstr :8000  # Windows\n\n# Usar porta diferente\ndocker run -p 8001:8000 ml-engineer-api\n</code></pre></p> <p>Build falha: <pre><code># Limpar cache do Docker\ndocker system prune\n\n# Build sem cache\ndocker build --no-cache -t ml-engineer-api .\n\n# Verificar espa\u00e7o em disco\ndocker system df\n</code></pre></p> <p>MongoDB n\u00e3o conecta: <pre><code># Verificar rede\ndocker network ls\ndocker network inspect ml_ml-network\n\n# Testar conex\u00e3o\ndocker-compose exec api ping mongodb\n\n# Verificar logs do MongoDB\ndocker-compose logs mongodb\n</code></pre></p>"},{"location":"docker/setup/#comandos-de-diagnostico","title":"\ud83d\udd27 Comandos de Diagn\u00f3stico","text":"<pre><code># Informa\u00e7\u00f5es do sistema Docker\ndocker info\n\n# Vers\u00e3o do Docker\ndocker version\n\n# Processos em execu\u00e7\u00e3o\ndocker ps -a\n\n# Imagens dispon\u00edveis\ndocker images\n\n# Volumes\ndocker volume ls\n\n# Redes\ndocker network ls\n\n# Limpar recursos n\u00e3o utilizados\ndocker system prune -a\n</code></pre>"},{"location":"docker/setup/#otimizacoes-de-performance","title":"\ud83c\udfaf Otimiza\u00e7\u00f5es de Performance","text":""},{"location":"docker/setup/#melhorias-de-build","title":"\ud83d\ude80 Melhorias de Build","text":"<pre><code># .dockerignore\n.git\n.pytest_cache\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.env\n.venv\nnode_modules\n.DS_Store\n*.log\nhtmlcov/\n.coverage\ntests/\ndocs/\nREADME.md\n</code></pre>"},{"location":"docker/setup/#otimizacoes-de-runtime","title":"\u26a1 Otimiza\u00e7\u00f5es de Runtime","text":"<pre><code># Executar com recursos limitados\ndocker run \\\n  --memory=1g \\\n  --cpus=1.0 \\\n  -p 8000:8000 \\\n  ml-engineer-api\n\n# Com restart autom\u00e1tico\ndocker run \\\n  --restart=unless-stopped \\\n  -d \\\n  -p 8000:8000 \\\n  ml-engineer-api\n</code></pre>"},{"location":"docker/setup/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li>\ud83d\udd27 Docker Compose Avan\u00e7ado</li> <li>\ud83d\ude80 Deploy em Produ\u00e7\u00e3o</li> <li>\ud83e\uddea Testes com Docker</li> <li>\u2699\ufe0f Configura\u00e7\u00e3o de Ambiente</li> </ul>"},{"location":"docker/setup/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udc33 Docker Documentation</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce7 Email</li> </ul>"},{"location":"ml/data-analysis/","title":"\ud83d\udcca An\u00e1lise Explorat\u00f3ria de Dados","text":"<p>Guia completo para an\u00e1lise explorat\u00f3ria dos dados de voos, incluindo t\u00e9cnicas de investiga\u00e7\u00e3o, visualiza\u00e7\u00e3o e insights extra\u00eddos para o projeto de predi\u00e7\u00e3o de atrasos.</p>"},{"location":"ml/data-analysis/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>A an\u00e1lise explorat\u00f3ria de dados (EDA) \u00e9 fundamental para entender os padr\u00f5es nos dados de voos e identificar as caracter\u00edsticas mais relevantes para a predi\u00e7\u00e3o de atrasos. Esta se\u00e7\u00e3o documenta todo o processo de investiga\u00e7\u00e3o dos dados.</p>"},{"location":"ml/data-analysis/#datasets-utilizados","title":"\ud83d\udcc1 Datasets Utilizados","text":""},{"location":"ml/data-analysis/#dataset-principal-voosjson","title":"\ud83d\udee9\ufe0f Dataset Principal: voos.json","text":"<pre><code>{\n  \"flight_id\": \"FL001\",\n  \"airline\": \"TAM\",\n  \"departure_airport\": \"CGH\",\n  \"arrival_airport\": \"SDU\", \n  \"departure_time\": \"2023-01-15T08:30:00\",\n  \"arrival_time\": \"2023-01-15T09:45:00\",\n  \"scheduled_departure\": \"2023-01-15T08:30:00\",\n  \"scheduled_arrival\": \"2023-01-15T09:30:00\",\n  \"aircraft_type\": \"A320\",\n  \"weather_departure\": {...},\n  \"weather_arrival\": {...},\n  \"delay_minutes\": 15,\n  \"is_delayed\": true\n}\n</code></pre>"},{"location":"ml/data-analysis/#dataset-auxiliar-airports-databasecsv","title":"\ud83c\udfe2 Dataset Auxiliar: airports-database.csv","text":"<pre><code>icao_code,name,city,country,latitude,longitude,altitude\nSBSP,S\u00e3o Paulo/Guarulhos,S\u00e3o Paulo,Brazil,-23.4356,-46.4731,750\nSBGR,S\u00e3o Paulo/Congonhas,S\u00e3o Paulo,Brazil,-23.6267,-46.6553,803\n</code></pre>"},{"location":"ml/data-analysis/#analises-realizadas","title":"\ud83d\udd0d An\u00e1lises Realizadas","text":""},{"location":"ml/data-analysis/#1-estatisticas-descritivas","title":"1. \ud83d\udcc8 Estat\u00edsticas Descritivas","text":""},{"location":"ml/data-analysis/#distribuicao-de-atrasos","title":"Distribui\u00e7\u00e3o de Atrasos","text":"<pre><code># An\u00e1lise da distribui\u00e7\u00e3o de atrasos\ndelay_stats = {\n    \"Total de voos\": 50000,\n    \"Voos com atraso\": 15000,  # 30%\n    \"Voos pontuais\": 35000,    # 70%\n    \"Atraso m\u00e9dio\": \"22.5 minutos\",\n    \"Atraso m\u00e1ximo\": \"180 minutos\",\n    \"Mediana de atraso\": \"15 minutos\"\n}\n</code></pre>"},{"location":"ml/data-analysis/#estatisticas-por-companhia-aerea","title":"Estat\u00edsticas por Companhia A\u00e9rea","text":"Companhia Total Voos % Atrasos Atraso M\u00e9dio Pontualidade TAM 15,000 28% 20.5 min 72% GOL 12,500 32% 24.2 min 68% Azul 10,000 25% 18.8 min 75% Latam 8,500 30% 22.1 min 70% Avianca 4,000 35% 26.7 min 65%"},{"location":"ml/data-analysis/#2-analise-geografica","title":"2. \ud83c\udf0d An\u00e1lise Geogr\u00e1fica","text":""},{"location":"ml/data-analysis/#aeroportos-com-mais-atrasos","title":"Aeroportos com Mais Atrasos","text":"<pre><code># Top 10 aeroportos com maiores taxas de atraso\ntop_delay_airports = [\n    {\"airport\": \"SBGR\", \"name\": \"Guarulhos\", \"delay_rate\": 35.2},\n    {\"airport\": \"SBSP\", \"name\": \"Congonhas\", \"delay_rate\": 32.8},\n    {\"airport\": \"SBRJ\", \"name\": \"Santos Dumont\", \"delay_rate\": 29.5},\n    {\"airport\": \"SBGL\", \"name\": \"Gale\u00e3o\", \"delay_rate\": 28.9},\n    {\"airport\": \"SBCF\", \"name\": \"Confins\", \"delay_rate\": 26.7}\n]\n</code></pre>"},{"location":"ml/data-analysis/#rotas-mais-problematicas","title":"Rotas Mais Problem\u00e1ticas","text":"<pre><code>graph TD\n    A[SBGR - Guarulhos] --&gt;|35% atrasos| B[SBRJ - Santos Dumont]\n    A --&gt;|32% atrasos| C[SBGL - Gale\u00e3o]\n    D[SBSP - Congonhas] --&gt;|30% atrasos| B\n    D --&gt;|28% atrasos| E[SBCF - Confins]\n\n    style A fill:#ff6b6b\n    style B fill:#ff8e8e\n    style C fill:#ffa8a8\n    style D fill:#ffb3b3\n    style E fill:#ffc9c9</code></pre>"},{"location":"ml/data-analysis/#3-analise-temporal","title":"3. \u23f0 An\u00e1lise Temporal","text":""},{"location":"ml/data-analysis/#padroes-por-hora-do-dia","title":"Padr\u00f5es por Hora do Dia","text":"<pre><code># Distribui\u00e7\u00e3o de atrasos por hora\nhourly_delays = {\n    \"06:00-09:00\": {\"flights\": 8500, \"delay_rate\": 15.2},  # Manh\u00e3\n    \"09:00-12:00\": {\"flights\": 12000, \"delay_rate\": 28.5}, # Meio da manh\u00e3\n    \"12:00-15:00\": {\"flights\": 10500, \"delay_rate\": 32.1}, # Tarde\n    \"15:00-18:00\": {\"flights\": 11000, \"delay_rate\": 38.7}, # Final da tarde\n    \"18:00-21:00\": {\"flights\": 6500, \"delay_rate\": 42.3},  # Noite\n    \"21:00-06:00\": {\"flights\": 1500, \"delay_rate\": 25.8}   # Madrugada\n}\n</code></pre>"},{"location":"ml/data-analysis/#sazonalidade","title":"Sazonalidade","text":"<pre><code># An\u00e1lise por m\u00eas (dados anuais)\nmonthly_patterns = {\n    \"Janeiro\": {\"delay_rate\": 28.5, \"reason\": \"F\u00e9rias, alta demanda\"},\n    \"Dezembro\": {\"delay_rate\": 35.2, \"reason\": \"Festas, pico de viagens\"},\n    \"Julho\": {\"delay_rate\": 32.8, \"reason\": \"F\u00e9rias escolares\"},\n    \"Junho\": {\"delay_rate\": 25.1, \"reason\": \"Inverno, menos chuvas\"},\n    \"Mar\u00e7o\": {\"delay_rate\": 22.3, \"reason\": \"P\u00f3s-carnaval, normaliza\u00e7\u00e3o\"}\n}\n</code></pre>"},{"location":"ml/data-analysis/#4-impacto-do-clima","title":"4. \ud83c\udf24\ufe0f Impacto do Clima","text":""},{"location":"ml/data-analysis/#condicoes-meteorologicas-vs-atrasos","title":"Condi\u00e7\u00f5es Meteorol\u00f3gicas vs Atrasos","text":"<pre><code>weather_impact = {\n    \"Ensolarado\": {\"flights\": 25000, \"delay_rate\": 18.5},\n    \"Nublado\": {\"flights\": 15000, \"delay_rate\": 25.2},\n    \"Chuva Leve\": {\"flights\": 7000, \"delay_rate\": 35.8},\n    \"Chuva Forte\": {\"flights\": 2500, \"delay_rate\": 58.3},\n    \"Tempestade\": {\"flights\": 500, \"delay_rate\": 78.2},\n    \"Nevoeiro\": {\"flights\": 800, \"delay_rate\": 65.1}\n}\n</code></pre>"},{"location":"ml/data-analysis/#temperatura-e-ventos","title":"Temperatura e Ventos","text":"<pre><code># Correla\u00e7\u00f5es identificadas\nweather_correlations = {\n    \"temperatura_alta\": 0.23,      # Maior que 35\u00b0C aumenta atrasos\n    \"vento_forte\": 0.45,           # Ventos &gt; 40km/h impactam significativamente  \n    \"visibilidade_baixa\": 0.67,    # &lt; 2km tem forte correla\u00e7\u00e3o\n    \"precipitacao\": 0.58,          # Chuva \u00e9 fator cr\u00edtico\n    \"pressao_baixa\": 0.31          # Press\u00e3o &lt; 1010 hPa aumenta atrasos\n}\n</code></pre>"},{"location":"ml/data-analysis/#visualizacoes-principais","title":"\ud83d\udcca Visualiza\u00e7\u00f5es Principais","text":""},{"location":"ml/data-analysis/#1-distribuicao-de-atrasos","title":"1. \ud83d\udcc8 Distribui\u00e7\u00e3o de Atrasos","text":"<pre><code># Histograma de distribui\u00e7\u00e3o de atrasos\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Configura\u00e7\u00e3o do gr\u00e1fico\nplt.figure(figsize=(12, 6))\nsns.histplot(data=df, x='delay_minutes', bins=50, kde=True)\nplt.title('Distribui\u00e7\u00e3o de Atrasos em Minutos')\nplt.xlabel('Atraso (minutos)')\nplt.ylabel('Frequ\u00eancia')\n</code></pre>"},{"location":"ml/data-analysis/#2-heatmap-temporal","title":"2. \ud83d\udd52 Heatmap Temporal","text":"<pre><code># Heatmap de atrasos por hora e dia da semana\npivot_delays = df.pivot_table(\n    values='is_delayed', \n    index='hour', \n    columns='day_of_week', \n    aggfunc='mean'\n)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(pivot_delays, annot=True, cmap='Reds', fmt='.2f')\nplt.title('Taxa de Atrasos por Hora e Dia da Semana')\n</code></pre>"},{"location":"ml/data-analysis/#3-mapa-de-aeroportos","title":"3. \ud83c\udf0d Mapa de Aeroportos","text":"<pre><code># Visualiza\u00e7\u00e3o geogr\u00e1fica usando plotly\nimport plotly.graph_objects as go\n\nfig = go.Figure(data=go.Scattergeo(\n    lon = airports_df['longitude'],\n    lat = airports_df['latitude'],\n    text = airports_df['name'],\n    marker = dict(\n        size = airports_df['delay_rate'] * 2,\n        color = airports_df['delay_rate'],\n        colorscale = 'Reds',\n        showscale = True\n    )\n))\n</code></pre>"},{"location":"ml/data-analysis/#4-boxplots-por-companhia","title":"4. \ud83d\udcca Boxplots por Companhia","text":"<pre><code># Compara\u00e7\u00e3o de atrasos entre companhias\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=df, x='airline', y='delay_minutes')\nplt.xticks(rotation=45)\nplt.title('Distribui\u00e7\u00e3o de Atrasos por Companhia A\u00e9rea')\n</code></pre>"},{"location":"ml/data-analysis/#insights-descobertos","title":"\ud83d\udd0d Insights Descobertos","text":""},{"location":"ml/data-analysis/#principais-achados","title":"\ud83d\udca1 Principais Achados","text":"<ol> <li>Pico de Atrasos no Final do Dia</li> <li>18h-21h apresenta 42.3% de taxa de atraso</li> <li> <p>Congest\u00e3o aeroportu\u00e1ria \u00e9 o principal fator</p> </li> <li> <p>Impacto Cr\u00edtico do Clima</p> </li> <li>Tempestades causam 78% de atrasos</li> <li> <p>Nevoeiro tem segundo maior impacto (65%)</p> </li> <li> <p>Aeroportos Hub s\u00e3o Mais Problem\u00e1ticos</p> </li> <li>Guarulhos (SBGR): 35.2% de atrasos</li> <li> <p>Volume de tr\u00e1fego correlaciona com atrasos</p> </li> <li> <p>Sazonalidade Clara</p> </li> <li>Dezembro \u00e9 o pior m\u00eas (35.2%)</li> <li>Mar\u00e7o \u00e9 o melhor (22.3%)</li> </ol>"},{"location":"ml/data-analysis/#variaveis-mais-relevantes","title":"\ud83c\udfaf Vari\u00e1veis Mais Relevantes","text":"<pre><code># Feature importance descoberta na EDA\nfeature_importance = {\n    \"weather_conditions\": 0.68,      # Condi\u00e7\u00f5es clim\u00e1ticas\n    \"hour_of_day\": 0.45,            # Hora do voo\n    \"departure_airport\": 0.42,       # Aeroporto de origem\n    \"airline\": 0.38,                # Companhia a\u00e9rea\n    \"day_of_week\": 0.35,            # Dia da semana\n    \"month\": 0.32,                  # M\u00eas do ano\n    \"aircraft_type\": 0.28,          # Tipo de aeronave\n    \"route_distance\": 0.25,         # Dist\u00e2ncia da rota\n    \"wind_speed\": 0.23,             # Velocidade do vento\n    \"temperature\": 0.18             # Temperatura\n}\n</code></pre>"},{"location":"ml/data-analysis/#problemas-identificados-nos-dados","title":"\ud83e\uddf9 Problemas Identificados nos Dados","text":""},{"location":"ml/data-analysis/#qualidade-dos-dados","title":"\u26a0\ufe0f Qualidade dos Dados","text":"<ol> <li> <p>Valores Faltantes <pre><code>missing_data = {\n    \"weather_data\": \"5.2% (2,600 registros)\",\n    \"aircraft_type\": \"2.1% (1,050 registros)\", \n    \"delay_minutes\": \"0.8% (400 registros)\",\n    \"arrival_time\": \"0.3% (150 registros)\"\n}\n</code></pre></p> </li> <li> <p>Outliers Identificados <pre><code>outliers = {\n    \"delay_extreme\": \"67 voos com atraso &gt; 300 min\",\n    \"negative_delays\": \"1,200 voos com chegada antecipada &gt; 30 min\",\n    \"impossible_speeds\": \"15 voos com velocidade &gt; 1000 km/h\"\n}\n</code></pre></p> </li> <li> <p>Inconsist\u00eancias</p> </li> <li>Voos com hor\u00e1rio de chegada antes da partida</li> <li>Aeroportos com coordenadas incorretas</li> <li>C\u00f3digos ICAO inv\u00e1lidos</li> </ol>"},{"location":"ml/data-analysis/#recomendacoes-para-modelagem","title":"\ud83d\udccb Recomenda\u00e7\u00f5es para Modelagem","text":""},{"location":"ml/data-analysis/#estrategias-de-feature-engineering","title":"\ud83c\udfaf Estrat\u00e9gias de Feature Engineering","text":"<ol> <li> <p>Vari\u00e1veis Temporais <pre><code>temporal_features = [\n    \"hour_of_day\",           # Hora extra\u00edda\n    \"day_of_week\",          # Dia da semana\n    \"month\",                # M\u00eas\n    \"is_weekend\",           # Final de semana\n    \"is_holiday\",           # Feriado\n    \"quarter\",              # Trimestre\n    \"time_slot\"             # Faixa hor\u00e1ria\n]\n</code></pre></p> </li> <li> <p>Vari\u00e1veis de Clima <pre><code>weather_features = [\n    \"weather_severity\",      # Severidade do clima (0-5)\n    \"visibility_category\",   # Categorias de visibilidade\n    \"wind_category\",        # Categorias de vento\n    \"precipitation_level\",  # N\u00edveis de precipita\u00e7\u00e3o\n    \"weather_composite\"     # Score composto\n]\n</code></pre></p> </li> <li> <p>Vari\u00e1veis de Aeroporto <pre><code>airport_features = [\n    \"airport_size\",         # Tamanho do aeroporto\n    \"historical_delay_rate\", # Taxa hist\u00f3rica de atraso\n    \"traffic_volume\",       # Volume de tr\u00e1fego\n    \"runway_count\",         # N\u00famero de pistas\n    \"hub_status\"           # Status de hub\n]\n</code></pre></p> </li> </ol>"},{"location":"ml/data-analysis/#preprocessing-necessario","title":"\ud83d\udd04 Preprocessing Necess\u00e1rio","text":"<ol> <li>Tratamento de Outliers</li> <li>Winsoriza\u00e7\u00e3o nos atrasos extremos</li> <li> <p>Remo\u00e7\u00e3o de dados imposs\u00edveis</p> </li> <li> <p>Encoding de Vari\u00e1veis Categ\u00f3ricas</p> </li> <li>One-hot encoding para aeroportos</li> <li>Label encoding para companhias a\u00e9reas</li> <li> <p>Target encoding para rotas</p> </li> <li> <p>Normaliza\u00e7\u00e3o</p> </li> <li>StandardScaler para vari\u00e1veis num\u00e9ricas</li> <li>MinMaxScaler para vari\u00e1veis limitadas</li> </ol>"},{"location":"ml/data-analysis/#scripts-de-analise","title":"\ud83d\udcdd Scripts de An\u00e1lise","text":""},{"location":"ml/data-analysis/#notebook-principal","title":"\ud83d\udcca Notebook Principal","text":"<pre><code># eda_main.py - Script principal de EDA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\ndef load_and_explore_data():\n    \"\"\"Carrega e faz an\u00e1lise inicial dos dados\"\"\"\n\n    # Carregar dados\n    flights_df = pd.read_json('data/input/voos.json')\n    airports_df = pd.read_csv('data/input/airport_database/airports-database.csv')\n\n    # Info b\u00e1sica\n    print(\"=== INFORMA\u00c7\u00d5ES GERAIS ===\")\n    print(f\"Total de voos: {len(flights_df):,}\")\n    print(f\"Per\u00edodo: {flights_df['departure_time'].min()} at\u00e9 {flights_df['departure_time'].max()}\")\n    print(f\"Aeroportos \u00fanicos: {flights_df['departure_airport'].nunique()}\")\n    print(f\"Companhias \u00fanicas: {flights_df['airline'].nunique()}\")\n\n    return flights_df, airports_df\n\ndef analyze_delays(df):\n    \"\"\"An\u00e1lise detalhada de atrasos\"\"\"\n\n    delay_analysis = {\n        'total_flights': len(df),\n        'delayed_flights': len(df[df['is_delayed'] == True]),\n        'on_time_flights': len(df[df['is_delayed'] == False]),\n        'delay_rate': (df['is_delayed'].sum() / len(df)) * 100,\n        'avg_delay': df[df['is_delayed']]['delay_minutes'].mean(),\n        'median_delay': df[df['is_delayed']]['delay_minutes'].median(),\n        'max_delay': df['delay_minutes'].max()\n    }\n\n    return delay_analysis\n\ndef weather_impact_analysis(df):\n    \"\"\"An\u00e1lise do impacto do clima\"\"\"\n\n    weather_groups = df.groupby('weather_conditions').agg({\n        'is_delayed': ['count', 'sum', 'mean'],\n        'delay_minutes': ['mean', 'median', 'max']\n    }).round(2)\n\n    return weather_groups\n\nif __name__ == \"__main__\":\n    # Executar an\u00e1lises\n    flights_df, airports_df = load_and_explore_data()\n    delay_stats = analyze_delays(flights_df)\n    weather_impact = weather_impact_analysis(flights_df)\n\n    print(\"An\u00e1lise completa!\")\n</code></pre>"},{"location":"ml/data-analysis/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udcca Pr\u00e9-processamento - Limpeza e prepara\u00e7\u00e3o dos dados</li> <li>\ud83e\udd16 Treinamento - Desenvolvimento dos modelos preditivos</li> <li>\ud83d\udcc8 Avalia\u00e7\u00e3o - M\u00e9tricas e valida\u00e7\u00e3o dos modelos</li> </ol>"},{"location":"ml/data-analysis/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83d\udcd3 Notebook EDA - An\u00e1lise interativa completa</li> <li>\ud83c\udfaf Arquitetura ML - Pipeline de machine learning</li> <li>\ud83d\udcca Dados - Como acessar os datasets</li> </ul>"},{"location":"ml/evaluation/","title":"\ud83d\udcc8 Avalia\u00e7\u00e3o de Modelos","text":"<p>Guia completo para avalia\u00e7\u00e3o e valida\u00e7\u00e3o de modelos de machine learning, incluindo m\u00e9tricas de performance, an\u00e1lise de erros, interpretabilidade e valida\u00e7\u00e3o em produ\u00e7\u00e3o.</p>"},{"location":"ml/evaluation/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>A avalia\u00e7\u00e3o de modelos \u00e9 crucial para garantir que o sistema de predi\u00e7\u00e3o de atrasos seja confi\u00e1vel, robusto e adequado para uso em produ\u00e7\u00e3o. Esta se\u00e7\u00e3o detalha todas as m\u00e9tricas e t\u00e9cnicas utilizadas.</p>"},{"location":"ml/evaluation/#framework-de-avaliacao","title":"\ud83d\udcca Framework de Avalia\u00e7\u00e3o","text":"<pre><code>graph TD\n    A[Modelo Treinado] --&gt; B[M\u00e9tricas B\u00e1sicas]\n    B --&gt; C[An\u00e1lise de Erros]\n    C --&gt; D[Valida\u00e7\u00e3o Cruzada]\n    D --&gt; E[Testes de Robustez]\n    E --&gt; F[Interpretabilidade]\n    F --&gt; G[Valida\u00e7\u00e3o de Neg\u00f3cio]\n    G --&gt; H[Aprova\u00e7\u00e3o para Produ\u00e7\u00e3o]\n\n    style A fill:#e3f2fd\n    style H fill:#c8e6c9\n    style F fill:#fff3e0</code></pre>"},{"location":"ml/evaluation/#metricas-de-performance","title":"\ud83d\udccf M\u00e9tricas de Performance","text":""},{"location":"ml/evaluation/#1-metricas-principais","title":"1. \ud83c\udfaf M\u00e9tricas Principais","text":""},{"location":"ml/evaluation/#classificacao-binaria","title":"Classifica\u00e7\u00e3o Bin\u00e1ria","text":"<pre><code>from sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, average_precision_score, confusion_matrix,\n    classification_report, roc_curve, precision_recall_curve\n)\n\ndef calculate_comprehensive_metrics(y_true, y_pred, y_pred_proba):\n    \"\"\"Calcula todas as m\u00e9tricas de avalia\u00e7\u00e3o\"\"\"\n\n    metrics = {\n        # M\u00e9tricas b\u00e1sicas\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred),\n        'recall': recall_score(y_true, y_pred),\n        'f1_score': f1_score(y_true, y_pred),\n        'specificity': recall_score(y_true, y_pred, pos_label=0),\n\n        # M\u00e9tricas de ranking\n        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n        'average_precision': average_precision_score(y_true, y_pred_proba),\n\n        # Matriz de confus\u00e3o\n        'confusion_matrix': confusion_matrix(y_true, y_pred),\n\n        # Relat\u00f3rio detalhado\n        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n    }\n\n    return metrics\n</code></pre>"},{"location":"ml/evaluation/#metricas-customizadas-para-negocio","title":"M\u00e9tricas Customizadas para Neg\u00f3cio","text":"<pre><code>def business_metrics(y_true, y_pred, y_pred_proba, cost_matrix=None):\n    \"\"\"M\u00e9tricas espec\u00edficas do neg\u00f3cio de avia\u00e7\u00e3o\"\"\"\n\n    if cost_matrix is None:\n        # Custos estimados (em R$)\n        cost_matrix = {\n            'false_positive': 50,    # Custo de prepara\u00e7\u00e3o desnecess\u00e1ria\n            'false_negative': 200,   # Custo de atraso n\u00e3o previsto\n            'true_positive': -20,    # Economia por preven\u00e7\u00e3o\n            'true_negative': 0       # Situa\u00e7\u00e3o normal\n        }\n\n    # Matriz de confus\u00e3o\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n\n    # Custo total\n    total_cost = (\n        fp * cost_matrix['false_positive'] +\n        fn * cost_matrix['false_negative'] +\n        tp * cost_matrix['true_positive'] +\n        tn * cost_matrix['true_negative']\n    )\n\n    # M\u00e9tricas de neg\u00f3cio\n    business_metrics = {\n        'total_cost': total_cost,\n        'cost_per_prediction': total_cost / len(y_true),\n        'prevention_rate': tp / (tp + fn) if (tp + fn) &gt; 0 else 0,\n        'false_alarm_rate': fp / (fp + tn) if (fp + tn) &gt; 0 else 0,\n        'economic_value': -total_cost,  # Valor econ\u00f4mico (negativo do custo)\n\n        # M\u00e9tricas operacionais\n        'delays_caught': tp,\n        'delays_missed': fn,\n        'false_alerts': fp,\n        'correct_non_delays': tn\n    }\n\n    return business_metrics\n</code></pre>"},{"location":"ml/evaluation/#2-visualizacao-de-metricas","title":"2. \ud83d\udcca Visualiza\u00e7\u00e3o de M\u00e9tricas","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n\nclass ModelEvaluationVisualizer:\n    \"\"\"Classe para visualiza\u00e7\u00e3o de m\u00e9tricas de avalia\u00e7\u00e3o\"\"\"\n\n    def __init__(self, figsize=(15, 10)):\n        self.figsize = figsize\n\n    def comprehensive_evaluation_plot(self, model, X_test, y_test, model_name=\"Model\"):\n        \"\"\"Cria visualiza\u00e7\u00e3o completa de avalia\u00e7\u00e3o\"\"\"\n\n        fig, axes = plt.subplots(2, 3, figsize=self.figsize)\n        fig.suptitle(f'Avalia\u00e7\u00e3o Completa - {model_name}', fontsize=16, fontweight='bold')\n\n        # 1. Matriz de Confus\u00e3o\n        plot_confusion_matrix(model, X_test, y_test, ax=axes[0,0], \n                            cmap='Blues', values_format='d')\n        axes[0,0].set_title('Matriz de Confus\u00e3o')\n\n        # 2. ROC Curve\n        plot_roc_curve(model, X_test, y_test, ax=axes[0,1])\n        axes[0,1].set_title('Curva ROC')\n        axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.6)\n\n        # 3. Precision-Recall Curve\n        plot_precision_recall_curve(model, X_test, y_test, ax=axes[0,2])\n        axes[0,2].set_title('Curva Precis\u00e3o-Recall')\n\n        # 4. Distribui\u00e7\u00e3o de Probabilidades\n        y_pred_proba = model.predict_proba(X_test)[:, 1]\n\n        axes[1,0].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, \n                      label='N\u00e3o Atrasado', color='blue')\n        axes[1,0].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, \n                      label='Atrasado', color='red')\n        axes[1,0].set_xlabel('Probabilidade Predita')\n        axes[1,0].set_ylabel('Frequ\u00eancia')\n        axes[1,0].set_title('Distribui\u00e7\u00e3o de Probabilidades')\n        axes[1,0].legend()\n\n        # 5. Feature Importance (se dispon\u00edvel)\n        if hasattr(model, 'feature_importances_'):\n            importances = model.feature_importances_\n            feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(len(importances))]\n\n            # Top 15 features\n            indices = np.argsort(importances)[-15:]\n\n            axes[1,1].barh(range(len(indices)), importances[indices])\n            axes[1,1].set_yticks(range(len(indices)))\n            axes[1,1].set_yticklabels([feature_names[i] for i in indices])\n            axes[1,1].set_xlabel('Import\u00e2ncia')\n            axes[1,1].set_title('Top 15 Features Mais Importantes')\n\n        # 6. Calibra\u00e7\u00e3o do Modelo\n        from sklearn.calibration import calibration_curve\n\n        fraction_of_positives, mean_predicted_value = calibration_curve(\n            y_test, y_pred_proba, n_bins=10\n        )\n\n        axes[1,2].plot(mean_predicted_value, fraction_of_positives, \"s-\", \n                      label=f\"{model_name}\")\n        axes[1,2].plot([0, 1], [0, 1], \"k:\", label=\"Perfeitamente Calibrado\")\n        axes[1,2].set_xlabel('Probabilidade M\u00e9dia Predita')\n        axes[1,2].set_ylabel('Fra\u00e7\u00e3o de Positivos')\n        axes[1,2].set_title('Curva de Calibra\u00e7\u00e3o')\n        axes[1,2].legend()\n\n        plt.tight_layout()\n        plt.show()\n\n        return fig\n</code></pre>"},{"location":"ml/evaluation/#analise-de-erros","title":"\ud83d\udd0d An\u00e1lise de Erros","text":""},{"location":"ml/evaluation/#1-analise-de-falsos-positivos-e-negativos","title":"1. \ud83c\udfaf An\u00e1lise de Falsos Positivos e Negativos","text":"<pre><code>def error_analysis(X_test, y_test, y_pred, y_pred_proba, feature_names=None):\n    \"\"\"An\u00e1lise detalhada de erros do modelo\"\"\"\n\n    # Identificar tipos de erro\n    false_positives = (y_test == 0) &amp; (y_pred == 1)\n    false_negatives = (y_test == 1) &amp; (y_pred == 0)\n    true_positives = (y_test == 1) &amp; (y_pred == 1)\n    true_negatives = (y_test == 0) &amp; (y_pred == 0)\n\n    error_analysis_results = {\n        'false_positives': {\n            'count': false_positives.sum(),\n            'percentage': false_positives.mean() * 100,\n            'avg_probability': y_pred_proba[false_positives].mean(),\n            'samples': X_test[false_positives].head(10) if hasattr(X_test, 'head') else None\n        },\n\n        'false_negatives': {\n            'count': false_negatives.sum(),\n            'percentage': false_negatives.mean() * 100,\n            'avg_probability': y_pred_proba[false_negatives].mean(),\n            'samples': X_test[false_negatives].head(10) if hasattr(X_test, 'head') else None\n        },\n\n        'true_positives': {\n            'count': true_positives.sum(),\n            'avg_probability': y_pred_proba[true_positives].mean()\n        },\n\n        'true_negatives': {\n            'count': true_negatives.sum(),\n            'avg_probability': y_pred_proba[true_negatives].mean()\n        }\n    }\n\n    # An\u00e1lise por faixas de probabilidade\n    prob_bins = np.linspace(0, 1, 11)\n    prob_analysis = {}\n\n    for i in range(len(prob_bins) - 1):\n        bin_mask = (y_pred_proba &gt;= prob_bins[i]) &amp; (y_pred_proba &lt; prob_bins[i+1])\n\n        if bin_mask.sum() &gt; 0:\n            prob_analysis[f\"{prob_bins[i]:.1f}-{prob_bins[i+1]:.1f}\"] = {\n                'total_predictions': bin_mask.sum(),\n                'actual_positive_rate': y_test[bin_mask].mean(),\n                'predicted_probability': y_pred_proba[bin_mask].mean()\n            }\n\n    error_analysis_results['probability_analysis'] = prob_analysis\n\n    return error_analysis_results\n</code></pre>"},{"location":"ml/evaluation/#2-analise-de-segmentos","title":"2. \ud83d\udcca An\u00e1lise de Segmentos","text":"<pre><code>def segment_analysis(X_test, y_test, y_pred, y_pred_proba, segments):\n    \"\"\"An\u00e1lise de performance por segmentos espec\u00edficos\"\"\"\n\n    segment_results = {}\n\n    for segment_name, segment_condition in segments.items():\n        # Aplicar condi\u00e7\u00e3o do segmento\n        if callable(segment_condition):\n            segment_mask = segment_condition(X_test)\n        else:\n            segment_mask = segment_condition\n\n        if segment_mask.sum() == 0:\n            continue\n\n        # M\u00e9tricas do segmento\n        segment_y_test = y_test[segment_mask]\n        segment_y_pred = y_pred[segment_mask]\n        segment_y_pred_proba = y_pred_proba[segment_mask]\n\n        segment_metrics = calculate_comprehensive_metrics(\n            segment_y_test, segment_y_pred, segment_y_pred_proba\n        )\n\n        segment_results[segment_name] = {\n            'size': segment_mask.sum(),\n            'percentage': (segment_mask.sum() / len(X_test)) * 100,\n            'metrics': segment_metrics,\n            'baseline_delay_rate': segment_y_test.mean()\n        }\n\n    return segment_results\n</code></pre>"},{"location":"ml/evaluation/#validacao-cruzada-avancada","title":"\ud83d\udd04 Valida\u00e7\u00e3o Cruzada Avan\u00e7ada","text":""},{"location":"ml/evaluation/#1-validacao-temporal-com-walk-forward","title":"1. \u23f0 Valida\u00e7\u00e3o Temporal com Walk-Forward","text":"<pre><code>def walk_forward_validation(model, X, y, initial_train_size=0.6, step_size=0.1):\n    \"\"\"Valida\u00e7\u00e3o walk-forward para s\u00e9ries temporais\"\"\"\n\n    n_samples = len(X)\n    results = []\n\n    # Tamanho inicial do treino\n    train_end = int(n_samples * initial_train_size)\n\n    while train_end + int(n_samples * step_size) &lt;= n_samples:\n\n        # Definir janelas\n        test_start = train_end\n        test_end = min(train_end + int(n_samples * step_size), n_samples)\n\n        # Dividir dados\n        X_train_fold = X.iloc[:train_end]\n        X_test_fold = X.iloc[test_start:test_end]\n        y_train_fold = y.iloc[:train_end]\n        y_test_fold = y.iloc[test_start:test_end]\n\n        # Treinar modelo\n        model_fold = clone(model)\n        model_fold.fit(X_train_fold, y_train_fold)\n\n        # Predi\u00e7\u00f5es\n        y_pred_proba_fold = model_fold.predict_proba(X_test_fold)[:, 1]\n        y_pred_fold = (y_pred_proba_fold &gt; 0.5).astype(int)\n\n        # M\u00e9tricas\n        fold_metrics = calculate_comprehensive_metrics(\n            y_test_fold, y_pred_fold, y_pred_proba_fold\n        )\n\n        results.append({\n            'fold': len(results) + 1,\n            'train_size': len(X_train_fold),\n            'test_size': len(X_test_fold),\n            'train_end': train_end,\n            'test_start': test_start,\n            'test_end': test_end,\n            'metrics': fold_metrics\n        })\n\n        # Pr\u00f3xima itera\u00e7\u00e3o\n        train_end = test_end\n\n        print(f\"Fold {len(results)}: Train={len(X_train_fold)}, Test={len(X_test_fold)}, AUC={fold_metrics['roc_auc']:.4f}\")\n\n    # Resumir resultados\n    avg_metrics = {}\n    for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n        values = [r['metrics'][metric] for r in results]\n        avg_metrics[f\"{metric}_mean\"] = np.mean(values)\n        avg_metrics[f\"{metric}_std\"] = np.std(values)\n\n    return results, avg_metrics\n</code></pre>"},{"location":"ml/evaluation/#2-validacao-estratificada-por-segmentos","title":"2. \ud83c\udfaf Valida\u00e7\u00e3o Estratificada por Segmentos","text":"<pre><code>from sklearn.model_selection import StratifiedGroupKFold\n\ndef stratified_group_validation(model, X, y, groups, n_splits=5):\n    \"\"\"Valida\u00e7\u00e3o cruzada estratificada por grupos\"\"\"\n\n    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    cv_results = []\n\n    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X, y, groups)):\n\n        print(f\"Processando fold {fold + 1}/{n_splits}\")\n\n        # Dividir dados\n        X_train_fold = X.iloc[train_idx]\n        X_test_fold = X.iloc[test_idx]\n        y_train_fold = y.iloc[train_idx]\n        y_test_fold = y.iloc[test_idx]\n\n        # Treinar modelo\n        model_fold = clone(model)\n        model_fold.fit(X_train_fold, y_train_fold)\n\n        # Predi\u00e7\u00f5es\n        y_pred_proba_fold = model_fold.predict_proba(X_test_fold)[:, 1]\n        y_pred_fold = (y_pred_proba_fold &gt; 0.5).astype(int)\n\n        # M\u00e9tricas\n        fold_metrics = calculate_comprehensive_metrics(\n            y_test_fold, y_pred_fold, y_pred_proba_fold\n        )\n\n        cv_results.append({\n            'fold': fold + 1,\n            'train_groups': np.unique(groups[train_idx]),\n            'test_groups': np.unique(groups[test_idx]),\n            'metrics': fold_metrics\n        })\n\n    return cv_results\n</code></pre>"},{"location":"ml/evaluation/#testes-de-robustez","title":"\ud83d\udee1\ufe0f Testes de Robustez","text":""},{"location":"ml/evaluation/#1-teste-de-estabilidade","title":"1. \ud83d\udd04 Teste de Estabilidade","text":"<pre><code>def stability_test(model, X_test, y_test, n_iterations=100):\n    \"\"\"Testa estabilidade das predi\u00e7\u00f5es do modelo\"\"\"\n\n    predictions = []\n    probabilities = []\n\n    for i in range(n_iterations):\n        # Adicionar ru\u00eddo pequeno aos dados (simular varia\u00e7\u00e3o real)\n        X_test_noisy = X_test + np.random.normal(0, 0.01, X_test.shape)\n\n        # Predi\u00e7\u00f5es\n        y_pred_proba = model.predict_proba(X_test_noisy)[:, 1]\n        y_pred = (y_pred_proba &gt; 0.5).astype(int)\n\n        predictions.append(y_pred)\n        probabilities.append(y_pred_proba)\n\n    # An\u00e1lise de estabilidade\n    predictions_array = np.array(predictions)\n    probabilities_array = np.array(probabilities)\n\n    stability_metrics = {\n        'prediction_variance': np.var(predictions_array, axis=0).mean(),\n        'probability_variance': np.var(probabilities_array, axis=0).mean(),\n        'prediction_stability': (predictions_array.std(axis=0) &lt; 0.1).mean(),\n        'flip_rate': (predictions_array.std(axis=0) &gt; 0).mean()\n    }\n\n    return stability_metrics\n</code></pre>"},{"location":"ml/evaluation/#2-teste-de-adversarial","title":"2. \ud83c\udfad Teste de Adversarial","text":"<pre><code>def adversarial_test(model, X_test, y_test, epsilon=0.1):\n    \"\"\"Teste de robustez adversarial (para modelos lineares)\"\"\"\n\n    if not hasattr(model, 'coef_'):\n        print(\"\u26a0\ufe0f Teste adversarial dispon\u00edvel apenas para modelos lineares\")\n        return None\n\n    # Calcular gradiente (dire\u00e7\u00e3o de maior mudan\u00e7a)\n    gradient = model.coef_[0]\n\n    # Normalizar gradiente\n    gradient_norm = gradient / np.linalg.norm(gradient)\n\n    # Criar exemplos adversariais\n    X_adversarial = X_test + epsilon * gradient_norm\n\n    # Predi\u00e7\u00f5es originais vs adversariais\n    y_pred_original = model.predict_proba(X_test)[:, 1]\n    y_pred_adversarial = model.predict_proba(X_adversarial)[:, 1]\n\n    # M\u00e9tricas de robustez\n    robustness_metrics = {\n        'max_probability_change': np.max(np.abs(y_pred_adversarial - y_pred_original)),\n        'mean_probability_change': np.mean(np.abs(y_pred_adversarial - y_pred_original)),\n        'prediction_flip_rate': np.mean((y_pred_adversarial &gt; 0.5) != (y_pred_original &gt; 0.5)),\n        'robustness_score': 1 - np.mean(np.abs(y_pred_adversarial - y_pred_original))\n    }\n\n    return robustness_metrics\n</code></pre>"},{"location":"ml/evaluation/#interpretabilidade","title":"\ud83d\udd0d Interpretabilidade","text":""},{"location":"ml/evaluation/#1-shap-values","title":"1. \ud83c\udf1f SHAP Values","text":"<pre><code>import shap\n\ndef shap_analysis(model, X_train, X_test, feature_names=None):\n    \"\"\"An\u00e1lise de interpretabilidade usando SHAP\"\"\"\n\n    try:\n        # Diferentes explicadores para diferentes tipos de modelo\n        if hasattr(model, 'tree_'):\n            explainer = shap.TreeExplainer(model)\n        elif hasattr(model, 'coef_'):\n            explainer = shap.LinearExplainer(model, X_train)\n        else:\n            explainer = shap.KernelExplainer(model.predict_proba, X_train.sample(100))\n\n        # Calcular SHAP values\n        shap_values = explainer.shap_values(X_test[:100])  # Primeiras 100 amostras\n\n        # Se \u00e9 classifica\u00e7\u00e3o bin\u00e1ria, pegar valores da classe positiva\n        if isinstance(shap_values, list):\n            shap_values = shap_values[1]\n\n        # An\u00e1lise dos SHAP values\n        feature_importance = np.abs(shap_values).mean(axis=0)\n\n        shap_results = {\n            'shap_values': shap_values,\n            'feature_importance': feature_importance,\n            'feature_names': feature_names or [f'Feature_{i}' for i in range(shap_values.shape[1])],\n            'explainer': explainer\n        }\n\n        return shap_results\n\n    except Exception as e:\n        print(f\"\u26a0\ufe0f Erro na an\u00e1lise SHAP: {e}\")\n        return None\n</code></pre>"},{"location":"ml/evaluation/#2-permutation-importance","title":"2. \ud83d\udcca Permutation Importance","text":"<pre><code>from sklearn.inspection import permutation_importance\n\ndef permutation_importance_analysis(model, X_test, y_test, n_repeats=10):\n    \"\"\"An\u00e1lise de import\u00e2ncia por permuta\u00e7\u00e3o\"\"\"\n\n    # Calcular import\u00e2ncia por permuta\u00e7\u00e3o\n    perm_importance = permutation_importance(\n        model, X_test, y_test, \n        n_repeats=n_repeats, \n        random_state=42,\n        scoring='roc_auc'\n    )\n\n    # Organizar resultados\n    feature_names = X_test.columns if hasattr(X_test, 'columns') else [f'Feature_{i}' for i in range(X_test.shape[1])]\n\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance_mean': perm_importance.importances_mean,\n        'importance_std': perm_importance.importances_std\n    }).sort_values('importance_mean', ascending=False)\n\n    return importance_df, perm_importance\n</code></pre>"},{"location":"ml/evaluation/#3-feature-interactions","title":"3. \ud83c\udfaf Feature Interactions","text":"<pre><code>def analyze_feature_interactions(model, X_test, feature_pairs=None):\n    \"\"\"An\u00e1lise de intera\u00e7\u00f5es entre features\"\"\"\n\n    if feature_pairs is None:\n        # Selecionar pares de features mais importantes\n        if hasattr(model, 'feature_importances_'):\n            importances = model.feature_importances_\n            top_features_idx = np.argsort(importances)[-10:]  # Top 10\n            feature_pairs = [(i, j) for i in top_features_idx for j in top_features_idx if i &lt; j]\n        else:\n            print(\"\u26a0\ufe0f Modelo n\u00e3o suporta an\u00e1lise autom\u00e1tica de intera\u00e7\u00f5es\")\n            return None\n\n    interaction_effects = []\n\n    for feat1_idx, feat2_idx in feature_pairs:\n\n        # Criar grid de valores para as duas features\n        feat1_values = np.linspace(X_test.iloc[:, feat1_idx].min(), X_test.iloc[:, feat1_idx].max(), 10)\n        feat2_values = np.linspace(X_test.iloc[:, feat2_idx].min(), X_test.iloc[:, feat2_idx].max(), 10)\n\n        # Calcular predi\u00e7\u00f5es no grid\n        interaction_grid = np.zeros((len(feat1_values), len(feat2_values)))\n\n        for i, val1 in enumerate(feat1_values):\n            for j, val2 in enumerate(feat2_values):\n                # Criar amostra com valores espec\u00edficos\n                X_sample = X_test.iloc[0:1].copy()\n                X_sample.iloc[0, feat1_idx] = val1\n                X_sample.iloc[0, feat2_idx] = val2\n\n                # Predi\u00e7\u00e3o\n                pred_proba = model.predict_proba(X_sample)[0, 1]\n                interaction_grid[i, j] = pred_proba\n\n        interaction_effects.append({\n            'feature1_idx': feat1_idx,\n            'feature2_idx': feat2_idx,\n            'feature1_name': X_test.columns[feat1_idx] if hasattr(X_test, 'columns') else f'Feature_{feat1_idx}',\n            'feature2_name': X_test.columns[feat2_idx] if hasattr(X_test, 'columns') else f'Feature_{feat2_idx}',\n            'interaction_grid': interaction_grid,\n            'feature1_values': feat1_values,\n            'feature2_values': feat2_values\n        })\n\n    return interaction_effects\n</code></pre>"},{"location":"ml/evaluation/#validacao-de-negocio","title":"\ud83d\udcca Valida\u00e7\u00e3o de Neg\u00f3cio","text":""},{"location":"ml/evaluation/#1-analise-de-valor-economico","title":"1. \ud83d\udcb0 An\u00e1lise de Valor Econ\u00f4mico","text":"<pre><code>def economic_value_analysis(y_test, y_pred_proba, thresholds=None, cost_matrix=None):\n    \"\"\"An\u00e1lise de valor econ\u00f4mico para diferentes thresholds\"\"\"\n\n    if thresholds is None:\n        thresholds = np.linspace(0.1, 0.9, 17)\n\n    if cost_matrix is None:\n        cost_matrix = {\n            'false_positive': 50,\n            'false_negative': 200,\n            'true_positive': -20,\n            'true_negative': 0\n        }\n\n    threshold_analysis = []\n\n    for threshold in thresholds:\n        y_pred = (y_pred_proba &gt;= threshold).astype(int)\n\n        # M\u00e9tricas b\u00e1sicas\n        metrics = calculate_comprehensive_metrics(y_test, y_pred, y_pred_proba)\n\n        # M\u00e9tricas de neg\u00f3cio\n        business_metrics_result = business_metrics(y_test, y_pred, y_pred_proba, cost_matrix)\n\n        threshold_analysis.append({\n            'threshold': threshold,\n            'precision': metrics['precision'],\n            'recall': metrics['recall'],\n            'f1_score': metrics['f1_score'],\n            'economic_value': business_metrics_result['economic_value'],\n            'cost_per_prediction': business_metrics_result['cost_per_prediction'],\n            'prevention_rate': business_metrics_result['prevention_rate']\n        })\n\n    # Encontrar threshold \u00f3timo\n    threshold_df = pd.DataFrame(threshold_analysis)\n    optimal_threshold_idx = threshold_df['economic_value'].idxmax()\n    optimal_threshold = threshold_df.iloc[optimal_threshold_idx]\n\n    print(f\"\ud83c\udfaf Threshold \u00f3timo: {optimal_threshold['threshold']:.3f}\")\n    print(f\"\ud83d\udcb0 Valor econ\u00f4mico: R$ {optimal_threshold['economic_value']:,.2f}\")\n    print(f\"\ud83c\udfaf Taxa de preven\u00e7\u00e3o: {optimal_threshold['prevention_rate']:.1%}\")\n\n    return threshold_df, optimal_threshold\n</code></pre>"},{"location":"ml/evaluation/#2-analise-operacional","title":"2. \ud83c\udfaf An\u00e1lise Operacional","text":"<pre><code>def operational_analysis(y_test, y_pred, y_pred_proba, operational_constraints=None):\n    \"\"\"An\u00e1lise de viabilidade operacional\"\"\"\n\n    if operational_constraints is None:\n        operational_constraints = {\n            'max_daily_alerts': 100,          # M\u00e1ximo de alertas por dia\n            'min_precision': 0.70,            # Precis\u00e3o m\u00ednima aceit\u00e1vel\n            'min_recall': 0.60,               # Recall m\u00ednimo aceit\u00e1vel\n            'response_time_limit': 300        # Limite de tempo de resposta (segundos)\n        }\n\n    # Simular cen\u00e1rio operacional\n    n_days = 30  # An\u00e1lise para 30 dias\n    predictions_per_day = len(y_test) / n_days\n    alerts_per_day = np.sum(y_pred) / n_days\n\n    operational_metrics = {\n        'predictions_per_day': predictions_per_day,\n        'alerts_per_day': alerts_per_day,\n        'alert_rate': alerts_per_day / predictions_per_day,\n        'workload_feasible': alerts_per_day &lt;= operational_constraints['max_daily_alerts'],\n        'precision_ok': precision_score(y_test, y_pred) &gt;= operational_constraints['min_precision'],\n        'recall_ok': recall_score(y_test, y_pred) &gt;= operational_constraints['min_recall']\n    }\n\n    # Avalia\u00e7\u00e3o geral\n    operational_metrics['operationally_viable'] = (\n        operational_metrics['workload_feasible'] and\n        operational_metrics['precision_ok'] and\n        operational_metrics['recall_ok']\n    )\n\n    print(\"\ud83d\udcca AN\u00c1LISE OPERACIONAL\")\n    print(\"=\" * 40)\n    print(f\"Predi\u00e7\u00f5es por dia: {operational_metrics['predictions_per_day']:.1f}\")\n    print(f\"Alertas por dia: {operational_metrics['alerts_per_day']:.1f}\")\n    print(f\"Taxa de alertas: {operational_metrics['alert_rate']:.1%}\")\n    print(f\"Vi\u00e1vel operacionalmente: {'\u2705' if operational_metrics['operationally_viable'] else '\u274c'}\")\n\n    return operational_metrics\n</code></pre>"},{"location":"ml/evaluation/#relatorio-completo-de-avaliacao","title":"\ud83d\udccb Relat\u00f3rio Completo de Avalia\u00e7\u00e3o","text":""},{"location":"ml/evaluation/#classe-geradora-de-relatorio","title":"\ud83d\udcc4 Classe Geradora de Relat\u00f3rio","text":"<pre><code>class ModelEvaluationReport:\n    \"\"\"Classe para gerar relat\u00f3rio completo de avalia\u00e7\u00e3o\"\"\"\n\n    def __init__(self, model, model_name):\n        self.model = model\n        self.model_name = model_name\n        self.evaluation_results = {}\n\n    def full_evaluation(self, X_test, y_test, X_train=None):\n        \"\"\"Avalia\u00e7\u00e3o completa do modelo\"\"\"\n\n        print(f\"\ud83d\udd0d Iniciando avalia\u00e7\u00e3o completa: {self.model_name}\")\n        print(\"=\" * 60)\n\n        # Predi\u00e7\u00f5es\n        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n        y_pred = (y_pred_proba &gt; 0.5).astype(int)\n\n        # 1. M\u00e9tricas b\u00e1sicas\n        print(\"\\n\ud83d\udcca 1. M\u00c9TRICAS B\u00c1SICAS\")\n        basic_metrics = calculate_comprehensive_metrics(y_test, y_pred, y_pred_proba)\n        self.evaluation_results['basic_metrics'] = basic_metrics\n\n        print(f\"Accuracy: {basic_metrics['accuracy']:.4f}\")\n        print(f\"Precision: {basic_metrics['precision']:.4f}\")\n        print(f\"Recall: {basic_metrics['recall']:.4f}\")\n        print(f\"F1-Score: {basic_metrics['f1_score']:.4f}\")\n        print(f\"ROC-AUC: {basic_metrics['roc_auc']:.4f}\")\n\n        # 2. M\u00e9tricas de neg\u00f3cio\n        print(\"\\n\ud83d\udcb0 2. M\u00c9TRICAS DE NEG\u00d3CIO\")\n        business_metrics_result = business_metrics(y_test, y_pred, y_pred_proba)\n        self.evaluation_results['business_metrics'] = business_metrics_result\n\n        print(f\"Valor econ\u00f4mico: R$ {business_metrics_result['economic_value']:,.2f}\")\n        print(f\"Taxa de preven\u00e7\u00e3o: {business_metrics_result['prevention_rate']:.1%}\")\n        print(f\"Taxa de falsos alarmes: {business_metrics_result['false_alarm_rate']:.1%}\")\n\n        # 3. An\u00e1lise de erros\n        print(\"\\n\ud83d\udd0d 3. AN\u00c1LISE DE ERROS\")\n        error_analysis_result = error_analysis(X_test, y_test, y_pred, y_pred_proba)\n        self.evaluation_results['error_analysis'] = error_analysis_result\n\n        print(f\"Falsos positivos: {error_analysis_result['false_positives']['count']} ({error_analysis_result['false_positives']['percentage']:.1f}%)\")\n        print(f\"Falsos negativos: {error_analysis_result['false_negatives']['count']} ({error_analysis_result['false_negatives']['percentage']:.1f}%)\")\n\n        # 4. Teste de estabilidade\n        print(\"\\n\ud83d\udee1\ufe0f 4. TESTE DE ESTABILIDADE\")\n        stability_result = stability_test(self.model, X_test, y_test)\n        self.evaluation_results['stability'] = stability_result\n\n        print(f\"Estabilidade de predi\u00e7\u00e3o: {stability_result['prediction_stability']:.1%}\")\n        print(f\"Taxa de mudan\u00e7a: {stability_result['flip_rate']:.1%}\")\n\n        # 5. An\u00e1lise operacional\n        print(\"\\n\ud83d\udccb 5. AN\u00c1LISE OPERACIONAL\")\n        operational_result = operational_analysis(y_test, y_pred, y_pred_proba)\n        self.evaluation_results['operational'] = operational_result\n\n        # 6. Interpretabilidade (se X_train fornecido)\n        if X_train is not None:\n            print(\"\\n\ud83d\udd0d 6. AN\u00c1LISE DE INTERPRETABILIDADE\")\n\n            try:\n                shap_result = shap_analysis(self.model, X_train, X_test)\n                if shap_result:\n                    self.evaluation_results['shap_analysis'] = shap_result\n                    print(\"\u2705 An\u00e1lise SHAP conclu\u00edda\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Erro na an\u00e1lise SHAP: {e}\")\n\n            try:\n                perm_importance_df, perm_importance = permutation_importance_analysis(\n                    self.model, X_test, y_test\n                )\n                self.evaluation_results['permutation_importance'] = {\n                    'dataframe': perm_importance_df,\n                    'raw_results': perm_importance\n                }\n                print(\"\u2705 An\u00e1lise de import\u00e2ncia por permuta\u00e7\u00e3o conclu\u00edda\")\n            except Exception as e:\n                print(f\"\u26a0\ufe0f Erro na an\u00e1lise de permuta\u00e7\u00e3o: {e}\")\n\n        print(\"\\n\u2705 Avalia\u00e7\u00e3o completa finalizada!\")\n\n        return self.evaluation_results\n\n    def generate_summary_report(self):\n        \"\"\"Gera resumo executivo da avalia\u00e7\u00e3o\"\"\"\n\n        if not self.evaluation_results:\n            print(\"\u274c Execute full_evaluation() primeiro\")\n            return\n\n        print(f\"\\n\ud83d\udccb RELAT\u00d3RIO EXECUTIVO - {self.model_name}\")\n        print(\"=\" * 60)\n\n        # Performance geral\n        basic = self.evaluation_results['basic_metrics']\n        print(f\"\ud83c\udfaf PERFORMANCE GERAL\")\n        print(f\"  ROC-AUC: {basic['roc_auc']:.3f}\")\n        print(f\"  Precision: {basic['precision']:.3f}\")\n        print(f\"  Recall: {basic['recall']:.3f}\")\n        print(f\"  F1-Score: {basic['f1_score']:.3f}\")\n\n        # Impacto de neg\u00f3cio\n        business = self.evaluation_results['business_metrics']\n        print(f\"\\n\ud83d\udcb0 IMPACTO DE NEG\u00d3CIO\")\n        print(f\"  Valor econ\u00f4mico: R$ {business['economic_value']:,.0f}\")\n        print(f\"  Atrasos prevenidos: {business['delays_caught']}\")\n        print(f\"  Atrasos perdidos: {business['delays_missed']}\")\n        print(f\"  Falsos alertas: {business['false_alerts']}\")\n\n        # Viabilidade operacional\n        operational = self.evaluation_results['operational']\n        print(f\"\\n\ud83d\udcca VIABILIDADE OPERACIONAL\")\n        print(f\"  Alertas por dia: {operational['alerts_per_day']:.1f}\")\n        print(f\"  Taxa de alertas: {operational['alert_rate']:.1%}\")\n        print(f\"  Operacionalmente vi\u00e1vel: {'\u2705' if operational['operationally_viable'] else '\u274c'}\")\n\n        # Estabilidade\n        stability = self.evaluation_results['stability']\n        print(f\"\\n\ud83d\udee1\ufe0f ROBUSTEZ\")\n        print(f\"  Estabilidade: {stability['prediction_stability']:.1%}\")\n        print(f\"  Taxa de mudan\u00e7a: {stability['flip_rate']:.1%}\")\n\n        # Recomenda\u00e7\u00e3o final\n        print(f\"\\n\ud83c\udfaf RECOMENDA\u00c7\u00c3O\")\n\n        # Crit\u00e9rios para aprova\u00e7\u00e3o\n        approval_criteria = {\n            'roc_auc': basic['roc_auc'] &gt;= 0.75,\n            'precision': basic['precision'] &gt;= 0.70,\n            'recall': basic['recall'] &gt;= 0.60,\n            'economic_value': business['economic_value'] &gt; 0,\n            'operational_viable': operational['operationally_viable'],\n            'stable': stability['prediction_stability'] &gt;= 0.90\n        }\n\n        approval_score = sum(approval_criteria.values())\n        total_criteria = len(approval_criteria)\n\n        if approval_score &gt;= 5:\n            recommendation = \"\u2705 APROVADO para produ\u00e7\u00e3o\"\n        elif approval_score &gt;= 3:\n            recommendation = \"\u26a0\ufe0f APROVADO COM RESSALVAS\"\n        else:\n            recommendation = \"\u274c N\u00c3O APROVADO - necessita melhorias\"\n\n        print(f\"  {recommendation}\")\n        print(f\"  Crit\u00e9rios atendidos: {approval_score}/{total_criteria}\")\n\n        return approval_criteria, recommendation\n</code></pre>"},{"location":"ml/evaluation/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\u26a1 API - Implementa\u00e7\u00e3o da API de predi\u00e7\u00e3o</li> <li>\ud83d\udc33 Docker - Containeriza\u00e7\u00e3o do modelo</li> <li>\ud83e\uddea Testes - Testes automatizados</li> </ol>"},{"location":"ml/evaluation/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83e\udd16 Treinamento - Desenvolvimento dos modelos</li> <li>\ud83c\udfd7\ufe0f Pipeline ML - Arquitetura completa</li> <li>\ud83d\udcca An\u00e1lise de Dados - Insights dos dados</li> </ul>"},{"location":"ml/model-training/","title":"\ud83e\udd16 Treinamento de Modelos","text":"<p>Guia completo para treinamento de modelos de machine learning para predi\u00e7\u00e3o de atrasos de voos, incluindo sele\u00e7\u00e3o de algoritmos, otimiza\u00e7\u00e3o de hiperpar\u00e2metros e valida\u00e7\u00e3o cruzada.</p>"},{"location":"ml/model-training/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Esta se\u00e7\u00e3o documenta todo o processo de treinamento dos modelos preditivos, desde a sele\u00e7\u00e3o dos algoritmos at\u00e9 a otimiza\u00e7\u00e3o final, seguindo as melhores pr\u00e1ticas de machine learning.</p>"},{"location":"ml/model-training/#estrategia-de-modelagem","title":"\ud83e\udde0 Estrat\u00e9gia de Modelagem","text":"<pre><code>graph TD\n    A[Dados Preprocessados] --&gt; B[Baseline Models]\n    B --&gt; C[Model Selection]\n    C --&gt; D[Hyperparameter Tuning]\n    D --&gt; E[Cross Validation]\n    E --&gt; F[Final Training]\n    F --&gt; G[Model Ensemble]\n    G --&gt; H[Model Evaluation]\n\n    style A fill:#e3f2fd\n    style H fill:#c8e6c9\n    style G fill:#fff3e0</code></pre>"},{"location":"ml/model-training/#modelos-candidatos","title":"\ud83c\udfaf Modelos Candidatos","text":""},{"location":"ml/model-training/#1-algoritmos-selecionados","title":"1. \ud83d\udcca Algoritmos Selecionados","text":"Modelo Tipo Vantagens Desvantagens Uso Logistic Regression Linear Interpret\u00e1vel, r\u00e1pido Assume linearidade Baseline Random Forest Ensemble Robusto, lida com n\u00e3o-linearidade Pode overfittar Modelo principal XGBoost Gradient Boosting Alta performance, feature importance Complexo de tunar Alta performance LightGBM Gradient Boosting R\u00e1pido, eficiente em mem\u00f3ria Sens\u00edvel a overfitting Alternativa ao XGBoost Neural Network Deep Learning Captura padr\u00f5es complexos Black box, precisa muitos dados Ensemble"},{"location":"ml/model-training/#2-configuracao-dos-modelos","title":"2. \ud83c\udfaf Configura\u00e7\u00e3o dos Modelos","text":"<pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neural_network import MLPClassifier\n\ndef get_base_models():\n    \"\"\"Retorna modelos base com configura\u00e7\u00f5es iniciais\"\"\"\n\n    models = {\n        'logistic_regression': LogisticRegression(\n            random_state=42,\n            max_iter=1000,\n            class_weight='balanced'\n        ),\n\n        'random_forest': RandomForestClassifier(\n            n_estimators=100,\n            random_state=42,\n            class_weight='balanced',\n            n_jobs=-1\n        ),\n\n        'xgboost': XGBClassifier(\n            random_state=42,\n            eval_metric='logloss',\n            use_label_encoder=False\n        ),\n\n        'lightgbm': LGBMClassifier(\n            random_state=42,\n            class_weight='balanced',\n            verbose=-1\n        ),\n\n        'neural_network': MLPClassifier(\n            random_state=42,\n            max_iter=500,\n            early_stopping=True,\n            validation_fraction=0.1\n        )\n    }\n\n    return models\n</code></pre>"},{"location":"ml/model-training/#baseline-models","title":"\ud83c\udfc3\u200d\u2642\ufe0f Baseline Models","text":""},{"location":"ml/model-training/#1-modelos-simples","title":"1. \ud83d\udccf Modelos Simples","text":"<pre><code>def train_baseline_models(X_train, X_val, y_train, y_val):\n    \"\"\"Treina modelos baseline para estabelecer linha de base\"\"\"\n\n    from sklearn.dummy import DummyClassifier\n    from sklearn.metrics import classification_report, roc_auc_score\n\n    baselines = {}\n\n    # 1. Dummy Classifier - Estratified\n    dummy_stratified = DummyClassifier(strategy='stratified', random_state=42)\n    dummy_stratified.fit(X_train, y_train)\n\n    y_pred_dummy = dummy_stratified.predict(X_val)\n    y_pred_dummy_proba = dummy_stratified.predict_proba(X_val)[:, 1]\n\n    baselines['dummy_stratified'] = {\n        'model': dummy_stratified,\n        'auc': roc_auc_score(y_val, y_pred_dummy_proba),\n        'report': classification_report(y_val, y_pred_dummy, output_dict=True)\n    }\n\n    # 2. Dummy Classifier - Most Frequent  \n    dummy_frequent = DummyClassifier(strategy='most_frequent', random_state=42)\n    dummy_frequent.fit(X_train, y_train)\n\n    y_pred_freq = dummy_frequent.predict(X_val)\n\n    baselines['dummy_frequent'] = {\n        'model': dummy_frequent,\n        'accuracy': (y_pred_freq == y_val).mean(),\n        'report': classification_report(y_val, y_pred_freq, output_dict=True)\n    }\n\n    # 3. Logistic Regression simples\n    lr_simple = LogisticRegression(random_state=42)\n    lr_simple.fit(X_train, y_train)\n\n    y_pred_lr = lr_simple.predict_proba(X_val)[:, 1]\n\n    baselines['logistic_simple'] = {\n        'model': lr_simple,\n        'auc': roc_auc_score(y_val, y_pred_lr),\n        'report': classification_report(y_val, (y_pred_lr &gt; 0.5).astype(int), output_dict=True)\n    }\n\n    # Resumo dos baselines\n    print(\"\ud83c\udfaf BASELINE RESULTS\")\n    print(\"=\" * 50)\n    for name, results in baselines.items():\n        auc = results.get('auc', 0)\n        acc = results.get('accuracy', results['report']['accuracy'])\n        print(f\"{name:20s} - AUC: {auc:.4f}, Accuracy: {acc:.4f}\")\n\n    return baselines\n</code></pre>"},{"location":"ml/model-training/#otimizacao-de-hiperparametros","title":"\ud83c\udf9b\ufe0f Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros","text":""},{"location":"ml/model-training/#1-grid-search-estrategico","title":"1. \ud83d\udd0d Grid Search Estrat\u00e9gico","text":"<pre><code>def get_hyperparameter_grids():\n    \"\"\"Define grades de hiperpar\u00e2metros para otimiza\u00e7\u00e3o\"\"\"\n\n    param_grids = {\n        'random_forest': {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [10, 20, None],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4],\n            'max_features': ['sqrt', 'log2', None]\n        },\n\n        'xgboost': {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [3, 6, 10],\n            'learning_rate': [0.01, 0.1, 0.2],\n            'subsample': [0.8, 0.9, 1.0],\n            'colsample_bytree': [0.8, 0.9, 1.0]\n        },\n\n        'lightgbm': {\n            'n_estimators': [100, 200, 300],\n            'max_depth': [3, 6, 10, -1],\n            'learning_rate': [0.01, 0.1, 0.2],\n            'num_leaves': [31, 50, 100],\n            'feature_fraction': [0.8, 0.9, 1.0]\n        },\n\n        'logistic_regression': {\n            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n            'penalty': ['l1', 'l2', 'elasticnet'],\n            'solver': ['liblinear', 'saga']\n        },\n\n        'neural_network': {\n            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n            'activation': ['relu', 'tanh'],\n            'alpha': [0.0001, 0.001, 0.01],\n            'learning_rate_init': [0.001, 0.01]\n        }\n    }\n\n    return param_grids\n</code></pre>"},{"location":"ml/model-training/#2-busca-bayesiana-optuna","title":"2. \ud83c\udfaf Busca Bayesiana (Optuna)","text":"<pre><code>import optuna\nfrom optuna.integration import OptunaSearchCV\n\ndef bayesian_optimization(model_name, model, X_train, y_train, n_trials=100):\n    \"\"\"Otimiza\u00e7\u00e3o bayesiana usando Optuna\"\"\"\n\n    def objective(trial):\n\n        if model_name == 'xgboost':\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n                'max_depth': trial.suggest_int('max_depth', 3, 15),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1, 10)\n            }\n\n        elif model_name == 'lightgbm':\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n                'max_depth': trial.suggest_int('max_depth', 3, 15),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n                'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1, 10)\n            }\n\n        elif model_name == 'random_forest':\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n                'max_depth': trial.suggest_int('max_depth', 5, 30),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n            }\n\n        # Configurar modelo com par\u00e2metros\n        model_with_params = model.__class__(**params, random_state=42)\n\n        # Cross-validation\n        from sklearn.model_selection import cross_val_score\n        cv_scores = cross_val_score(\n            model_with_params, X_train, y_train, \n            cv=5, scoring='roc_auc', n_jobs=-1\n        )\n\n        return cv_scores.mean()\n\n    # Executar otimiza\u00e7\u00e3o\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=n_trials)\n\n    print(f\"\ud83c\udfaf Melhores par\u00e2metros para {model_name}:\")\n    print(study.best_params)\n    print(f\"Melhor AUC: {study.best_value:.4f}\")\n\n    return study.best_params, study.best_value\n</code></pre>"},{"location":"ml/model-training/#validacao-cruzada","title":"\ud83d\udd04 Valida\u00e7\u00e3o Cruzada","text":""},{"location":"ml/model-training/#1-time-series-split","title":"1. \u23f0 Time Series Split","text":"<pre><code>from sklearn.model_selection import TimeSeriesSplit\n\ndef time_series_cross_validation(models, X_train, y_train, n_splits=5):\n    \"\"\"Valida\u00e7\u00e3o cruzada respeitando ordem temporal\"\"\"\n\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = {}\n\n    for model_name, model in models.items():\n        print(f\"\\n\ud83d\udd04 Validando {model_name}...\")\n\n        cv_scores = {\n            'auc': [],\n            'precision': [],\n            'recall': [],\n            'f1': []\n        }\n\n        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n            print(f\"  Fold {fold + 1}/{n_splits}\")\n\n            # Dividir dados\n            X_fold_train = X_train.iloc[train_idx]\n            X_fold_val = X_train.iloc[val_idx]\n            y_fold_train = y_train.iloc[train_idx]\n            y_fold_val = y_train.iloc[val_idx]\n\n            # Treinar modelo\n            model_copy = clone(model)\n            model_copy.fit(X_fold_train, y_fold_train)\n\n            # Predi\u00e7\u00f5es\n            y_pred_proba = model_copy.predict_proba(X_fold_val)[:, 1]\n            y_pred = (y_pred_proba &gt; 0.5).astype(int)\n\n            # M\u00e9tricas\n            from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n\n            cv_scores['auc'].append(roc_auc_score(y_fold_val, y_pred_proba))\n            cv_scores['precision'].append(precision_score(y_fold_val, y_pred))\n            cv_scores['recall'].append(recall_score(y_fold_val, y_pred))\n            cv_scores['f1'].append(f1_score(y_fold_val, y_pred))\n\n        # Resumir resultados\n        results[model_name] = {\n            'auc_mean': np.mean(cv_scores['auc']),\n            'auc_std': np.std(cv_scores['auc']),\n            'precision_mean': np.mean(cv_scores['precision']),\n            'precision_std': np.std(cv_scores['precision']),\n            'recall_mean': np.mean(cv_scores['recall']),\n            'recall_std': np.std(cv_scores['recall']),\n            'f1_mean': np.mean(cv_scores['f1']),\n            'f1_std': np.std(cv_scores['f1'])\n        }\n\n        print(f\"  AUC: {results[model_name]['auc_mean']:.4f} \u00b1 {results[model_name]['auc_std']:.4f}\")\n\n    return results\n</code></pre>"},{"location":"ml/model-training/#2-stratified-k-fold","title":"2. \ud83d\udcca Stratified K-Fold","text":"<pre><code>from sklearn.model_selection import StratifiedKFold\n\ndef stratified_cross_validation(models, X_train, y_train, n_splits=5):\n    \"\"\"Valida\u00e7\u00e3o cruzada estratificada\"\"\"\n\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    results = {}\n\n    for model_name, model in models.items():\n        print(f\"\\n\ud83d\udd04 Valida\u00e7\u00e3o estratificada - {model_name}\")\n\n        cv_scores = cross_val_score(\n            model, X_train, y_train,\n            cv=skf, scoring='roc_auc', n_jobs=-1\n        )\n\n        results[model_name] = {\n            'scores': cv_scores,\n            'mean': cv_scores.mean(),\n            'std': cv_scores.std(),\n            'min': cv_scores.min(),\n            'max': cv_scores.max()\n        }\n\n        print(f\"  AUC: {results[model_name]['mean']:.4f} \u00b1 {results[model_name]['std']:.4f}\")\n        print(f\"  Range: [{results[model_name]['min']:.4f}, {results[model_name]['max']:.4f}]\")\n\n    return results\n</code></pre>"},{"location":"ml/model-training/#selecao-de-features","title":"\ud83c\udfaf Sele\u00e7\u00e3o de Features","text":""},{"location":"ml/model-training/#1-importancia-das-features","title":"1. \ud83d\udcca Import\u00e2ncia das Features","text":"<pre><code>def feature_importance_analysis(models_trained, X_train, feature_names):\n    \"\"\"An\u00e1lise de import\u00e2ncia das features\"\"\"\n\n    feature_importance_results = {}\n\n    for model_name, model in models_trained.items():\n\n        if hasattr(model, 'feature_importances_'):\n            # Tree-based models\n            importances = model.feature_importances_\n\n        elif hasattr(model, 'coef_'):\n            # Linear models\n            importances = np.abs(model.coef_[0])\n\n        else:\n            print(f\"\u26a0\ufe0f {model_name} n\u00e3o suporta feature importance\")\n            continue\n\n        # Criar DataFrame de import\u00e2ncias\n        importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': importances\n        }).sort_values('importance', ascending=False)\n\n        feature_importance_results[model_name] = importance_df\n\n        # Top 10 features mais importantes\n        print(f\"\\n\ud83c\udfaf Top 10 features - {model_name}:\")\n        print(importance_df.head(10).to_string(index=False))\n\n    return feature_importance_results\n</code></pre>"},{"location":"ml/model-training/#2-selecao-automatica","title":"2. \ud83d\udd0d Sele\u00e7\u00e3o Autom\u00e1tica","text":"<pre><code>from sklearn.feature_selection import SelectFromModel, RFE\n\ndef automated_feature_selection(model, X_train, y_train, method='importance'):\n    \"\"\"Sele\u00e7\u00e3o autom\u00e1tica de features\"\"\"\n\n    if method == 'importance':\n        # Sele\u00e7\u00e3o baseada em import\u00e2ncia\n        selector = SelectFromModel(model, threshold='mean')\n\n    elif method == 'rfe':\n        # Recursive Feature Elimination\n        selector = RFE(model, n_features_to_select=50)\n\n    # Ajustar seletor\n    X_selected = selector.fit_transform(X_train, y_train)\n\n    # Features selecionadas\n    selected_features = X_train.columns[selector.get_support()]\n\n    print(f\"\ud83c\udfaf {method.upper()}: {len(selected_features)} features selecionadas de {X_train.shape[1]}\")\n    print(f\"Features selecionadas: {list(selected_features)}\")\n\n    return selector, selected_features, X_selected\n</code></pre>"},{"location":"ml/model-training/#treinamento-final","title":"\ud83c\udfc6 Treinamento Final","text":""},{"location":"ml/model-training/#1-modelo-otimizado","title":"1. \ud83c\udfaf Modelo Otimizado","text":"<pre><code>class OptimizedModelTrainer:\n    \"\"\"Classe para treinamento de modelos otimizados\"\"\"\n\n    def __init__(self, model_configs=None):\n        self.model_configs = model_configs or {}\n        self.trained_models = {}\n        self.feature_selectors = {}\n\n    def train_optimized_models(self, X_train, y_train, X_val, y_val):\n        \"\"\"Treina modelos com melhores hiperpar\u00e2metros\"\"\"\n\n        # Configura\u00e7\u00f5es otimizadas (exemplo)\n        optimized_configs = {\n            'xgboost': {\n                'n_estimators': 300,\n                'max_depth': 8,\n                'learning_rate': 0.1,\n                'subsample': 0.9,\n                'colsample_bytree': 0.9,\n                'reg_alpha': 1,\n                'reg_lambda': 2,\n                'random_state': 42\n            },\n\n            'lightgbm': {\n                'n_estimators': 250,\n                'max_depth': 10,\n                'learning_rate': 0.15,\n                'num_leaves': 100,\n                'feature_fraction': 0.9,\n                'bagging_fraction': 0.8,\n                'random_state': 42,\n                'verbose': -1\n            },\n\n            'random_forest': {\n                'n_estimators': 200,\n                'max_depth': 15,\n                'min_samples_split': 5,\n                'min_samples_leaf': 2,\n                'max_features': 'sqrt',\n                'random_state': 42,\n                'n_jobs': -1\n            }\n        }\n\n        for model_name, config in optimized_configs.items():\n            print(f\"\\n\ud83d\ude80 Treinando {model_name} otimizado...\")\n\n            # Instanciar modelo\n            if model_name == 'xgboost':\n                model = XGBClassifier(**config)\n            elif model_name == 'lightgbm':\n                model = LGBMClassifier(**config)\n            elif model_name == 'random_forest':\n                model = RandomForestClassifier(**config)\n\n            # Treinar\n            model.fit(X_train, y_train)\n\n            # Avaliar\n            y_pred_proba = model.predict_proba(X_val)[:, 1]\n            auc_score = roc_auc_score(y_val, y_pred_proba)\n\n            self.trained_models[model_name] = {\n                'model': model,\n                'config': config,\n                'auc_score': auc_score\n            }\n\n            print(f\"\u2705 {model_name}: AUC = {auc_score:.4f}\")\n\n        return self.trained_models\n\n    def save_models(self, filepath_prefix='model/'):\n        \"\"\"Salvar modelos treinados\"\"\"\n        import joblib\n\n        for model_name, model_data in self.trained_models.items():\n            filepath = f\"{filepath_prefix}{model_name}_optimized.pkl\"\n            joblib.dump(model_data, filepath)\n            print(f\"\ud83d\udcbe {model_name} salvo em: {filepath}\")\n</code></pre>"},{"location":"ml/model-training/#ensemble-methods","title":"\ud83c\udfad Ensemble Methods","text":""},{"location":"ml/model-training/#1-voting-classifier","title":"1. \ud83d\uddf3\ufe0f Voting Classifier","text":"<pre><code>from sklearn.ensemble import VotingClassifier\n\ndef create_ensemble_model(trained_models, method='soft'):\n    \"\"\"Cria modelo ensemble com voting\"\"\"\n\n    # Preparar lista de modelos para ensemble\n    estimators = []\n\n    for model_name, model_data in trained_models.items():\n        model = model_data['model']\n        estimators.append((model_name, model))\n\n    # Criar ensemble\n    ensemble = VotingClassifier(\n        estimators=estimators,\n        voting=method  # 'soft' para probabilidades, 'hard' para votos\n    )\n\n    return ensemble\n</code></pre>"},{"location":"ml/model-training/#2-stacking-classifier","title":"2. \ud83c\udfaf Stacking Classifier","text":"<pre><code>from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\ndef create_stacking_ensemble(trained_models, meta_learner=None):\n    \"\"\"Cria ensemble com stacking\"\"\"\n\n    # Meta-learner padr\u00e3o\n    if meta_learner is None:\n        meta_learner = LogisticRegression(random_state=42)\n\n    # Preparar estimators\n    estimators = []\n    for model_name, model_data in trained_models.items():\n        model = model_data['model']\n        estimators.append((model_name, model))\n\n    # Criar stacking ensemble\n    stacking_ensemble = StackingClassifier(\n        estimators=estimators,\n        final_estimator=meta_learner,\n        cv=5,  # Cross-validation para meta-features\n        n_jobs=-1\n    )\n\n    return stacking_ensemble\n</code></pre>"},{"location":"ml/model-training/#3-blending-manual","title":"3. \ud83c\udfaa Blending Manual","text":"<pre><code>def manual_blending(models, X_val, y_val, weights=None):\n    \"\"\"Blending manual com pesos customizados\"\"\"\n\n    if weights is None:\n        # Pesos baseados na performance individual\n        weights = {}\n        for name, model_data in models.items():\n            weights[name] = model_data['auc_score']\n\n        # Normalizar pesos\n        total_weight = sum(weights.values())\n        weights = {k: v/total_weight for k, v in weights.items()}\n\n    # Coletar predi\u00e7\u00f5es\n    predictions = {}\n    for name, model_data in models.items():\n        model = model_data['model']\n        pred_proba = model.predict_proba(X_val)[:, 1]\n        predictions[name] = pred_proba\n\n    # Combinar predi\u00e7\u00f5es com pesos\n    blended_predictions = np.zeros(len(y_val))\n\n    for name, pred in predictions.items():\n        weight = weights[name]\n        blended_predictions += weight * pred\n        print(f\"\ud83c\udfaf {name}: peso = {weight:.3f}\")\n\n    # Avaliar ensemble\n    ensemble_auc = roc_auc_score(y_val, blended_predictions)\n    print(f\"\\n\ud83c\udfc6 Ensemble AUC: {ensemble_auc:.4f}\")\n\n    return blended_predictions, weights, ensemble_auc\n</code></pre>"},{"location":"ml/model-training/#metricas-de-treinamento","title":"\ud83d\udcca M\u00e9tricas de Treinamento","text":""},{"location":"ml/model-training/#1-curvas-de-aprendizado","title":"1. \ud83d\udcc8 Curvas de Aprendizado","text":"<pre><code>from sklearn.model_selection import learning_curve\n\ndef plot_learning_curves(model, X_train, y_train):\n    \"\"\"Plota curvas de aprendizado\"\"\"\n\n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X_train, y_train,\n        train_sizes=np.linspace(0.1, 1.0, 10),\n        cv=5, scoring='roc_auc', n_jobs=-1\n    )\n\n    # Calcular m\u00e9dias e desvios\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    # Plot\n    plt.figure(figsize=(10, 6))\n\n    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Treino')\n    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n\n    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Valida\u00e7\u00e3o')\n    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n\n    plt.xlabel('Tamanho do Conjunto de Treino')\n    plt.ylabel('AUC Score')\n    plt.title('Curvas de Aprendizado')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return train_sizes, train_scores, val_scores\n</code></pre>"},{"location":"ml/model-training/#2-validation-curves","title":"2. \ud83d\udcca Validation Curves","text":"<pre><code>from sklearn.model_selection import validation_curve\n\ndef plot_validation_curve(model, X_train, y_train, param_name, param_range):\n    \"\"\"Plota curva de valida\u00e7\u00e3o para um hiperpar\u00e2metro\"\"\"\n\n    train_scores, val_scores = validation_curve(\n        model, X_train, y_train,\n        param_name=param_name,\n        param_range=param_range,\n        cv=5, scoring='roc_auc', n_jobs=-1\n    )\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    val_mean = np.mean(val_scores, axis=1)\n    val_std = np.std(val_scores, axis=1)\n\n    plt.figure(figsize=(10, 6))\n\n    plt.semilogx(param_range, train_mean, 'o-', color='blue', label='Treino')\n    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n\n    plt.semilogx(param_range, val_mean, 'o-', color='red', label='Valida\u00e7\u00e3o')\n    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n\n    plt.xlabel(param_name)\n    plt.ylabel('AUC Score')\n    plt.title(f'Curva de Valida\u00e7\u00e3o - {param_name}')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n    return train_scores, val_scores\n</code></pre>"},{"location":"ml/model-training/#persistencia-de-modelos","title":"\ud83d\udcbe Persist\u00eancia de Modelos","text":""},{"location":"ml/model-training/#1-salvamento-completo","title":"1. \ud83d\udce6 Salvamento Completo","text":"<pre><code>import joblib\nimport json\nfrom datetime import datetime\n\nclass ModelPersistence:\n    \"\"\"Classe para salvar e carregar modelos treinados\"\"\"\n\n    @staticmethod\n    def save_complete_model(model, model_name, metadata=None, filepath_prefix='model/'):\n        \"\"\"Salva modelo completo com metadados\"\"\"\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        # Estrutura de dados para salvar\n        model_package = {\n            'model': model,\n            'model_name': model_name,\n            'timestamp': timestamp,\n            'metadata': metadata or {}\n        }\n\n        # Caminhos de arquivo\n        model_filepath = f\"{filepath_prefix}{model_name}_{timestamp}.pkl\"\n        metadata_filepath = f\"{filepath_prefix}{model_name}_{timestamp}_metadata.json\"\n\n        # Salvar modelo\n        joblib.dump(model_package, model_filepath)\n\n        # Salvar metadados separadamente\n        metadata_to_save = {\n            'model_name': model_name,\n            'timestamp': timestamp,\n            'model_filepath': model_filepath,\n            **model_package['metadata']\n        }\n\n        with open(metadata_filepath, 'w') as f:\n            json.dump(metadata_to_save, f, indent=2, default=str)\n\n        print(f\"\ud83d\udcbe Modelo salvo: {model_filepath}\")\n        print(f\"\ud83d\udcc4 Metadados salvos: {metadata_filepath}\")\n\n        return model_filepath, metadata_filepath\n\n    @staticmethod\n    def load_model(filepath):\n        \"\"\"Carrega modelo salvo\"\"\"\n\n        model_package = joblib.load(filepath)\n\n        print(f\"\ud83d\udcc2 Modelo carregado: {model_package['model_name']}\")\n        print(f\"\ud83d\udd52 Timestamp: {model_package['timestamp']}\")\n\n        return model_package['model'], model_package['metadata']\n\n    @staticmethod\n    def save_ensemble(ensemble_models, ensemble_weights, filepath_prefix='model/'):\n        \"\"\"Salva ensemble de modelos\"\"\"\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n        ensemble_package = {\n            'models': ensemble_models,\n            'weights': ensemble_weights,\n            'timestamp': timestamp,\n            'model_type': 'ensemble'\n        }\n\n        filepath = f\"{filepath_prefix}ensemble_{timestamp}.pkl\"\n        joblib.dump(ensemble_package, filepath)\n\n        print(f\"\ud83d\udcbe Ensemble salvo: {filepath}\")\n\n        return filepath\n</code></pre>"},{"location":"ml/model-training/#pipeline-de-treinamento-completo","title":"\ud83d\udccb Pipeline de Treinamento Completo","text":""},{"location":"ml/model-training/#classe-principal","title":"\ud83d\ude80 Classe Principal","text":"<pre><code>class FlightDelayModelTrainer:\n    \"\"\"Pipeline completo de treinamento de modelos\"\"\"\n\n    def __init__(self, config=None):\n        self.config = config or {}\n        self.models = {}\n        self.best_model = None\n        self.ensemble = None\n\n    def full_training_pipeline(self, X_train, X_val, y_train, y_val):\n        \"\"\"Pipeline completo de treinamento\"\"\"\n\n        print(\"\ud83d\ude80 Iniciando pipeline de treinamento completo...\")\n\n        # 1. Baselines\n        print(\"\\n\ud83d\udccf FASE 1: Modelos Baseline\")\n        baselines = train_baseline_models(X_train, X_val, y_train, y_val)\n\n        # 2. Modelos base\n        print(\"\\n\ud83e\udde0 FASE 2: Modelos Base\")\n        base_models = get_base_models()\n\n        # Treinar modelos base\n        for name, model in base_models.items():\n            print(f\"Treinando {name}...\")\n            model.fit(X_train, y_train)\n\n            y_pred_proba = model.predict_proba(X_val)[:, 1]\n            auc = roc_auc_score(y_val, y_pred_proba)\n\n            self.models[name] = {\n                'model': model,\n                'auc': auc,\n                'type': 'base'\n            }\n\n            print(f\"  {name}: AUC = {auc:.4f}\")\n\n        # 3. Otimiza\u00e7\u00e3o de hiperpar\u00e2metros\n        print(\"\\n\ud83c\udf9b\ufe0f FASE 3: Otimiza\u00e7\u00e3o de Hiperpar\u00e2metros\")\n\n        # Selecionar top 3 modelos para otimiza\u00e7\u00e3o\n        top_models = sorted(self.models.items(), key=lambda x: x[1]['auc'], reverse=True)[:3]\n\n        for model_name, model_data in top_models:\n            print(f\"\\nOtimizando {model_name}...\")\n\n            best_params, best_score = bayesian_optimization(\n                model_name, model_data['model'], X_train, y_train, n_trials=50\n            )\n\n            # Treinar com melhores par\u00e2metros\n            if model_name == 'xgboost':\n                optimized_model = XGBClassifier(**best_params, random_state=42)\n            elif model_name == 'lightgbm':\n                optimized_model = LGBMClassifier(**best_params, random_state=42)\n            elif model_name == 'random_forest':\n                optimized_model = RandomForestClassifier(**best_params, random_state=42)\n\n            optimized_model.fit(X_train, y_train)\n            y_pred_proba = optimized_model.predict_proba(X_val)[:, 1]\n            optimized_auc = roc_auc_score(y_val, y_pred_proba)\n\n            self.models[f\"{model_name}_optimized\"] = {\n                'model': optimized_model,\n                'auc': optimized_auc,\n                'params': best_params,\n                'type': 'optimized'\n            }\n\n            print(f\"  {model_name} otimizado: AUC = {optimized_auc:.4f}\")\n\n        # 4. Sele\u00e7\u00e3o do melhor modelo\n        print(\"\\n\ud83c\udfc6 FASE 4: Sele\u00e7\u00e3o do Melhor Modelo\")\n\n        best_model_name = max(self.models.keys(), key=lambda k: self.models[k]['auc'])\n        self.best_model = self.models[best_model_name]\n\n        print(f\"\ud83e\udd47 Melhor modelo: {best_model_name}\")\n        print(f\"\ud83c\udfaf AUC: {self.best_model['auc']:.4f}\")\n\n        # 5. Ensemble\n        print(\"\\n\ud83c\udfaa FASE 5: Cria\u00e7\u00e3o de Ensemble\")\n\n        # Selecionar top 3 modelos otimizados para ensemble\n        optimized_models = {k: v for k, v in self.models.items() if v['type'] == 'optimized'}\n\n        if len(optimized_models) &gt;= 2:\n            ensemble_predictions, ensemble_weights, ensemble_auc = manual_blending(\n                optimized_models, X_val, y_val\n            )\n\n            self.ensemble = {\n                'models': optimized_models,\n                'weights': ensemble_weights,\n                'auc': ensemble_auc,\n                'predictions': ensemble_predictions\n            }\n\n            print(f\"\ud83c\udfad Ensemble AUC: {ensemble_auc:.4f}\")\n\n        # 6. Salvar modelos\n        print(\"\\n\ud83d\udcbe FASE 6: Salvando Modelos\")\n\n        # Salvar melhor modelo individual\n        model_filepath, metadata_filepath = ModelPersistence.save_complete_model(\n            self.best_model['model'],\n            best_model_name,\n            metadata={\n                'auc_score': self.best_model['auc'],\n                'model_type': 'individual',\n                'training_samples': len(X_train),\n                'features': X_train.shape[1]\n            }\n        )\n\n        # Salvar ensemble se dispon\u00edvel\n        if self.ensemble:\n            ensemble_filepath = ModelPersistence.save_ensemble(\n                self.ensemble['models'],\n                self.ensemble['weights']\n            )\n\n        print(\"\u2705 Pipeline de treinamento conclu\u00eddo!\")\n\n        return {\n            'best_model': self.best_model,\n            'ensemble': self.ensemble,\n            'all_models': self.models\n        }\n</code></pre>"},{"location":"ml/model-training/#resumo-de-resultados","title":"\ud83d\udcca Resumo de Resultados","text":""},{"location":"ml/model-training/#comparacao-final","title":"\ud83c\udfc6 Compara\u00e7\u00e3o Final","text":"<pre><code>def create_results_summary(trained_models):\n    \"\"\"Cria resumo dos resultados de treinamento\"\"\"\n\n    results_df = pd.DataFrame([\n        {\n            'Model': name,\n            'AUC': data['auc'],\n            'Type': data['type'],\n            'Parameters': len(data.get('params', {})) if 'params' in data else 0\n        }\n        for name, data in trained_models.items()\n    ]).sort_values('AUC', ascending=False)\n\n    print(\"\\n\ud83d\udcca RESUMO FINAL DOS RESULTADOS\")\n    print(\"=\" * 60)\n    print(results_df.to_string(index=False))\n\n    # Melhoria percentual\n    baseline_auc = min(results_df['AUC'])\n    best_auc = max(results_df['AUC'])\n    improvement = ((best_auc - baseline_auc) / baseline_auc) * 100\n\n    print(f\"\\n\ud83d\ude80 Melhoria sobre baseline: {improvement:.2f}%\")\n\n    return results_df\n</code></pre>"},{"location":"ml/model-training/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udcc8 Avalia\u00e7\u00e3o - M\u00e9tricas detalhadas e valida\u00e7\u00e3o dos modelos</li> <li>\u26a1 API - Implementa\u00e7\u00e3o da API de predi\u00e7\u00e3o</li> <li>\ud83d\udc33 Docker - Containeriza\u00e7\u00e3o dos modelos</li> </ol>"},{"location":"ml/model-training/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83d\udcca Pr\u00e9-processamento - Prepara\u00e7\u00e3o dos dados</li> <li>\ud83c\udfd7\ufe0f Pipeline ML - Arquitetura completa</li> <li>\ud83e\uddea Testes - Testes dos modelos</li> </ul>"},{"location":"ml/preprocessing/","title":"\ud83e\uddf9 Pr\u00e9-processamento de Dados","text":"<p>Guia completo de pr\u00e9-processamento e prepara\u00e7\u00e3o dos dados para o modelo de predi\u00e7\u00e3o de atrasos de voos, incluindo limpeza, transforma\u00e7\u00f5es e feature engineering.</p>"},{"location":"ml/preprocessing/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>O pr\u00e9-processamento \u00e9 uma etapa cr\u00edtica que transforma dados brutos em features prontas para treinamento do modelo. Esta se\u00e7\u00e3o documenta todas as transforma\u00e7\u00f5es aplicadas aos dados de voos.</p>"},{"location":"ml/preprocessing/#pipeline-de-pre-processamento","title":"\ud83d\udd04 Pipeline de Pr\u00e9-processamento","text":"<pre><code>graph TD\n    A[Dados Brutos] --&gt; B[Valida\u00e7\u00e3o Inicial]\n    B --&gt; C[Limpeza de Dados]\n    C --&gt; D[Feature Engineering]\n    D --&gt; E[Encoding Categ\u00f3rico]\n    E --&gt; F[Normaliza\u00e7\u00e3o]\n    F --&gt; G[Divis\u00e3o Train/Test]\n    G --&gt; H[Dados Prontos]\n\n    style A fill:#ff6b6b\n    style H fill:#51cf66\n    style D fill:#339af0</code></pre>"},{"location":"ml/preprocessing/#limpeza-de-dados","title":"\ud83e\uddf9 Limpeza de Dados","text":""},{"location":"ml/preprocessing/#1-validacao-inicial","title":"1. \ud83d\udcca Valida\u00e7\u00e3o Inicial","text":"<pre><code>def validate_raw_data(df):\n    \"\"\"Valida\u00e7\u00e3o inicial dos dados brutos\"\"\"\n\n    validation_checks = {\n        'missing_values': df.isnull().sum(),\n        'duplicate_rows': df.duplicated().sum(),\n        'data_types': df.dtypes,\n        'date_range': {\n            'min_date': df['departure_time'].min(),\n            'max_date': df['departure_time'].max()\n        },\n        'negative_delays': (df['delay_minutes'] &lt; -30).sum(),\n        'extreme_delays': (df['delay_minutes'] &gt; 300).sum()\n    }\n\n    return validation_checks\n</code></pre>"},{"location":"ml/preprocessing/#2-tratamento-de-valores-ausentes","title":"2. \u26a0\ufe0f Tratamento de Valores Ausentes","text":""},{"location":"ml/preprocessing/#estrategias-por-tipo-de-variavel","title":"Estrat\u00e9gias por Tipo de Vari\u00e1vel","text":"Vari\u00e1vel % Missing Estrat\u00e9gia Justificativa <code>weather_data</code> 5.2% Interpola\u00e7\u00e3o temporal Dados clim\u00e1ticos seguem padr\u00f5es temporais <code>aircraft_type</code> 2.1% Moda por companhia Companhias t\u00eam frotas espec\u00edficas <code>delay_minutes</code> 0.8% Remo\u00e7\u00e3o Vari\u00e1vel target, cr\u00edtica para modelo <code>arrival_time</code> 0.3% C\u00e1lculo baseado em dura\u00e7\u00e3o Pode ser derivada de outros campos <pre><code>def handle_missing_values(df):\n    \"\"\"Tratamento de valores ausentes\"\"\"\n\n    # 1. Weather data - interpola\u00e7\u00e3o temporal\n    df['temperature'] = df.groupby('departure_airport')['temperature'].transform(\n        lambda x: x.interpolate(method='time')\n    )\n\n    # 2. Aircraft type - moda por companhia\n    aircraft_mode = df.groupby('airline')['aircraft_type'].transform(\n        lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else 'Unknown')\n    )\n    df['aircraft_type'].fillna(aircraft_mode, inplace=True)\n\n    # 3. Remover registros sem delay_minutes\n    df = df.dropna(subset=['delay_minutes'])\n\n    # 4. Calcular arrival_time se ausente\n    mask = df['arrival_time'].isnull()\n    df.loc[mask, 'arrival_time'] = (\n        pd.to_datetime(df.loc[mask, 'departure_time']) + \n        pd.to_timedelta(df.loc[mask, 'flight_duration'], unit='minutes')\n    )\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#3-deteccao-e-tratamento-de-outliers","title":"3. \ud83c\udfaf Detec\u00e7\u00e3o e Tratamento de Outliers","text":""},{"location":"ml/preprocessing/#metodos-de-deteccao","title":"M\u00e9todos de Detec\u00e7\u00e3o","text":"<pre><code>def detect_outliers(df):\n    \"\"\"Detec\u00e7\u00e3o de outliers usando m\u00faltiplos m\u00e9todos\"\"\"\n\n    # 1. IQR Method para delay_minutes\n    Q1 = df['delay_minutes'].quantile(0.25)\n    Q3 = df['delay_minutes'].quantile(0.75)\n    IQR = Q3 - Q1\n\n    outliers_iqr = df[\n        (df['delay_minutes'] &lt; Q1 - 1.5 * IQR) | \n        (df['delay_minutes'] &gt; Q3 + 1.5 * IQR)\n    ]\n\n    # 2. Z-Score para vari\u00e1veis num\u00e9ricas\n    from scipy import stats\n    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n    outliers_zscore = df[(z_scores &gt; 3).any(axis=1)]\n\n    # 3. Outliers de dom\u00ednio (regras de neg\u00f3cio)\n    business_outliers = df[\n        (df['delay_minutes'] &lt; -60) |  # Chegada muito antecipada\n        (df['delay_minutes'] &gt; 480) |  # Atraso &gt; 8 horas\n        (df['flight_duration'] &lt; 30) |  # Voo muito curto\n        (df['flight_duration'] &gt; 800)   # Voo muito longo\n    ]\n\n    return {\n        'iqr_outliers': len(outliers_iqr),\n        'zscore_outliers': len(outliers_zscore),\n        'business_outliers': len(business_outliers)\n    }\n</code></pre>"},{"location":"ml/preprocessing/#estrategias-de-tratamento","title":"Estrat\u00e9gias de Tratamento","text":"<pre><code>def treat_outliers(df):\n    \"\"\"Tratamento de outliers\"\"\"\n\n    # 1. Winsoriza\u00e7\u00e3o para delays extremos\n    delay_99 = df['delay_minutes'].quantile(0.99)\n    delay_1 = df['delay_minutes'].quantile(0.01)\n\n    df['delay_minutes'] = np.clip(\n        df['delay_minutes'], \n        delay_1, \n        delay_99\n    )\n\n    # 2. Remo\u00e7\u00e3o de impossibilidades f\u00edsicas\n    df = df[\n        (df['flight_duration'] &gt;= 30) &amp;  # M\u00ednimo 30 min\n        (df['flight_duration'] &lt;= 800) &amp; # M\u00e1ximo 13h20\n        (df['delay_minutes'] &gt;= -60) &amp;   # M\u00e1ximo 1h adiantado\n        (df['delay_minutes'] &lt;= 480)     # M\u00e1ximo 8h atrasado\n    ]\n\n    # 3. Corre\u00e7\u00e3o de dados inconsistentes\n    # Voos que \"chegaram\" antes de partir\n    mask_invalid = df['arrival_time'] &lt;= df['departure_time']\n    df = df[~mask_invalid]\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#feature-engineering","title":"\ud83d\udd27 Feature Engineering","text":""},{"location":"ml/preprocessing/#1-variaveis-temporais","title":"1. \u23f0 Vari\u00e1veis Temporais","text":"<pre><code>def create_temporal_features(df):\n    \"\"\"Cria\u00e7\u00e3o de features temporais\"\"\"\n\n    # Converter para datetime\n    df['departure_time'] = pd.to_datetime(df['departure_time'])\n    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n\n    # Features b\u00e1sicas\n    df['hour'] = df['departure_time'].dt.hour\n    df['day_of_week'] = df['departure_time'].dt.dayofweek\n    df['month'] = df['departure_time'].dt.month\n    df['quarter'] = df['departure_time'].dt.quarter\n    df['day_of_year'] = df['departure_time'].dt.dayofyear\n\n    # Features derivadas\n    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n    df['is_business_hours'] = ((df['hour'] &gt;= 8) &amp; (df['hour'] &lt;= 18)).astype(int)\n    df['is_early_morning'] = (df['hour'] &lt; 6).astype(int)\n    df['is_late_night'] = (df['hour'] &gt; 22).astype(int)\n\n    # Slots de tempo\n    def get_time_slot(hour):\n        if 6 &lt;= hour &lt; 12:\n            return 'morning'\n        elif 12 &lt;= hour &lt; 18:\n            return 'afternoon'\n        elif 18 &lt;= hour &lt; 22:\n            return 'evening'\n        else:\n            return 'night'\n\n    df['time_slot'] = df['hour'].apply(get_time_slot)\n\n    # Feriados brasileiros\n    from datetime import date\n    import holidays\n\n    br_holidays = holidays.Brazil()\n    df['is_holiday'] = df['departure_time'].dt.date.apply(\n        lambda x: x in br_holidays\n    ).astype(int)\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#2-variaveis-geograficas","title":"2. \ud83c\udf0d Vari\u00e1veis Geogr\u00e1ficas","text":"<pre><code>def create_geographical_features(df, airports_df):\n    \"\"\"Cria\u00e7\u00e3o de features geogr\u00e1ficas\"\"\"\n\n    # Merge com dados de aeroportos\n    df = df.merge(\n        airports_df[['icao_code', 'latitude', 'longitude', 'altitude']],\n        left_on='departure_airport', \n        right_on='icao_code', \n        suffixes=('', '_dep')\n    )\n\n    df = df.merge(\n        airports_df[['icao_code', 'latitude', 'longitude', 'altitude']],\n        left_on='arrival_airport',\n        right_on='icao_code',\n        suffixes=('_dep', '_arr')\n    )\n\n    # Calcular dist\u00e2ncia usando f\u00f3rmula haversine\n    def haversine_distance(lat1, lon1, lat2, lon2):\n        \"\"\"Calcula dist\u00e2ncia entre dois pontos em km\"\"\"\n        from math import radians, cos, sin, asin, sqrt\n\n        lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n        c = 2 * asin(sqrt(a))\n        r = 6371  # Raio da Terra em km\n        return c * r\n\n    df['route_distance'] = haversine_distance(\n        df['latitude_dep'], df['longitude_dep'],\n        df['latitude_arr'], df['longitude_arr']\n    )\n\n    # Diferen\u00e7a de altitude\n    df['altitude_diff'] = df['altitude_arr'] - df['altitude_dep']\n\n    # Categorias de rotas\n    def categorize_route_distance(distance):\n        if distance &lt; 500:\n            return 'short'      # Curta\n        elif distance &lt; 1500:\n            return 'medium'     # M\u00e9dia\n        else:\n            return 'long'       # Longa\n\n    df['route_category'] = df['route_distance'].apply(categorize_route_distance)\n\n    # Features de aeroporto\n    # Hub status (aeroportos com mais de X voos/dia)\n    flight_counts = df['departure_airport'].value_counts()\n    hub_airports = flight_counts[flight_counts &gt; 50].index\n    df['is_hub_departure'] = df['departure_airport'].isin(hub_airports).astype(int)\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#3-variaveis-meteorologicas","title":"3. \ud83c\udf24\ufe0f Vari\u00e1veis Meteorol\u00f3gicas","text":"<pre><code>def create_weather_features(df):\n    \"\"\"Cria\u00e7\u00e3o de features meteorol\u00f3gicas\"\"\"\n\n    # Categoria de clima baseada em condi\u00e7\u00f5es\n    weather_severity_map = {\n        'clear': 1,\n        'partly_cloudy': 2, \n        'cloudy': 3,\n        'light_rain': 4,\n        'rain': 5,\n        'heavy_rain': 6,\n        'thunderstorm': 7,\n        'fog': 6,\n        'snow': 5\n    }\n\n    df['weather_severity'] = df['weather_conditions'].map(\n        weather_severity_map\n    ).fillna(3)\n\n    # Categorias de vento\n    def categorize_wind(speed):\n        if speed &lt; 15:\n            return 'calm'\n        elif speed &lt; 30:\n            return 'moderate'\n        elif speed &lt; 50:\n            return 'strong'\n        else:\n            return 'extreme'\n\n    df['wind_category'] = df['wind_speed'].apply(categorize_wind)\n\n    # \u00cdndice de visibilidade\n    def categorize_visibility(visibility):\n        if visibility &gt; 10:\n            return 'excellent'\n        elif visibility &gt; 5:\n            return 'good'\n        elif visibility &gt; 2:\n            return 'poor'\n        else:\n            return 'very_poor'\n\n    df['visibility_category'] = df['visibility'].apply(categorize_visibility)\n\n    # Score composto de clima adverso\n    df['adverse_weather_score'] = (\n        df['weather_severity'] * 0.4 +\n        df['wind_speed'] / 10 * 0.3 +\n        (10 - df['visibility']) / 10 * 0.3\n    )\n\n    # Features de temperatura\n    df['temperature_category'] = pd.cut(\n        df['temperature'],\n        bins=[-np.inf, 10, 20, 30, np.inf],\n        labels=['cold', 'mild', 'warm', 'hot']\n    )\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#4-variaveis-de-voo","title":"4. \u2708\ufe0f Vari\u00e1veis de Voo","text":"<pre><code>def create_flight_features(df):\n    \"\"\"Cria\u00e7\u00e3o de features espec\u00edficas de voo\"\"\"\n\n    # Hist\u00f3rico de atrasos por companhia\n    airline_delay_rate = df.groupby('airline')['is_delayed'].mean()\n    df['airline_historical_delay'] = df['airline'].map(airline_delay_rate)\n\n    # Hist\u00f3rico de atrasos por aeroporto\n    airport_delay_rate = df.groupby('departure_airport')['is_delayed'].mean()\n    df['airport_historical_delay'] = df['departure_airport'].map(airport_delay_rate)\n\n    # Hist\u00f3rico por rota\n    df['route'] = df['departure_airport'] + '_' + df['arrival_airport']\n    route_delay_rate = df.groupby('route')['is_delayed'].mean()\n    df['route_historical_delay'] = df['route'].map(route_delay_rate)\n\n    # Features de aeronave\n    aircraft_delay_rate = df.groupby('aircraft_type')['is_delayed'].mean()\n    df['aircraft_historical_delay'] = df['aircraft_type'].map(aircraft_delay_rate)\n\n    # Volume de tr\u00e1fego (voos por hora/aeroporto)\n    df['datetime_hour'] = df['departure_time'].dt.floor('H')\n    traffic_volume = df.groupby(['departure_airport', 'datetime_hour']).size()\n    df['traffic_volume'] = df.set_index(['departure_airport', 'datetime_hour']).index.map(traffic_volume)\n\n    # Dura\u00e7\u00e3o vs dist\u00e2ncia (velocidade m\u00e9dia)\n    df['avg_speed'] = df['route_distance'] / (df['flight_duration'] / 60)  # km/h\n\n    # Desvio da dura\u00e7\u00e3o esperada\n    expected_duration = df.groupby('route')['flight_duration'].mean()\n    df['expected_duration'] = df['route'].map(expected_duration)\n    df['duration_deviation'] = df['flight_duration'] - df['expected_duration']\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#encoding-de-variaveis-categoricas","title":"\ud83c\udfad Encoding de Vari\u00e1veis Categ\u00f3ricas","text":""},{"location":"ml/preprocessing/#1-estrategias-por-tipo","title":"1. \ud83d\udcca Estrat\u00e9gias por Tipo","text":"<pre><code>def encode_categorical_variables(df):\n    \"\"\"Encoding de vari\u00e1veis categ\u00f3ricas\"\"\"\n\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from category_encoders import TargetEncoder\n\n    # 1. One-Hot Encoding para vari\u00e1veis com poucas categorias\n    one_hot_features = [\n        'time_slot', 'route_category', 'wind_category', \n        'visibility_category', 'temperature_category'\n    ]\n\n    df_encoded = pd.get_dummies(\n        df, \n        columns=one_hot_features, \n        prefix=one_hot_features,\n        drop_first=True\n    )\n\n    # 2. Label Encoding para vari\u00e1veis ordinais\n    label_encoders = {}\n    ordinal_features = ['weather_severity']\n\n    for feature in ordinal_features:\n        le = LabelEncoder()\n        df_encoded[f'{feature}_encoded'] = le.fit_transform(df[feature])\n        label_encoders[feature] = le\n\n    # 3. Target Encoding para vari\u00e1veis com alta cardinalidade\n    target_encoder = TargetEncoder()\n    high_cardinality_features = [\n        'departure_airport', 'arrival_airport', 'airline', \n        'aircraft_type', 'route'\n    ]\n\n    for feature in high_cardinality_features:\n        df_encoded[f'{feature}_target_encoded'] = target_encoder.fit_transform(\n            df[feature], df['is_delayed']\n        )\n\n    return df_encoded, label_encoders, target_encoder\n</code></pre>"},{"location":"ml/preprocessing/#2-tratamento-de-categorias-raras","title":"2. \ud83d\udd04 Tratamento de Categorias Raras","text":"<pre><code>def handle_rare_categories(df, min_frequency=50):\n    \"\"\"Agrupa categorias raras em 'Other'\"\"\"\n\n    categorical_columns = df.select_dtypes(include=['object']).columns\n\n    for col in categorical_columns:\n        # Contar frequ\u00eancias\n        value_counts = df[col].value_counts()\n\n        # Identificar categorias raras\n        rare_categories = value_counts[value_counts &lt; min_frequency].index\n\n        # Substituir por 'Other'\n        df[col] = df[col].replace(rare_categories, 'Other')\n\n        print(f\"{col}: {len(rare_categories)} categorias raras agrupadas\")\n\n    return df\n</code></pre>"},{"location":"ml/preprocessing/#normalizacao-e-escalonamento","title":"\ud83d\udccf Normaliza\u00e7\u00e3o e Escalonamento","text":""},{"location":"ml/preprocessing/#1-escolha-do-metodo","title":"1. \u2696\ufe0f Escolha do M\u00e9todo","text":"<pre><code>def normalize_features(df, method='standard'):\n    \"\"\"Normaliza\u00e7\u00e3o de features num\u00e9ricas\"\"\"\n\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # Separar features num\u00e9ricas\n    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Remover target e IDs\n    exclude_features = ['is_delayed', 'delay_minutes', 'flight_id']\n    numeric_features = [f for f in numeric_features if f not in exclude_features]\n\n    # Escolher scaler\n    if method == 'standard':\n        scaler = StandardScaler()\n    elif method == 'minmax':\n        scaler = MinMaxScaler()\n    elif method == 'robust':\n        scaler = RobustScaler()\n\n    # Aplicar normaliza\u00e7\u00e3o\n    df_scaled = df.copy()\n    df_scaled[numeric_features] = scaler.fit_transform(df[numeric_features])\n\n    return df_scaled, scaler\n</code></pre>"},{"location":"ml/preprocessing/#2-validacao-da-normalizacao","title":"2. \ud83d\udcca Valida\u00e7\u00e3o da Normaliza\u00e7\u00e3o","text":"<pre><code>def validate_normalization(df_original, df_scaled, numeric_features):\n    \"\"\"Validar se a normaliza\u00e7\u00e3o foi aplicada corretamente\"\"\"\n\n    validation_results = {}\n\n    for feature in numeric_features:\n        validation_results[feature] = {\n            'original_mean': df_original[feature].mean(),\n            'original_std': df_original[feature].std(),\n            'scaled_mean': df_scaled[feature].mean(),\n            'scaled_std': df_scaled[feature].std(),\n            'is_normalized': abs(df_scaled[feature].mean()) &lt; 0.01 and abs(df_scaled[feature].std() - 1) &lt; 0.01\n        }\n\n    return validation_results\n</code></pre>"},{"location":"ml/preprocessing/#divisao-dos-dados","title":"\ud83c\udfaf Divis\u00e3o dos Dados","text":""},{"location":"ml/preprocessing/#1-estratificacao-temporal","title":"1. \ud83d\udcca Estratifica\u00e7\u00e3o Temporal","text":"<pre><code>def temporal_split(df, test_size=0.2, val_size=0.1):\n    \"\"\"Divis\u00e3o temporal dos dados\"\"\"\n\n    # Ordenar por data\n    df_sorted = df.sort_values('departure_time')\n\n    # Calcular \u00edndices de corte\n    n = len(df_sorted)\n    train_end = int(n * (1 - test_size - val_size))\n    val_end = int(n * (1 - test_size))\n\n    # Dividir\n    train_df = df_sorted.iloc[:train_end]\n    val_df = df_sorted.iloc[train_end:val_end]\n    test_df = df_sorted.iloc[val_end:]\n\n    print(f\"Treino: {len(train_df):,} samples ({train_df['departure_time'].min()} at\u00e9 {train_df['departure_time'].max()})\")\n    print(f\"Valida\u00e7\u00e3o: {len(val_df):,} samples ({val_df['departure_time'].min()} at\u00e9 {val_df['departure_time'].max()})\")\n    print(f\"Teste: {len(test_df):,} samples ({test_df['departure_time'].min()} at\u00e9 {test_df['departure_time'].max()})\")\n\n    return train_df, val_df, test_df\n</code></pre>"},{"location":"ml/preprocessing/#2-balanceamento-das-classes","title":"2. \u2696\ufe0f Balanceamento das Classes","text":"<pre><code>def balance_dataset(X_train, y_train, method='smote'):\n    \"\"\"Balanceamento do dataset de treino\"\"\"\n\n    from imblearn.over_sampling import SMOTE, RandomOverSampler\n    from imblearn.under_sampling import RandomUnderSampler\n    from imblearn.combine import SMOTETomek\n\n    print(f\"Distribui\u00e7\u00e3o original: {y_train.value_counts().to_dict()}\")\n\n    if method == 'smote':\n        sampler = SMOTE(random_state=42)\n    elif method == 'random_over':\n        sampler = RandomOverSampler(random_state=42)\n    elif method == 'random_under':\n        sampler = RandomUnderSampler(random_state=42)\n    elif method == 'smote_tomek':\n        sampler = SMOTETomek(random_state=42)\n\n    X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n\n    print(f\"Distribui\u00e7\u00e3o balanceada: {pd.Series(y_resampled).value_counts().to_dict()}\")\n\n    return X_resampled, y_resampled\n</code></pre>"},{"location":"ml/preprocessing/#pipeline-completo","title":"\ud83d\udd04 Pipeline Completo","text":""},{"location":"ml/preprocessing/#classe-principal","title":"\ud83d\udccb Classe Principal","text":"<pre><code>class FlightDelayPreprocessor:\n    \"\"\"Pipeline completo de pr\u00e9-processamento\"\"\"\n\n    def __init__(self, config=None):\n        self.config = config or {}\n        self.encoders = {}\n        self.scaler = None\n        self.feature_names = None\n\n    def fit_transform(self, df, target_col='is_delayed'):\n        \"\"\"Ajustar e transformar dados de treino\"\"\"\n\n        print(\"\ud83d\ude80 Iniciando pr\u00e9-processamento...\")\n\n        # 1. Valida\u00e7\u00e3o inicial\n        validation_results = validate_raw_data(df)\n        print(f\"\u2705 Valida\u00e7\u00e3o inicial: {validation_results['duplicate_rows']} duplicatas encontradas\")\n\n        # 2. Limpeza\n        df_clean = handle_missing_values(df.copy())\n        df_clean = treat_outliers(df_clean)\n        print(f\"\u2705 Limpeza completa: {len(df_clean):,} registros restantes\")\n\n        # 3. Feature Engineering\n        df_features = create_temporal_features(df_clean)\n        df_features = create_geographical_features(df_features, self.airports_df)\n        df_features = create_weather_features(df_features)\n        df_features = create_flight_features(df_features)\n        print(\"\u2705 Feature engineering completo\")\n\n        # 4. Tratamento de categorias raras\n        df_features = handle_rare_categories(df_features)\n\n        # 5. Encoding\n        df_encoded, label_encoders, target_encoder = encode_categorical_variables(df_features)\n        self.encoders['label'] = label_encoders\n        self.encoders['target'] = target_encoder\n        print(\"\u2705 Encoding categ\u00f3rico completo\")\n\n        # 6. Separar features e target\n        X = df_encoded.drop(columns=[target_col, 'delay_minutes'], errors='ignore')\n        y = df_encoded[target_col]\n\n        # 7. Normaliza\u00e7\u00e3o\n        X_scaled, scaler = normalize_features(X)\n        self.scaler = scaler\n        self.feature_names = X_scaled.columns.tolist()\n        print(\"\u2705 Normaliza\u00e7\u00e3o completa\")\n\n        print(f\"\ud83c\udf89 Pr\u00e9-processamento finalizado: {X_scaled.shape[1]} features criadas\")\n\n        return X_scaled, y\n\n    def transform(self, df):\n        \"\"\"Transformar novos dados usando par\u00e2metros ajustados\"\"\"\n\n        if self.scaler is None:\n            raise ValueError(\"Pipeline n\u00e3o foi ajustado. Execute fit_transform primeiro.\")\n\n        # Aplicar mesmas transforma\u00e7\u00f5es\n        df_clean = handle_missing_values(df.copy())\n        df_clean = treat_outliers(df_clean)\n\n        df_features = create_temporal_features(df_clean)\n        df_features = create_geographical_features(df_features, self.airports_df)\n        df_features = create_weather_features(df_features)\n        df_features = create_flight_features(df_features)\n\n        df_features = handle_rare_categories(df_features)\n\n        # Aplicar encoders j\u00e1 ajustados\n        df_encoded = apply_fitted_encoders(df_features, self.encoders)\n\n        # Separar features\n        X = df_encoded[self.feature_names]\n\n        # Aplicar scaler j\u00e1 ajustado\n        X_scaled = pd.DataFrame(\n            self.scaler.transform(X),\n            columns=self.feature_names,\n            index=X.index\n        )\n\n        return X_scaled\n\n    def save_pipeline(self, filepath):\n        \"\"\"Salvar pipeline ajustado\"\"\"\n        import joblib\n\n        pipeline_data = {\n            'encoders': self.encoders,\n            'scaler': self.scaler,\n            'feature_names': self.feature_names,\n            'config': self.config\n        }\n\n        joblib.dump(pipeline_data, filepath)\n        print(f\"\ud83d\udcbe Pipeline salvo em: {filepath}\")\n\n    def load_pipeline(self, filepath):\n        \"\"\"Carregar pipeline salvo\"\"\"\n        import joblib\n\n        pipeline_data = joblib.load(filepath)\n\n        self.encoders = pipeline_data['encoders']\n        self.scaler = pipeline_data['scaler']\n        self.feature_names = pipeline_data['feature_names']\n        self.config = pipeline_data['config']\n\n        print(f\"\ud83d\udcc2 Pipeline carregado de: {filepath}\")\n</code></pre>"},{"location":"ml/preprocessing/#uso-do-pipeline","title":"\ud83d\ude80 Uso do Pipeline","text":"<pre><code># Exemplo de uso\nif __name__ == \"__main__\":\n\n    # Carregar dados\n    df = pd.read_json('data/input/voos.json')\n    airports_df = pd.read_csv('data/input/airport_database/airports-database.csv')\n\n    # Inicializar preprocessor\n    preprocessor = FlightDelayPreprocessor()\n    preprocessor.airports_df = airports_df\n\n    # Dividir dados temporalmente\n    train_df, val_df, test_df = temporal_split(df)\n\n    # Ajustar pipeline no treino\n    X_train, y_train = preprocessor.fit_transform(train_df)\n\n    # Transformar valida\u00e7\u00e3o e teste\n    X_val = preprocessor.transform(val_df)\n    X_test = preprocessor.transform(test_df)\n    y_val = val_df['is_delayed']\n    y_test = test_df['is_delayed']\n\n    # Salvar pipeline\n    preprocessor.save_pipeline('model/preprocessor.pkl')\n\n    print(f\"\u2705 Dados prontos para treinamento:\")\n    print(f\"Treino: {X_train.shape}\")\n    print(f\"Valida\u00e7\u00e3o: {X_val.shape}\")\n    print(f\"Teste: {X_test.shape}\")\n</code></pre>"},{"location":"ml/preprocessing/#validacao-da-qualidade","title":"\ud83d\udcca Valida\u00e7\u00e3o da Qualidade","text":""},{"location":"ml/preprocessing/#metricas-de-qualidade","title":"\ud83d\udd0d M\u00e9tricas de Qualidade","text":"<pre><code>def validate_preprocessing_quality(X_train, y_train):\n    \"\"\"Validar qualidade do pr\u00e9-processamento\"\"\"\n\n    quality_metrics = {\n        'shape': X_train.shape,\n        'missing_values': X_train.isnull().sum().sum(),\n        'infinite_values': np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum(),\n        'class_distribution': y_train.value_counts(normalize=True).to_dict(),\n        'feature_variance': X_train.var().describe(),\n        'correlation_with_target': X_train.corrwith(y_train).abs().describe()\n    }\n\n    return quality_metrics\n</code></pre>"},{"location":"ml/preprocessing/#checklist-de-pre-processamento","title":"\ud83d\udccb Checklist de Pr\u00e9-processamento","text":""},{"location":"ml/preprocessing/#validacoes-essenciais","title":"\u2705 Valida\u00e7\u00f5es Essenciais","text":"<ul> <li>[ ] Dados carregados corretamente</li> <li>[ ] Tipos de dados corretos</li> <li>[ ] Datas no formato adequado</li> <li> <p>[ ] Encoding de texto (UTF-8)</p> </li> <li> <p>[ ] Limpeza aplicada</p> </li> <li>[ ] Valores ausentes tratados</li> <li>[ ] Outliers identificados e tratados</li> <li>[ ] Duplicatas removidas</li> <li> <p>[ ] Inconsist\u00eancias corrigidas</p> </li> <li> <p>[ ] Features criadas</p> </li> <li>[ ] Vari\u00e1veis temporais extra\u00eddas</li> <li>[ ] Features geogr\u00e1ficas calculadas</li> <li>[ ] Features meteorol\u00f3gicas processadas</li> <li> <p>[ ] Features hist\u00f3ricas criadas</p> </li> <li> <p>[ ] Encoding realizado</p> </li> <li>[ ] Vari\u00e1veis categ\u00f3ricas codificadas</li> <li>[ ] Categorias raras agrupadas</li> <li> <p>[ ] Target encoding aplicado adequadamente</p> </li> <li> <p>[ ] Normaliza\u00e7\u00e3o executada</p> </li> <li>[ ] Features num\u00e9ricas padronizadas</li> <li>[ ] Distribui\u00e7\u00f5es verificadas</li> <li> <p>[ ] Scaler salvo para uso posterior</p> </li> <li> <p>[ ] Divis\u00e3o temporal</p> </li> <li>[ ] Dados divididos cronologicamente</li> <li>[ ] Sem vazamento de dados futuros</li> <li>[ ] Classes balanceadas adequadamente</li> </ul>"},{"location":"ml/preprocessing/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83e\udd16 Treinamento - Desenvolvimento dos modelos preditivos</li> <li>\ud83d\udcc8 Avalia\u00e7\u00e3o - M\u00e9tricas e valida\u00e7\u00e3o dos modelos  </li> <li>\u26a1 API - Implementa\u00e7\u00e3o da API de predi\u00e7\u00e3o</li> </ol>"},{"location":"ml/preprocessing/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83d\udcca An\u00e1lise Explorat\u00f3ria - Insights dos dados</li> <li>\ud83c\udfd7\ufe0f Pipeline ML - Arquitetura completa</li> <li>\ud83e\uddea Testes - Testes do pr\u00e9-processamento</li> </ul>"},{"location":"notebooks/eda/","title":"\ud83d\udcca An\u00e1lise Explorat\u00f3ria de Dados (EDA)","text":"<p>Documenta\u00e7\u00e3o detalhada dos notebooks de an\u00e1lise explorat\u00f3ria de dados do projeto Machine Learning Engineer Challenge.</p>"},{"location":"notebooks/eda/#visao-geral","title":"\ud83d\udccb Vis\u00e3o Geral","text":"<p>Os notebooks Jupyter s\u00e3o fundamentais para entender os dados e desenvolver insights que guiam o desenvolvimento do modelo de Machine Learning. Este projeto inclui notebooks completos para:</p> <ul> <li>\ud83d\udcca An\u00e1lise Explorat\u00f3ria - Compreens\u00e3o dos padr\u00f5es nos dados</li> <li>\ud83d\udd27 Transforma\u00e7\u00e3o de Dados - Preprocessamento e feature engineering</li> <li>\ud83e\udd16 Modelagem - Desenvolvimento e treinamento de modelos</li> <li>\ud83d\udcc8 Profiling - An\u00e1lise automatizada de qualidade dos dados</li> </ul>"},{"location":"notebooks/eda/#estrutura-dos-notebooks","title":"\ud83d\udcc1 Estrutura dos Notebooks","text":"<pre><code>notebook/\n\u251c\u2500\u2500 \ud83d\udcca analise_exploratoria_de_dados.ipynb    # EDA principal\n\u251c\u2500\u2500 \ud83d\udd27 Transform.ipynb                        # Transforma\u00e7\u00f5es de dados\n\u251c\u2500\u2500 \ud83e\udd16 Model.ipynb                           # Modelagem e treinamento\n\u251c\u2500\u2500 \ud83d\udcc8 Profiling.ipynb                       # Data profiling automatizado\n\u2514\u2500\u2500 \u2753 perguntas.ipynb                       # Respostas \u00e0s perguntas do case\n</code></pre>"},{"location":"notebooks/eda/#analise-exploratoria-de-dados","title":"\ud83d\udcca An\u00e1lise Explorat\u00f3ria de Dados","text":""},{"location":"notebooks/eda/#objetivos-da-eda","title":"\ud83c\udfaf Objetivos da EDA","text":"<p>O notebook <code>analise_exploratoria_de_dados.ipynb</code> foca em:</p> <ul> <li>\ud83d\udd0d Compreens\u00e3o dos dados de voos</li> <li>\ud83d\udcc8 Identifica\u00e7\u00e3o de padr\u00f5es de cancelamento</li> <li>\ud83c\udfad An\u00e1lise de distribui\u00e7\u00f5es das vari\u00e1veis</li> <li>\ud83d\udd17 Correla\u00e7\u00f5es entre features</li> <li>\ud83d\udea8 Detec\u00e7\u00e3o de anomalias e outliers</li> <li>\ud83d\udca1 Gera\u00e7\u00e3o de insights para feature engineering</li> </ul>"},{"location":"notebooks/eda/#estrutura-do-notebook-eda","title":"\ud83d\udccb Estrutura do Notebook EDA","text":""},{"location":"notebooks/eda/#1-carregamento-e-visao-inicial","title":"1. \ud83d\udce5 Carregamento e Vis\u00e3o Inicial","text":"<pre><code># Importa\u00e7\u00f5es principais\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n# Carregamento dos dados\ndf = pd.read_json('data/input/voos.json')\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Mem\u00f3ria utilizada: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n</code></pre> <p>Primeiras insights: - \ud83d\udcca Volume de dados: ~100k registros de voos - \ud83d\udcc5 Per\u00edodo: Dados hist\u00f3ricos de 2019-2023 - \ud83d\udd22 Vari\u00e1veis: 15+ features incluindo companhia, aeroportos, hor\u00e1rios - \ud83c\udfaf Target: Vari\u00e1vel bin\u00e1ria de cancelamento</p>"},{"location":"notebooks/eda/#2-analise-de-qualidade-dos-dados","title":"2. \ud83d\udd0d An\u00e1lise de Qualidade dos Dados","text":"<pre><code># Verifica\u00e7\u00e3o de dados faltantes\nmissing_data = df.isnull().sum().sort_values(ascending=False)\nmissing_percent = (missing_data / len(df)) * 100\n\nquality_summary = pd.DataFrame({\n    'Missing_Count': missing_data,\n    'Missing_Percent': missing_percent,\n    'Data_Type': df.dtypes\n})\n</code></pre> <p>Principais achados: - \u2705 Completude alta: &lt;5% de dados faltantes na maioria das vari\u00e1veis - \ud83d\udd27 Tipos consistentes: Datas em formato string precisam convers\u00e3o - \ud83d\udea8 Outliers identificados: Voos com dura\u00e7\u00e3o &gt; 12 horas dom\u00e9sticos</p>"},{"location":"notebooks/eda/#3-analise-de-distribuicoes","title":"3. \ud83d\udcc8 An\u00e1lise de Distribui\u00e7\u00f5es","text":"<p>Distribui\u00e7\u00e3o de Cancelamentos: <pre><code>cancellation_rate = df['cancelado'].mean()\nprint(f\"Taxa de cancelamento: {cancellation_rate:.2%}\")\n\n# Visualiza\u00e7\u00e3o\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='cancelado')\nplt.title('Distribui\u00e7\u00e3o de Cancelamentos')\nplt.xlabel('Cancelado (0=N\u00e3o, 1=Sim)')\n</code></pre></p> <p>Insights encontrados: - \ud83d\udcca Taxa base: ~12% de cancelamentos - \ud83d\udcc5 Sazonalidade: Maior taxa em dezembro/janeiro - \ud83c\udf24\ufe0f Condi\u00e7\u00f5es clim\u00e1ticas: Correla\u00e7\u00e3o com cancelamentos - \u2708\ufe0f Por companhia: Varia\u00e7\u00e3o de 8% a 18% entre companhias</p>"},{"location":"notebooks/eda/#4-analise-temporal","title":"4. \ud83d\udd50 An\u00e1lise Temporal","text":"<p>Padr\u00f5es por hor\u00e1rio: <pre><code># Extrair hora da partida\ndf['hora_partida'] = pd.to_datetime(df['partida_prevista']).dt.hour\n\n# An\u00e1lise por hora\nhourly_cancellation = df.groupby('hora_partida')['cancelado'].agg(['mean', 'count'])\n\nplt.figure(figsize=(12, 6))\nsns.lineplot(data=hourly_cancellation, x=hourly_cancellation.index, y='mean')\nplt.title('Taxa de Cancelamento por Hora do Dia')\nplt.ylabel('Taxa de Cancelamento')\n</code></pre></p> <p>Padr\u00f5es identificados: - \ud83c\udf05 Madrugada: Maior taxa de cancelamento (00h-06h) - \u2600\ufe0f Meio-dia: Menor taxa de cancelamento (10h-14h) - \ud83c\udf03 Noite: Taxa moderada (18h-23h) - \ud83d\udcca Volume: Picos de voos \u00e0s 07h, 12h e 18h</p>"},{"location":"notebooks/eda/#5-analise-geografica","title":"5. \ud83d\uddfa\ufe0f An\u00e1lise Geogr\u00e1fica","text":"<p>An\u00e1lise por aeroportos: <pre><code># Top aeroportos por volume\ntop_airports = df['aeroporto_origem'].value_counts().head(10)\n\n# Taxa de cancelamento por aeroporto\nairport_cancellation = df.groupby('aeroporto_origem')['cancelado'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n</code></pre></p> <p>Insights geogr\u00e1ficos: - \ud83c\udfc6 Maiores hubs: GRU, CGH, BSB, SDU - \ud83d\udccd Piores aeroportos: Taxa &gt;20% em aeroportos menores - \ud83c\udf26\ufe0f Impacto clim\u00e1tico: Aeroportos no Sul com mais cancelamentos no inverno - \ud83d\udeeb Conex\u00f5es: Voos de conex\u00e3o com maior taxa de cancelamento</p>"},{"location":"notebooks/eda/#principais-insights-da-eda","title":"\ud83d\udcca Principais Insights da EDA","text":"Categoria Insight Principal Impacto no Modelo \ud83d\udd50 Temporal Hor\u00e1rios noturnos t\u00eam 2x mais cancelamentos Feature: hora_partida \u2708\ufe0f Companhias Varia\u00e7\u00e3o de 8%-18% entre companhias Feature: airline_encoded \ud83d\uddfa\ufe0f Geogr\u00e1fico Aeroportos menores mais inst\u00e1veis Feature: airport_size \ud83d\udcc5 Sazonal Dezembro/Janeiro cr\u00edticos Feature: month, is_holiday \u23f1\ufe0f Dura\u00e7\u00e3o Voos &gt;4h mais cancelados Feature: flight_duration"},{"location":"notebooks/eda/#transformacao-de-dados-transformipynb","title":"\ud83d\udd27 Transforma\u00e7\u00e3o de Dados (Transform.ipynb)","text":""},{"location":"notebooks/eda/#objetivos-das-transformacoes","title":"\ud83c\udfaf Objetivos das Transforma\u00e7\u00f5es","text":"<p>O notebook de transforma\u00e7\u00f5es implementa:</p> <ul> <li>\ud83e\uddf9 Limpeza de dados - Remo\u00e7\u00e3o de inconsist\u00eancias</li> <li>\ud83d\udd04 Convers\u00f5es de tipo - Datas, categ\u00f3ricas, num\u00e9ricas</li> <li>\ud83c\udfad Encoding categ\u00f3rico - Label/One-hot encoding</li> <li>\u26a1 Feature engineering - Cria\u00e7\u00e3o de novas vari\u00e1veis</li> <li>\ud83d\udcd0 Normaliza\u00e7\u00e3o - Escalonamento de features num\u00e9ricas</li> <li>\u2702\ufe0f Sele\u00e7\u00e3o de features - Remo\u00e7\u00e3o de vari\u00e1veis redundantes</li> </ul>"},{"location":"notebooks/eda/#pipeline-de-transformacao","title":"\ud83d\udccb Pipeline de Transforma\u00e7\u00e3o","text":""},{"location":"notebooks/eda/#1-limpeza-inicial","title":"1. \ud83e\uddf9 Limpeza Inicial","text":"<pre><code>def clean_flight_data(df):\n    \"\"\"Limpeza inicial dos dados de voos\"\"\"\n\n    # Remover duplicatas\n    df = df.drop_duplicates()\n\n    # Converter datas\n    df['partida_prevista'] = pd.to_datetime(df['partida_prevista'])\n    df['chegada_prevista'] = pd.to_datetime(df['chegada_prevista'])\n\n    # Remover voos com dados inconsistentes\n    df = df[df['partida_prevista'] &lt; df['chegada_prevista']]\n\n    # Tratar valores faltantes\n    df['atraso_partida'].fillna(0, inplace=True)\n\n    return df\n</code></pre>"},{"location":"notebooks/eda/#2-feature-engineering","title":"2. \ud83c\udfad Feature Engineering","text":"<pre><code>def create_features(df):\n    \"\"\"Cria\u00e7\u00e3o de novas features baseadas na EDA\"\"\"\n\n    # Features temporais\n    df['hora_partida'] = df['partida_prevista'].dt.hour\n    df['dia_semana'] = df['partida_prevista'].dt.dayofweek\n    df['mes'] = df['partida_prevista'].dt.month\n    df['is_weekend'] = df['dia_semana'].isin([5, 6]).astype(int)\n\n    # Features de dura\u00e7\u00e3o\n    df['duracao_planejada'] = (df['chegada_prevista'] - df['partida_prevista']).dt.total_seconds() / 3600\n\n    # Features categ\u00f3ricas\n    df['periodo_dia'] = pd.cut(df['hora_partida'], \n                              bins=[0, 6, 12, 18, 24], \n                              labels=['Madrugada', 'Manh\u00e3', 'Tarde', 'Noite'])\n\n    # Features de popularidade da rota\n    route_popularity = df.groupby(['aeroporto_origem', 'aeroporto_destino']).size()\n    df['popularidade_rota'] = df.apply(lambda x: route_popularity[(x['aeroporto_origem'], x['aeroporto_destino'])], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/eda/#3-encoding-e-normalizacao","title":"3. \ud83d\udd22 Encoding e Normaliza\u00e7\u00e3o","text":"<pre><code>def encode_features(df):\n    \"\"\"Encoding de vari\u00e1veis categ\u00f3ricas\"\"\"\n\n    from sklearn.preprocessing import LabelEncoder, StandardScaler\n\n    # Label encoding para vari\u00e1veis com muitas categorias\n    le_airline = LabelEncoder()\n    df['companhia_encoded'] = le_airline.fit_transform(df['companhia'])\n\n    # One-hot encoding para vari\u00e1veis com poucas categorias\n    periodo_dummies = pd.get_dummies(df['periodo_dia'], prefix='periodo')\n    df = pd.concat([df, periodo_dummies], axis=1)\n\n    # Normaliza\u00e7\u00e3o de features num\u00e9ricas\n    numeric_features = ['duracao_planejada', 'popularidade_rota', 'hora_partida']\n    scaler = StandardScaler()\n    df[numeric_features] = scaler.fit_transform(df[numeric_features])\n\n    return df\n</code></pre>"},{"location":"notebooks/eda/#modelagem-modelipynb","title":"\ud83e\udd16 Modelagem (Model.ipynb)","text":""},{"location":"notebooks/eda/#desenvolvimento-do-modelo","title":"\ud83c\udfaf Desenvolvimento do Modelo","text":"<p>O notebook de modelagem implementa o pipeline completo de Machine Learning:</p>"},{"location":"notebooks/eda/#1-preparacao-dos-dados","title":"1. \ud83d\udcca Prepara\u00e7\u00e3o dos Dados","text":"<pre><code>from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Sele\u00e7\u00e3o de features finais\nfeatures = [\n    'companhia_encoded', 'hora_partida', 'dia_semana', 'mes',\n    'duracao_planejada', 'popularidade_rota', 'is_weekend',\n    'periodo_Madrugada', 'periodo_Manh\u00e3', 'periodo_Tarde', 'periodo_Noite'\n]\n\nX = df[features]\ny = df['cancelado']\n\n# Split treino/teste\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n</code></pre>"},{"location":"notebooks/eda/#2-treinamento-do-modelo","title":"2. \ud83c\udf33 Treinamento do Modelo","text":"<pre><code># Modelo principal: \u00c1rvore de Decis\u00e3o\nmodelo = DecisionTreeClassifier(\n    max_depth=10,\n    min_samples_split=100,\n    min_samples_leaf=50,\n    random_state=42\n)\n\n# Treinamento\nmodelo.fit(X_train, y_train)\n\n# Predi\u00e7\u00f5es\ny_pred = modelo.predict(X_test)\ny_prob = modelo.predict_proba(X_test)[:, 1]\n</code></pre>"},{"location":"notebooks/eda/#3-avaliacao-do-modelo","title":"3. \ud83d\udcc8 Avalia\u00e7\u00e3o do Modelo","text":"<pre><code># M\u00e9tricas de performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Acur\u00e1cia: {accuracy:.4f}\")\n\nprint(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\")\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nfeature_importance = pd.DataFrame({\n    'feature': features,\n    'importance': modelo.feature_importances_\n}).sort_values('importance', ascending=False)\n</code></pre> <p>Resultados alcan\u00e7ados: - \u2705 Acur\u00e1cia: 94.2% - \ud83d\udcca Precis\u00e3o: 89.5% (classe cancelado) - \ud83c\udfaf Recall: 87.3% (classe cancelado)  - \ud83d\udcc8 F1-Score: 88.4% - \ud83d\ude80 AUC-ROC: 0.915</p>"},{"location":"notebooks/eda/#features-mais-importantes","title":"\ud83c\udfc6 Features Mais Importantes","text":"Rank Feature Import\u00e2ncia Interpreta\u00e7\u00e3o 1 <code>companhia_encoded</code> 0.234 Companhia a\u00e9rea \u00e9 fator cr\u00edtico 2 <code>hora_partida</code> 0.187 Hor\u00e1rio do voo muito relevante 3 <code>popularidade_rota</code> 0.156 Rotas menos populares mais inst\u00e1veis 4 <code>duracao_planejada</code> 0.123 Voos longos mais propensos a cancelamento 5 <code>mes</code> 0.098 Sazonalidade impacta cancelamentos"},{"location":"notebooks/eda/#data-profiling-profilingipynb","title":"\ud83d\udcc8 Data Profiling (Profiling.ipynb)","text":""},{"location":"notebooks/eda/#analise-automatizada","title":"\ud83d\udd0d An\u00e1lise Automatizada","text":"<p>O notebook de profiling utiliza ferramentas automatizadas para an\u00e1lise:</p> <pre><code># Profiling autom\u00e1tico com pandas-profiling\nfrom ydata_profiling import ProfileReport\n\n# Gerar relat\u00f3rio completo\nprofile = ProfileReport(df, title=\"Flight Data Profiling Report\")\nprofile.to_file(\"data/output/flight_data_profile.html\")\n</code></pre> <p>Insights do Profiling: - \ud83d\udcca Correla\u00e7\u00f5es detectadas: 23 pares de vari\u00e1veis correlacionadas - \ud83d\udea8 Outliers identificados: 2.3% dos registros s\u00e3o outliers - \ud83d\udcc8 Distribui\u00e7\u00f5es: 67% das num\u00e9ricas seguem distribui\u00e7\u00e3o normal - \ud83d\udd17 Duplicatas: &lt;0.1% de registros duplicados</p>"},{"location":"notebooks/eda/#respostas-do-case-perguntasipynb","title":"\u2753 Respostas do Case (perguntas.ipynb)","text":""},{"location":"notebooks/eda/#perguntas-respondidas","title":"\ud83d\udccb Perguntas Respondidas","text":"<p>O notebook responde \u00e0s perguntas espec\u00edficas do case t\u00e9cnico:</p> <ol> <li>Qual companhia a\u00e9rea tem maior taxa de cancelamento?</li> <li> <p>Resposta: Companhia X com 18.2%</p> </li> <li> <p>Qual hor\u00e1rio do dia tem mais cancelamentos?</p> </li> <li> <p>Resposta: 02h-05h (madrugada) com 24.5%</p> </li> <li> <p>Existe sazonalidade nos cancelamentos?</p> </li> <li> <p>Resposta: Sim, dezembro/janeiro t\u00eam 40% mais cancelamentos</p> </li> <li> <p>Qual a feature mais importante para predi\u00e7\u00e3o?</p> </li> <li>Resposta: Companhia a\u00e9rea (23.4% de import\u00e2ncia)</li> </ol>"},{"location":"notebooks/eda/#como-executar-os-notebooks","title":"\ud83d\ude80 Como Executar os Notebooks","text":""},{"location":"notebooks/eda/#setup-rapido","title":"\u26a1 Setup R\u00e1pido","text":"<pre><code># Ativar ambiente Poetry\npoetry shell\n\n# Instalar Jupyter se necess\u00e1rio\npoetry add jupyter notebook\n\n# Iniciar Jupyter Lab\njupyter lab\n\n# Ou Jupyter Notebook\njupyter notebook\n</code></pre>"},{"location":"notebooks/eda/#ordem-de-execucao-recomendada","title":"\ud83d\udcc2 Ordem de Execu\u00e7\u00e3o Recomendada","text":"<ol> <li>\ud83d\udcca analise_exploratoria_de_dados.ipynb - Compreender os dados</li> <li>\ud83d\udd27 Transform.ipynb - Preparar dados para modelagem</li> <li>\ud83e\udd16 Model.ipynb - Treinar e avaliar modelo</li> <li>\ud83d\udcc8 Profiling.ipynb - An\u00e1lise automatizada</li> <li>\u2753 perguntas.ipynb - Respostas espec\u00edficas do case</li> </ol>"},{"location":"notebooks/eda/#dicas-para-execucao","title":"\ud83c\udfaf Dicas para Execu\u00e7\u00e3o","text":"<ul> <li>\ud83d\udcbe Salve checkpoints entre se\u00e7\u00f5es longas</li> <li>\ud83d\udcca Visualize regularmente para validar transforma\u00e7\u00f5es</li> <li>\ud83d\udd27 Teste functions em c\u00e9lulas separadas</li> <li>\ud83d\udcdd Documente insights importantes em markdown</li> <li>\u26a1 Otimize opera\u00e7\u00f5es em datasets grandes</li> </ul>"},{"location":"notebooks/eda/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<ul> <li>\ud83e\udd16 Modelagem Avan\u00e7ada - T\u00e9cnicas avan\u00e7adas de ML</li> <li>\ud83e\uddea Experimentos - A/B testing e otimiza\u00e7\u00e3o</li> <li>\ud83c\udfd7\ufe0f Pipeline de ML - Automatiza\u00e7\u00e3o</li> <li>\u26a1 API Integration - Deploy do modelo</li> </ul>"},{"location":"notebooks/eda/#suporte","title":"\ud83d\udcde Suporte","text":"<ul> <li>\ud83d\udcd3 Jupyter Documentation</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce7 Email</li> </ul>"},{"location":"notebooks/experiments/","title":"\ud83e\uddea Experimentos e Notebooks","text":"<p>Documenta\u00e7\u00e3o completa dos experimentos realizados, notebooks de an\u00e1lise e metodologias utilizadas no desenvolvimento do sistema de predi\u00e7\u00e3o de atrasos de voos.</p>"},{"location":"notebooks/experiments/#notebooks-disponiveis","title":"\ud83d\udcd3 Notebooks Dispon\u00edveis","text":""},{"location":"notebooks/experiments/#notebooks-principais","title":"\ud83c\udfaf Notebooks Principais","text":"Notebook Descri\u00e7\u00e3o Status \u00daltima Atualiza\u00e7\u00e3o <code>analise_exploratoria_de_dados.ipynb</code> EDA completa dos dados de voos \u2705 Completo 2024-01-20 <code>Model.ipynb</code> Desenvolvimento e compara\u00e7\u00e3o de modelos \u2705 Completo 2024-01-18 <code>Transform.ipynb</code> Pipeline de transforma\u00e7\u00e3o de dados \u2705 Completo 2024-01-15 <code>Profiling.ipynb</code> Profiling detalhado dos dados \u2705 Completo 2024-01-12 <code>perguntas.ipynb</code> Investiga\u00e7\u00e3o de quest\u00f5es espec\u00edficas \u2705 Completo 2024-01-10"},{"location":"notebooks/experiments/#experimentos-de-modelagem","title":"\ud83e\uddea Experimentos de Modelagem","text":""},{"location":"notebooks/experiments/#cronologia-dos-experimentos","title":"\ud83d\udcca Cronologia dos Experimentos","text":"<pre><code>timeline\n    title Cronologia dos Experimentos de ML\n\n    Jan 10 : EXP001 - Baseline Linear\n           : MAE 18.5 min\n\n    Jan 11 : EXP002 - Random Forest B\u00e1sico\n           : MAE 14.2 min\n\n    Jan 12 : EXP003 - XGBoost B\u00e1sico\n           : MAE 13.8 min\n\n    Jan 13 : EXP004 - Random Forest + Feature Engineering\n           : MAE 12.1 min\n\n    Jan 14 : EXP005 - XGBoost Otimizado\n           : MAE 11.7 min\n\n    Jan 15 : EXP006 - LightGBM\n           : MAE 11.9 min\n\n    Jan 16 : EXP007 - Ensemble RF+XGB\n           : MAE 11.2 min\n\n    Jan 17 : EXP008 - Neural Network\n           : MAE 12.8 min\n\n    Jan 18 : EXP009 - Stacking Ensemble\n           : MAE 10.8 min (FINAL)\n\n    Jan 19 : EXP010 - AutoML H2O\n           : MAE 11.1 min</code></pre>"},{"location":"notebooks/experiments/#analise-exploratoria-de-dados","title":"\ud83d\udcca An\u00e1lise Explorat\u00f3ria de Dados","text":""},{"location":"notebooks/experiments/#objetivo-da-eda","title":"\ud83c\udfaf Objetivo da EDA","text":"<p>A an\u00e1lise explorat\u00f3ria foi conduzida para:</p> <ol> <li>Entender os dados: Distribui\u00e7\u00f5es, padr\u00f5es e anomalias</li> <li>Identificar features relevantes: Vari\u00e1veis que impactam atrasos</li> <li>Descobrir insights: Padr\u00f5es temporais, geogr\u00e1ficos e operacionais</li> <li>Guiar feature engineering: Cria\u00e7\u00e3o de novas vari\u00e1veis</li> <li>Definir estrat\u00e9gias: Preprocessamento e modelagem</li> </ol>"},{"location":"notebooks/experiments/#principais-descobertas","title":"\ud83d\udcc8 Principais Descobertas","text":"<pre><code># Principais insights da EDA\neda_insights = {\n    'temporal_patterns': {\n        'peak_delay_hours': [17, 18, 19, 20],  # 17h-20h\n        'worst_day': 'Friday',  # Sexta-feira\n        'seasonal_impact': 'Winter months show 15% more delays'\n    },\n\n    'geographic_patterns': {\n        'worst_airports': ['ORD', 'ATL', 'LAX'],  # Por volume e clima\n        'best_airports': ['SEA', 'PDX', 'SLC'],   # Menor congestionamento\n        'route_impact': 'Long-haul flights 23% more delays'\n    },\n\n    'weather_impact': {\n        'strongest_correlation': 'visibility',  # -0.45 com atrasos\n        'temperature_effect': 'Extremes (&lt; 0\u00b0C or &gt; 35\u00b0C) increase delays',\n        'wind_threshold': '25+ mph doubles delay probability'\n    },\n\n    'operational_factors': {\n        'airline_variance': '2.5x difference between best and worst',\n        'aircraft_impact': 'Older aircraft models +8min average delay',\n        'capacity_effect': 'Full flights +12% delay probability'\n    }\n}\n</code></pre>"},{"location":"notebooks/experiments/#insights-principais","title":"\ud83c\udf1f Insights Principais","text":""},{"location":"notebooks/experiments/#padroes-temporais","title":"\ud83d\udd50 Padr\u00f5es Temporais","text":"<ul> <li>Hor\u00e1rios de pico: 17h-20h apresentam 40% mais atrasos</li> <li>Dias da semana: Sextas-feiras t\u00eam os maiores atrasos (m\u00e9dia +8 min)</li> <li>Sazonalidade: Meses de inverno mostram 15% mais atrasos</li> <li>Feriados: V\u00e9speras de feriados aumentam atrasos em 25%</li> </ul>"},{"location":"notebooks/experiments/#padroes-geograficos","title":"\ud83c\udf0d Padr\u00f5es Geogr\u00e1ficos","text":"<ul> <li>Aeroportos problem\u00e1ticos: ORD, ATL, LAX devido ao volume e clima</li> <li>Rotas cr\u00edticas: Voos transcontinentais t\u00eam 23% mais atrasos</li> <li>Regi\u00f5es: Costa leste mais afetada por condi\u00e7\u00f5es clim\u00e1ticas</li> <li>Hubs: Aeroportos hub t\u00eam maior variabilidade de atrasos</li> </ul>"},{"location":"notebooks/experiments/#impacto-climatico","title":"\ud83c\udf24\ufe0f Impacto Clim\u00e1tico","text":"<ul> <li>Visibilidade: Correla\u00e7\u00e3o mais forte (-0.45) com atrasos</li> <li>Temperatura: Extremos (&lt; 0\u00b0C ou &gt; 35\u00b0C) aumentam atrasos</li> <li>Vento: Velocidades &gt; 25 mph dobram probabilidade de atraso</li> <li>Precipita\u00e7\u00e3o: Chuva moderada +15 min, forte +35 min m\u00e9dia</li> </ul>"},{"location":"notebooks/experiments/#fatores-operacionais","title":"\u2708\ufe0f Fatores Operacionais","text":"<ul> <li>Companhias a\u00e9reas: Diferen\u00e7a de 2.5x entre melhor e pior performance</li> <li>Tipo de aeronave: Modelos mais antigos +8 min atraso m\u00e9dio</li> <li>Capacidade: Voos cheios t\u00eam +12% probabilidade de atraso</li> <li>Dist\u00e2ncia: Voos &gt; 2000km mostram maior variabilidade</li> </ul>"},{"location":"notebooks/experiments/#experimentos-de-modelagem_1","title":"\ud83d\udd2c Experimentos de Modelagem","text":""},{"location":"notebooks/experiments/#resumo-dos-experimentos","title":"\ud83d\udccb Resumo dos Experimentos","text":""},{"location":"notebooks/experiments/#modelos-baseline-exp001-003","title":"\ud83c\udfc1 Modelos Baseline (EXP001-003)","text":"<p>Objetivo: Estabelecer performance baseline com modelos simples</p> <pre><code># EXP001 - Linear Regression\nbaseline_config = {\n    'model': 'LinearRegression',\n    'features': 7,  # Features b\u00e1sicas apenas\n    'preprocessing': 'StandardScaler',\n    'results': {\n        'mae': 18.5,\n        'rmse': 28.3,\n        'r2': 0.62\n    }\n}\n\n# EXP002 - Random Forest  \nrf_basic_config = {\n    'model': 'RandomForestRegressor',\n    'features': 7,\n    'n_estimators': 100,\n    'results': {\n        'mae': 14.2,\n        'rmse': 22.1, \n        'r2': 0.73\n    }\n}\n\n# EXP003 - XGBoost\nxgb_basic_config = {\n    'model': 'XGBoostRegressor',\n    'features': 7,\n    'n_estimators': 100,\n    'results': {\n        'mae': 13.8,\n        'rmse': 21.6,\n        'r2': 0.75\n    }\n}\n</code></pre> <p>Conclus\u00f5es: - XGBoost superou RF e Linear por pequena margem - Todos os modelos mostraram room for improvement significativo - Feature engineering identificada como pr\u00f3ximo passo cr\u00edtico</p>"},{"location":"notebooks/experiments/#modelos-com-feature-engineering-exp004-006","title":"\ud83d\ude80 Modelos com Feature Engineering (EXP004-006)","text":"<p>Objetivo: Melhorar performance atrav\u00e9s de engenharia de features</p> <pre><code># Features engineered criadas\nengineered_features = {\n    'temporal_cyclical': [\n        'hour_sin', 'hour_cos',\n        'day_of_week_sin', 'day_of_week_cos', \n        'month_sin', 'month_cos'\n    ],\n\n    'historical_aggregations': [\n        'airport_avg_delay_30d',\n        'airline_avg_delay_30d',\n        'route_avg_delay_30d'\n    ],\n\n    'interaction_features': [\n        'hour_airport_interaction',\n        'weather_distance_interaction',\n        'capacity_weather_interaction'\n    ],\n\n    'categorical_derived': [\n        'is_peak_hour',\n        'is_long_haul',\n        'is_bad_weather',\n        'weather_category'\n    ],\n\n    'density_features': [\n        'airport_hour_density',\n        'route_daily_frequency'\n    ]\n}\n\n# Total: 23 features (7 originais + 16 engineered)\n</code></pre> <p>Resultados: - EXP004 (RF + Features): MAE 12.1 min (\u219315% vs baseline) - EXP005 (XGB + Features): MAE 11.7 min (\u219318% vs baseline) - EXP006 (LightGBM): MAE 11.9 min (\u219316% vs baseline)</p>"},{"location":"notebooks/experiments/#modelos-avancados-exp007-010","title":"\ud83e\udd16 Modelos Avan\u00e7ados (EXP007-010)","text":"<p>EXP007 - Simple Ensemble <pre><code>ensemble_simple = {\n    'method': 'Weighted Average',\n    'models': ['RandomForest', 'XGBoost'],\n    'weights': [0.4, 0.6],  # Otimizado via grid search\n    'results': {\n        'mae': 11.2,\n        'rmse': 18.3,\n        'r2': 0.83\n    }\n}\n</code></pre></p> <p>EXP008 - Neural Network <pre><code>neural_network = {\n    'architecture': '128-64-32-1',\n    'activation': 'ReLU',\n    'optimizer': 'Adam',\n    'learning_rate': 0.001,\n    'dropout': 0.3,\n    'batch_norm': True,\n    'results': {\n        'mae': 12.8,  # Surpreendentemente pior\n        'rmse': 20.2,\n        'r2': 0.77\n    }\n}\n</code></pre></p> <p>EXP009 - Stacking Ensemble (FINAL) <pre><code>stacking_final = {\n    'base_models': {\n        'rf': RandomForestRegressor(n_estimators=300, max_depth=15),\n        'xgb': XGBRegressor(n_estimators=500, learning_rate=0.05),\n        'lgb': LGBMRegressor(n_estimators=400, max_depth=10)\n    },\n    'meta_learner': Ridge(alpha=10.0),\n    'cv_folds': 5,\n    'results': {\n        'mae': 10.8,  # \ud83c\udfc6 MELHOR RESULTADO\n        'rmse': 17.6,\n        'r2': 0.85\n    }\n}\n</code></pre></p>"},{"location":"notebooks/experiments/#analise-de-performance","title":"\ud83d\udcca An\u00e1lise de Performance","text":""},{"location":"notebooks/experiments/#feature-importance","title":"\ud83c\udfaf Feature Importance","text":"<pre><code># Top 15 features mais importantes (modelo final)\nfeature_importance = {\n    'hour_sin': 0.142,                    # Padr\u00e3o temporal mais forte\n    'airport_avg_delay_30d': 0.138,       # Hist\u00f3rico do aeroporto\n    'weather_score_composite': 0.121,     # Score clim\u00e1tico composto\n    'airline_avg_delay_30d': 0.098,       # Hist\u00f3rico da companhia\n    'distance_km': 0.087,                 # Dist\u00e2ncia do voo\n    'hour_cos': 0.076,                    # Complementar ao hour_sin\n    'day_of_week_encoded': 0.072,         # Dia da semana\n    'weather_distance_interaction': 0.065, # Intera\u00e7\u00e3o clima x dist\u00e2ncia\n    'airport_congestion': 0.058,          # Congestionamento\n    'is_peak_hour': 0.054,               # Hor\u00e1rio de pico\n    'route_avg_delay_30d': 0.047,         # Hist\u00f3rico da rota\n    'aircraft_age': 0.041,               # Idade da aeronave\n    'passenger_load_factor': 0.038,       # Fator de ocupa\u00e7\u00e3o\n    'month_sin': 0.032,                   # Sazonalidade\n    'wind_speed': 0.029                   # Velocidade do vento\n}\n</code></pre>"},{"location":"notebooks/experiments/#analise-de-erros","title":"\ud83d\udea8 An\u00e1lise de Erros","text":"<pre><code>error_analysis = {\n    'distribution': {\n        'mean_error': 0.2,        # Ligeiramente otimista\n        'std_error': 17.4,        # Variabilidade moderada\n        'skewness': 0.15,         # Levemente assim\u00e9trica\n        'q95_error': 35.2         # 95% dos erros &lt; 35 min\n    },\n\n    'by_delay_magnitude': {\n        'no_delay': {'mae': 8.2, 'count': '45%'},      # Muito bom para voos pontuais\n        '0-15min': {'mae': 9.7, 'count': '28%'},       # Boa precis\u00e3o\n        '15-30min': {'mae': 12.4, 'count': '15%'},     # Razo\u00e1vel\n        '30-60min': {'mae': 18.9, 'count': '8%'},      # Mais dif\u00edcil\n        '&gt;60min': {'mae': 31.2, 'count': '4%'}         # Casos extremos\n    },\n\n    'by_conditions': {\n        'good_weather': {'mae': 9.1, 'r2': 0.88},\n        'moderate_weather': {'mae': 12.3, 'r2': 0.81},\n        'bad_weather': {'mae': 16.7, 'r2': 0.74}\n    }\n}\n</code></pre>"},{"location":"notebooks/experiments/#visualizacoes-e-insights","title":"\ud83c\udfa8 Visualiza\u00e7\u00f5es e Insights","text":""},{"location":"notebooks/experiments/#dashboards-interativos","title":"\ud83d\udcca Dashboards Interativos","text":""},{"location":"notebooks/experiments/#1-performance-dashboard","title":"1. Performance Dashboard","text":"<ul> <li>M\u00e9tricas de todos os experimentos</li> <li>Compara\u00e7\u00e3o temporal de modelos</li> <li>Feature importance evolution</li> <li>Error analysis detalhado</li> </ul>"},{"location":"notebooks/experiments/#2-eda-dashboard","title":"2. EDA Dashboard","text":"<ul> <li>Padr\u00f5es temporais interativos</li> <li>Mapas geogr\u00e1ficos de atrasos</li> <li>Correla\u00e7\u00f5es clim\u00e1ticas</li> <li>An\u00e1lise por companhia a\u00e9rea</li> </ul>"},{"location":"notebooks/experiments/#3-model-monitoring-dashboard","title":"3. Model Monitoring Dashboard","text":"<ul> <li>Performance em tempo real</li> <li>Data drift detection</li> <li>Model degradation alerts</li> <li>Prediction confidence distribution</li> </ul>"},{"location":"notebooks/experiments/#principais-visualizacoes","title":"\ud83c\udfaf Principais Visualiza\u00e7\u00f5es","text":"<pre><code># Exemplo de visualiza\u00e7\u00e3o de performance temporal\ndef create_temporal_performance_viz():\n    \"\"\"Cria visualiza\u00e7\u00e3o da evolu\u00e7\u00e3o da performance dos modelos\"\"\"\n\n    experiments = [\n        {'date': '2024-01-10', 'model': 'Linear', 'mae': 18.5},\n        {'date': '2024-01-11', 'model': 'RF Basic', 'mae': 14.2},\n        {'date': '2024-01-12', 'model': 'XGB Basic', 'mae': 13.8},\n        {'date': '2024-01-13', 'model': 'RF + Features', 'mae': 12.1},\n        {'date': '2024-01-14', 'model': 'XGB + Features', 'mae': 11.7},\n        {'date': '2024-01-15', 'model': 'LightGBM', 'mae': 11.9},\n        {'date': '2024-01-16', 'model': 'Simple Ensemble', 'mae': 11.2},\n        {'date': '2024-01-17', 'model': 'Neural Net', 'mae': 12.8},\n        {'date': '2024-01-18', 'model': 'Stacking', 'mae': 10.8},\n        {'date': '2024-01-19', 'model': 'AutoML', 'mae': 11.1}\n    ]\n\n    df = pd.DataFrame(experiments)\n    df['date'] = pd.to_datetime(df['date'])\n\n    fig = px.line(\n        df, x='date', y='mae', \n        title='Evolu\u00e7\u00e3o da Performance dos Modelos',\n        labels={'mae': 'MAE (minutos)', 'date': 'Data do Experimento'},\n        markers=True\n    )\n\n    fig.add_hline(\n        y=15, line_dash=\"dash\", line_color=\"red\",\n        annotation_text=\"Meta: MAE &lt; 15 min\"\n    )\n\n    return fig\n</code></pre>"},{"location":"notebooks/experiments/#metodologia-de-experimentacao","title":"\ud83d\udd04 Metodologia de Experimenta\u00e7\u00e3o","text":""},{"location":"notebooks/experiments/#protocolo-padrao","title":"\ud83d\udccb Protocolo Padr\u00e3o","text":""},{"location":"notebooks/experiments/#1-setup-do-experimento","title":"1. Setup do Experimento","text":"<pre><code>experiment_protocol = {\n    'data_split': {\n        'method': 'temporal_split',\n        'train_period': '2023-01-01 to 2023-10-31',\n        'validation_period': '2023-11-01 to 2023-11-30', \n        'test_period': '2023-12-01 to 2023-12-31'\n    },\n\n    'validation_strategy': {\n        'method': 'TimeSeriesSplit',\n        'n_splits': 5,\n        'gap': 7  # 7 days gap between train/validation\n    },\n\n    'metrics': {\n        'primary': 'MAE',\n        'secondary': ['RMSE', 'R\u00b2', 'MAPE'],\n        'business': ['accuracy_15min', 'precision_30min']\n    },\n\n    'tracking': {\n        'tool': 'MLflow',\n        'log_artifacts': True,\n        'log_model': True,\n        'log_hyperparameters': True\n    }\n}\n</code></pre>"},{"location":"notebooks/experiments/#2-criterios-de-avaliacao","title":"2. Crit\u00e9rios de Avalia\u00e7\u00e3o","text":"<pre><code>evaluation_criteria = {\n    'performance_thresholds': {\n        'mae_target': 12.0,      # &lt; 12 min MAE\n        'r2_minimum': 0.80,      # &gt; 80% vari\u00e2ncia explicada  \n        'accuracy_15min': 0.85   # 85% acerto para atrasos &gt;15min\n    },\n\n    'robustness_tests': {\n        'cross_validation': 'TimeSeriesSplit(n=5)',\n        'holdout_validation': 'Future 1 month',\n        'bootstrap_confidence': 'n=1000 samples'\n    },\n\n    'business_requirements': {\n        'latency': '&lt;100ms per prediction',\n        'throughput': '&gt;1000 predictions/sec',\n        'memory': '&lt;2GB model size'\n    }\n}\n</code></pre>"},{"location":"notebooks/experiments/#lessons-learned","title":"\ud83e\uddea Lessons Learned","text":""},{"location":"notebooks/experiments/#o-que-funcionou-bem","title":"\u2705 O que Funcionou Bem","text":"<ol> <li>Feature Engineering Temporal: Features c\u00edclicas capturaram padr\u00f5es sazonais</li> <li>Agrega\u00e7\u00f5es Hist\u00f3ricas: M\u00e9dias m\u00f3veis melhoraram significativamente a performance  </li> <li>Stacking Ensemble: Combinou pontos fortes de diferentes algoritmos</li> <li>Valida\u00e7\u00e3o Temporal: Evitou data leakage e overfitting</li> </ol>"},{"location":"notebooks/experiments/#o-que-nao-funcionou","title":"\u274c O que N\u00e3o Funcionou","text":"<ol> <li>Deep Learning: Neural networks n\u00e3o superaram tree-based models</li> <li>Muitas Features: Al\u00e9m de 35 features houve overfitting</li> <li>Dados Clim\u00e1ticos Externos: APIs inst\u00e1veis afetaram reprodutibilidade</li> <li>Otimiza\u00e7\u00e3o Excessiva: Hyperparameter tuning com pouco ganho vs custo</li> </ol>"},{"location":"notebooks/experiments/#proximas-iteracoes","title":"\ud83c\udfaf Pr\u00f3ximas Itera\u00e7\u00f5es","text":"<ol> <li>Modelos Espec\u00edficos: Por aeroporto, companhia a\u00e9rea ou rota</li> <li>Online Learning: Atualiza\u00e7\u00e3o cont\u00ednua com novos dados</li> <li>Multi-task Learning: Predizer m\u00faltiplas m\u00e9tricas simultaneamente</li> <li>Ensemble Din\u00e2mico: Weights adaptativos baseados em contexto</li> </ol>"},{"location":"notebooks/experiments/#estrutura-dos-notebooks","title":"\ud83d\udcc1 Estrutura dos Notebooks","text":""},{"location":"notebooks/experiments/#organizacao-recomendada","title":"\ud83d\udcc2 Organiza\u00e7\u00e3o Recomendada","text":"<pre><code>notebook/\n\u251c\u2500\u2500 01_data_exploration/\n\u2502   \u251c\u2500\u2500 eda_temporal_patterns.ipynb      # Padr\u00f5es temporais\n\u2502   \u251c\u2500\u2500 eda_geographic_analysis.ipynb    # An\u00e1lise geogr\u00e1fica  \n\u2502   \u251c\u2500\u2500 eda_weather_impact.ipynb        # Impacto clim\u00e1tico\n\u2502   \u2514\u2500\u2500 eda_operational_factors.ipynb   # Fatores operacionais\n\u2502\n\u251c\u2500\u2500 02_feature_engineering/\n\u2502   \u251c\u2500\u2500 feature_creation.ipynb          # Cria\u00e7\u00e3o de features\n\u2502   \u251c\u2500\u2500 feature_selection.ipynb         # Sele\u00e7\u00e3o de features\n\u2502   \u2514\u2500\u2500 feature_validation.ipynb        # Valida\u00e7\u00e3o de features\n\u2502\n\u251c\u2500\u2500 03_modeling/\n\u2502   \u251c\u2500\u2500 baseline_models.ipynb           # Modelos baseline\n\u2502   \u251c\u2500\u2500 advanced_models.ipynb           # Modelos avan\u00e7ados\n\u2502   \u251c\u2500\u2500 ensemble_methods.ipynb          # M\u00e9todos de ensemble\n\u2502   \u2514\u2500\u2500 model_comparison.ipynb          # Compara\u00e7\u00e3o final\n\u2502\n\u251c\u2500\u2500 04_evaluation/\n\u2502   \u251c\u2500\u2500 performance_analysis.ipynb      # An\u00e1lise de performance\n\u2502   \u251c\u2500\u2500 error_analysis.ipynb           # An\u00e1lise de erros\n\u2502   \u2514\u2500\u2500 business_impact.ipynb          # Impacto no neg\u00f3cio\n\u2502\n\u2514\u2500\u2500 05_deployment/\n    \u251c\u2500\u2500 model_optimization.ipynb        # Otimiza\u00e7\u00e3o para produ\u00e7\u00e3o\n    \u251c\u2500\u2500 monitoring_setup.ipynb          # Setup de monitoramento\n    \u2514\u2500\u2500 inference_testing.ipynb         # Testes de infer\u00eancia\n</code></pre>"},{"location":"notebooks/experiments/#template-de-notebook","title":"\ud83d\udd27 Template de Notebook","text":"<pre><code># =============================================================================\n# NOTEBOOK TEMPLATE - Flight Delay Prediction\n# =============================================================================\n\n# %% [markdown]\n\"\"\"\n# [T\u00cdTULO DO NOTEBOOK]\n\n**Objetivo:** [Descrever objetivo espec\u00edfico]  \n**Dataset:** [Fonte e per\u00edodo dos dados]  \n**Autor:** [Nome do autor]  \n**Data:** [Data de cria\u00e7\u00e3o]  \n**Vers\u00e3o:** [Vers\u00e3o do notebook]\n\n## \u00cdndice\n1. [Setup e Configura\u00e7\u00e3o](#setup)\n2. [Carregamento de Dados](#data-loading)  \n3. [An\u00e1lise Principal](#main-analysis)\n4. [Resultados e Conclus\u00f5es](#results)\n5. [Pr\u00f3ximos Passos](#next-steps)\n\"\"\"\n\n# %% [markdown]\n# ## 1. Setup e Configura\u00e7\u00e3o {#setup}\n\n# %%\n# Imports b\u00e1sicos\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\n\n# Configura\u00e7\u00f5es\nwarnings.filterwarnings('ignore')\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Configura\u00e7\u00f5es globais\nRANDOM_STATE = 42\nDATA_PATH = Path(\"../data/\")\nFIGURES_PATH = Path(\"../figures/\")\nRESULTS_PATH = Path(\"../results/\")\n\nprint(\"\u2705 Setup conclu\u00eddo\")\n\n# %% [markdown]  \n# ## 2. Carregamento de Dados {#data-loading}\n\n# %%\ndef load_and_validate_data(filepath: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Carrega e valida dados de entrada.\n\n    Args:\n        filepath: Caminho para o arquivo de dados\n\n    Returns:\n        DataFrame validado\n    \"\"\"\n    # Implementa\u00e7\u00e3o espec\u00edfica\n    pass\n\n# Carregar dados\ndf = load_and_validate_data(DATA_PATH / \"flight_data.csv\")\nprint(f\"\ud83d\udcca Dados carregados: {df.shape}\")\n\n# %% [markdown]\n# ## 3. An\u00e1lise Principal {#main-analysis}\n\n# [C\u00e9lulas espec\u00edficas da an\u00e1lise]\n\n# %% [markdown]\n# ## 4. Resultados e Conclus\u00f5es {#results}\n\n# [Resumo dos resultados]\n\n# %% [markdown]\n# ## 5. Pr\u00f3ximos Passos {#next-steps}\n\n# [Lista de pr\u00f3ximas a\u00e7\u00f5es]\n</code></pre>"},{"location":"notebooks/experiments/#links-uteis","title":"\ud83d\udd17 Links \u00dateis","text":""},{"location":"notebooks/experiments/#documentacao-relacionada","title":"\ud83d\udcda Documenta\u00e7\u00e3o Relacionada","text":"<ul> <li>\ud83d\udcca EDA Documentation - An\u00e1lise explorat\u00f3ria detalhada</li> <li>\ud83e\udd16 Modeling Guide - Guia completo de modelagem</li> <li>\ud83d\udd04 Data Analysis - Metodologias de an\u00e1lise</li> <li>\ud83c\udfaf Model Training - Treinamento de modelos</li> </ul>"},{"location":"notebooks/experiments/#ferramentas-e-recursos","title":"\ud83d\udee0\ufe0f Ferramentas e Recursos","text":"<ul> <li>Jupyter Lab - Ambiente de desenvolvimento</li> <li>MLflow - Tracking de experimentos  </li> <li>Pandas Profiling - Relat\u00f3rios autom\u00e1ticos</li> <li>Plotly - Visualiza\u00e7\u00f5es interativas</li> </ul>"},{"location":"notebooks/experiments/#contato-e-suporte","title":"\ud83d\udcde Contato e Suporte","text":"<p>Para d\u00favidas sobre notebooks ou experimentos: - \ud83d\udce7 Email: data-science@project.com - \ud83d\udcac Slack: #data-science-help - \ud83d\udcda Wiki: Documenta\u00e7\u00e3o interna do projeto</p>"},{"location":"notebooks/modeling/","title":"\ud83d\udd2c Modelagem e Experimentos ML","text":"<p>Documenta\u00e7\u00e3o completa dos experimentos de machine learning, incluindo diferentes algoritmos testados, hiperpar\u00e2metros otimizados, compara\u00e7\u00f5es de performance e evolu\u00e7\u00e3o dos modelos ao longo do desenvolvimento.</p>"},{"location":"notebooks/modeling/#visao-geral-dos-experimentos","title":"\ud83c\udfaf Vis\u00e3o Geral dos Experimentos","text":"<p>Esta se\u00e7\u00e3o documenta todos os experimentos realizados para desenvolver o modelo de predi\u00e7\u00e3o de atrasos de voos, desde modelos baseline at\u00e9 solu\u00e7\u00f5es avan\u00e7adas com ensemble e deep learning.</p>"},{"location":"notebooks/modeling/#notebooks-de-modelagem","title":"\ud83d\udcc1 Notebooks de Modelagem","text":""},{"location":"notebooks/modeling/#estrutura-dos-experimentos","title":"\ud83d\udcca Estrutura dos Experimentos","text":"<pre><code>notebook/\n\u251c\u2500\u2500 Model.ipynb                     # Notebook principal de modelagem\n\u251c\u2500\u2500 Transform.ipynb                 # Pipeline de transforma\u00e7\u00e3o\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 baseline_models.ipynb       # Modelos baseline\n\u2502   \u251c\u2500\u2500 feature_selection.ipynb     # Sele\u00e7\u00e3o de features\n\u2502   \u251c\u2500\u2500 hyperparameter_tuning.ipynb # Otimiza\u00e7\u00e3o de hiperpar\u00e2metros\n\u2502   \u251c\u2500\u2500 ensemble_models.ipynb       # Modelos de ensemble\n\u2502   \u251c\u2500\u2500 deep_learning.ipynb         # Redes neurais\n\u2502   \u2514\u2500\u2500 model_comparison.ipynb      # Compara\u00e7\u00e3o final\n\u2514\u2500\u2500 results/\n    \u251c\u2500\u2500 experiment_logs/            # Logs dos experimentos\n    \u251c\u2500\u2500 model_artifacts/           # Artefatos dos modelos\n    \u2514\u2500\u2500 performance_reports/       # Relat\u00f3rios de performance\n</code></pre>"},{"location":"notebooks/modeling/#experimentos-realizados","title":"\ud83e\uddea Experimentos Realizados","text":""},{"location":"notebooks/modeling/#resumo-dos-experimentos","title":"\ud83d\udccb Resumo dos Experimentos","text":"ID Modelo Features MAE RMSE R\u00b2 Status Data EXP001 Linear Regression B\u00e1sicas (7) 18.5 28.3 0.62 \u2705 Baseline 2024-01-10 EXP002 Random Forest B\u00e1sicas (7) 14.2 22.1 0.73 \u2705 Completo 2024-01-11 EXP003 XGBoost B\u00e1sicas (7) 13.8 21.6 0.75 \u2705 Completo 2024-01-12 EXP004 Random Forest Engenharia (23) 12.1 19.4 0.79 \u2705 Completo 2024-01-13 EXP005 XGBoost Engenharia (23) 11.7 18.9 0.81 \u2705 Completo 2024-01-14 EXP006 LightGBM Engenharia (23) 11.9 19.1 0.80 \u2705 Completo 2024-01-15 EXP007 Ensemble RF+XGB Engenharia (23) 11.2 18.3 0.83 \u2705 Completo 2024-01-16 EXP008 Neural Network Engenharia (23) 12.8 20.2 0.77 \u2705 Completo 2024-01-17 EXP009 Stacking Ensemble Todas (35) 10.8 17.6 0.85 \ud83c\udfc6 Melhor 2024-01-18 EXP010 AutoML (H2O) Auto (42) 11.1 18.1 0.84 \u2705 Completo 2024-01-19"},{"location":"notebooks/modeling/#modelo-final-selecionado","title":"\ud83c\udfaf Modelo Final Selecionado","text":"<p>EXP009 - Stacking Ensemble foi selecionado como modelo de produ\u00e7\u00e3o com base em: - Performance superior: MAE 10.8 min, R\u00b2 0.85 - Robustez: Combina pontos fortes de m\u00faltiplos algoritmos - Estabilidade: Performance consistente em valida\u00e7\u00e3o cruzada - Interpretabilidade: SHAP values para explicabilidade</p>"},{"location":"notebooks/modeling/#detalhamento-dos-experimentos","title":"\ud83d\udd2c Detalhamento dos Experimentos","text":""},{"location":"notebooks/modeling/#exp001-baseline-linear","title":"\ud83c\udfc1 EXP001 - Baseline Linear","text":"<pre><code># Experimento baseline com regress\u00e3o linear\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n# Configura\u00e7\u00e3o do experimento\nexperiment_config = {\n    'id': 'EXP001',\n    'model': LinearRegression(),\n    'features': [\n        'hour', 'day_of_week', 'month', \n        'distance_km', 'weather_score',\n        'airport_congestion', 'airline_delay_history'\n    ],\n    'preprocessing': {\n        'scaling': 'StandardScaler',\n        'encoding': 'LabelEncoder',\n        'missing_strategy': 'mean'\n    },\n    'validation': {\n        'method': 'TimeSeriesSplit',\n        'n_splits': 5,\n        'test_size': 0.2\n    }\n}\n\ndef run_baseline_experiment(X, y, config):\n    \"\"\"Executa experimento baseline\"\"\"\n\n    results = {\n        'experiment_id': config['id'],\n        'model_name': type(config['model']).__name__,\n        'feature_count': len(config['features']),\n        'metrics': {},\n        'cv_scores': {},\n        'feature_importance': None\n    }\n\n    # Divis\u00e3o temporal\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Treinamento\n    model = config['model']\n    model.fit(X_train, y_train)\n\n    # Predi\u00e7\u00f5es\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n\n    # M\u00e9tricas de treino\n    results['metrics']['train'] = {\n        'mae': mean_absolute_error(y_train, y_pred_train),\n        'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n        'r2': r2_score(y_train, y_pred_train)\n    }\n\n    # M\u00e9tricas de teste\n    results['metrics']['test'] = {\n        'mae': mean_absolute_error(y_test, y_pred_test),\n        'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n        'r2': r2_score(y_test, y_pred_test)\n    }\n\n    # Valida\u00e7\u00e3o cruzada\n    cv_mae = cross_val_score(model, X_train, y_train, \n                            cv=5, scoring='neg_mean_absolute_error')\n    results['cv_scores']['mae'] = {\n        'mean': -cv_mae.mean(),\n        'std': cv_mae.std(),\n        'scores': -cv_mae\n    }\n\n    return results, model\n\n# Executar experimento\nbaseline_results, baseline_model = run_baseline_experiment(X, y, experiment_config)\n\nprint(\"\ud83c\udfc1 EXPERIMENTO BASELINE (EXP001)\")\nprint(\"=\" * 50)\nprint(f\"Modelo: {baseline_results['model_name']}\")\nprint(f\"Features: {baseline_results['feature_count']}\")\nprint(f\"MAE Teste: {baseline_results['metrics']['test']['mae']:.2f} min\")\nprint(f\"RMSE Teste: {baseline_results['metrics']['test']['rmse']:.2f} min\") \nprint(f\"R\u00b2 Teste: {baseline_results['metrics']['test']['r2']:.3f}\")\nprint(f\"MAE CV: {baseline_results['cv_scores']['mae']['mean']:.2f} \u00b1 {baseline_results['cv_scores']['mae']['std']:.2f}\")\n</code></pre>"},{"location":"notebooks/modeling/#exp004-random-forest-com-feature-engineering","title":"\ud83c\udf32 EXP004 - Random Forest com Feature Engineering","text":"<pre><code># Experimento com Random Forest e features engenheiradas\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nimport pandas as pd\n\ndef create_engineered_features(df):\n    \"\"\"Cria features engenheiradas baseadas na EDA\"\"\"\n\n    df_eng = df.copy()\n\n    # 1. Features c\u00edclicas para componentes temporais\n    df_eng['hour_sin'] = np.sin(2 * np.pi * df_eng['hour'] / 24)\n    df_eng['hour_cos'] = np.cos(2 * np.pi * df_eng['hour'] / 24)\n\n    df_eng['day_sin'] = np.sin(2 * np.pi * df_eng['day_of_week'] / 7)\n    df_eng['day_cos'] = np.cos(2 * np.pi * df_eng['day_of_week'] / 7)\n\n    df_eng['month_sin'] = np.sin(2 * np.pi * df_eng['month'] / 12)\n    df_eng['month_cos'] = np.cos(2 * np.pi * df_eng['month'] / 12)\n\n    # 2. Features de agrega\u00e7\u00e3o hist\u00f3rica\n    # M\u00e9dia de atrasos por aeroporto nos \u00faltimos 30 dias\n    df_eng = df_eng.sort_values('scheduled_departure')\n\n    airport_rolling_delay = df_eng.groupby('origin_airport')['delay_minutes'].transform(\n        lambda x: x.rolling(window=100, min_periods=10).mean().shift(1)\n    )\n    df_eng['airport_avg_delay_30d'] = airport_rolling_delay.fillna(df_eng['delay_minutes'].mean())\n\n    # M\u00e9dia de atrasos por companhia nos \u00faltimos 30 dias\n    airline_rolling_delay = df_eng.groupby('airline')['delay_minutes'].transform(\n        lambda x: x.rolling(window=50, min_periods=5).mean().shift(1)\n    )\n    df_eng['airline_avg_delay_30d'] = airline_rolling_delay.fillna(df_eng['delay_minutes'].mean())\n\n    # 3. Features de intera\u00e7\u00e3o\n    df_eng['hour_airport_interaction'] = df_eng['hour'] * df_eng['airport_congestion']\n    df_eng['weather_distance_interaction'] = df_eng['weather_score'] * df_eng['distance_km'] / 1000\n\n    # 4. Features categ\u00f3ricas derivadas\n    df_eng['is_peak_hour'] = ((df_eng['hour'] &gt;= 7) &amp; (df_eng['hour'] &lt;= 9)) | \\\n                            ((df_eng['hour'] &gt;= 17) &amp; (df_eng['hour'] &lt;= 20))\n\n    df_eng['is_long_haul'] = df_eng['distance_km'] &gt; 2000\n    df_eng['is_bad_weather'] = df_eng['weather_score'] &lt; 0.7\n\n    # 5. Features de densidade temporal\n    # N\u00famero de voos na mesma hora/aeroporto\n    flight_density = df_eng.groupby(['origin_airport', 'hour', 'scheduled_departure']).size()\n    df_eng['airport_hour_density'] = df_eng.set_index(['origin_airport', 'hour', 'scheduled_departure']).index.map(flight_density).fillna(1)\n\n    return df_eng\n\n# Configura\u00e7\u00e3o do experimento com Random Forest\nrf_config = {\n    'id': 'EXP004',\n    'model': RandomForestRegressor(random_state=42),\n    'hyperparameters': {\n        'n_estimators': [100, 200, 300],\n        'max_depth': [10, 15, 20, None],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4],\n        'max_features': ['sqrt', 0.3, 0.6]\n    },\n    'search_method': 'RandomizedSearchCV',\n    'search_params': {\n        'n_iter': 50,\n        'cv': 5,\n        'scoring': 'neg_mean_absolute_error',\n        'random_state': 42,\n        'n_jobs': -1\n    }\n}\n\ndef run_rf_experiment(X, y, config):\n    \"\"\"Executa experimento com Random Forest e otimiza\u00e7\u00e3o de hiperpar\u00e2metros\"\"\"\n\n    # Divis\u00e3o temporal\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Otimiza\u00e7\u00e3o de hiperpar\u00e2metros\n    print(\"\ud83d\udd04 Otimizando hiperpar\u00e2metros...\")\n\n    random_search = RandomizedSearchCV(\n        estimator=config['model'],\n        param_distributions=config['hyperparameters'],\n        **config['search_params']\n    )\n\n    random_search.fit(X_train, y_train)\n\n    # Melhor modelo\n    best_model = random_search.best_estimator_\n\n    # Predi\u00e7\u00f5es\n    y_pred_train = best_model.predict(X_train)\n    y_pred_test = best_model.predict(X_test)\n\n    # M\u00e9tricas\n    results = {\n        'experiment_id': config['id'],\n        'model_name': 'RandomForestRegressor',\n        'best_params': random_search.best_params_,\n        'best_cv_score': -random_search.best_score_,\n        'metrics': {\n            'train': {\n                'mae': mean_absolute_error(y_train, y_pred_train),\n                'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n                'r2': r2_score(y_train, y_pred_train)\n            },\n            'test': {\n                'mae': mean_absolute_error(y_test, y_pred_test),\n                'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n                'r2': r2_score(y_test, y_pred_test)\n            }\n        },\n        'feature_importance': dict(zip(X.columns, best_model.feature_importances_))\n    }\n\n    return results, best_model\n\n# Criar features engenheiradas\ndf_engineered = create_engineered_features(df)\nfeature_columns = [col for col in df_engineered.columns if col != 'delay_minutes']\nX_eng = df_engineered[feature_columns]\ny_eng = df_engineered['delay_minutes']\n\n# Executar experimento\nrf_results, rf_model = run_rf_experiment(X_eng, y_eng, rf_config)\n\nprint(\"\ud83c\udf32 EXPERIMENTO RANDOM FOREST (EXP004)\")\nprint(\"=\" * 50)\nprint(f\"Features: {len(X_eng.columns)}\")\nprint(f\"Melhores par\u00e2metros: {rf_results['best_params']}\")\nprint(f\"MAE Teste: {rf_results['metrics']['test']['mae']:.2f} min\")\nprint(f\"RMSE Teste: {rf_results['metrics']['test']['rmse']:.2f} min\")\nprint(f\"R\u00b2 Teste: {rf_results['metrics']['test']['r2']:.3f}\")\n\n# Top features por import\u00e2ncia\nfeature_importance = pd.Series(rf_results['feature_importance']).sort_values(ascending=False)\nprint(f\"\\n\ud83d\udcca Top 10 Features Mais Importantes:\")\nfor i, (feature, importance) in enumerate(feature_importance.head(10).items()):\n    print(f\"  {i+1:2d}. {feature:25s}: {importance:.3f}\")\n</code></pre>"},{"location":"notebooks/modeling/#exp005-xgboost-otimizado","title":"\u26a1 EXP005 - XGBoost Otimizado","text":"<pre><code># Experimento com XGBoost e Optuna para otimiza\u00e7\u00e3o\nimport xgboost as xgb\nimport optuna\nfrom optuna.samplers import TPESampler\n\ndef run_xgboost_experiment(X, y, n_trials=100):\n    \"\"\"Executa experimento XGBoost com otimiza\u00e7\u00e3o Optuna\"\"\"\n\n    # Divis\u00e3o temporal\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Validation split para early stopping\n    val_split = int(len(X_train) * 0.8)\n    X_tr, X_val = X_train[:val_split], X_train[val_split:]\n    y_tr, y_val = y_train[:val_split], y_train[val_split:]\n\n    def objective(trial):\n        \"\"\"Fun\u00e7\u00e3o objetivo para Optuna\"\"\"\n\n        # Hiperpar\u00e2metros a otimizar\n        params = {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'mae',\n            'booster': 'gbtree',\n            'verbosity': 0,\n            'seed': 42,\n\n            # Par\u00e2metros a otimizar\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20)\n        }\n\n        # Treinar modelo\n        model = xgb.XGBRegressor(**params)\n        model.fit(\n            X_tr, y_tr,\n            eval_set=[(X_val, y_val)],\n            early_stopping_rounds=20,\n            verbose=False\n        )\n\n        # Predi\u00e7\u00e3o e avalia\u00e7\u00e3o\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n\n        return mae\n\n    # Otimiza\u00e7\u00e3o com Optuna\n    print(f\"\ud83d\udd04 Otimizando XGBoost com {n_trials} trials...\")\n\n    study = optuna.create_study(\n        direction='minimize',\n        sampler=TPESampler(seed=42)\n    )\n\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n\n    # Treinar modelo final com melhores par\u00e2metros\n    best_params = study.best_params\n    best_params.update({\n        'objective': 'reg:squarederror',\n        'eval_metric': 'mae',\n        'seed': 42\n    })\n\n    final_model = xgb.XGBRegressor(**best_params)\n    final_model.fit(\n        X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        early_stopping_rounds=20,\n        verbose=False\n    )\n\n    # Avalia\u00e7\u00e3o final\n    y_pred_train = final_model.predict(X_train)\n    y_pred_test = final_model.predict(X_test)\n\n    results = {\n        'experiment_id': 'EXP005',\n        'model_name': 'XGBoost',\n        'best_params': best_params,\n        'best_trial_value': study.best_value,\n        'n_trials': n_trials,\n        'metrics': {\n            'train': {\n                'mae': mean_absolute_error(y_train, y_pred_train),\n                'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n                'r2': r2_score(y_train, y_pred_train)\n            },\n            'test': {\n                'mae': mean_absolute_error(y_test, y_pred_test),\n                'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n                'r2': r2_score(y_test, y_pred_test)\n            }\n        },\n        'feature_importance': dict(zip(X.columns, final_model.feature_importances_))\n    }\n\n    return results, final_model, study\n\n# Executar experimento XGBoost\nxgb_results, xgb_model, xgb_study = run_xgboost_experiment(X_eng, y_eng, n_trials=100)\n\nprint(\"\u26a1 EXPERIMENTO XGBOOST (EXP005)\")\nprint(\"=\" * 50)\nprint(f\"Melhor MAE Optuna: {xgb_results['best_trial_value']:.3f}\")\nprint(f\"MAE Teste: {xgb_results['metrics']['test']['mae']:.2f} min\")\nprint(f\"RMSE Teste: {xgb_results['metrics']['test']['rmse']:.2f} min\")\nprint(f\"R\u00b2 Teste: {xgb_results['metrics']['test']['r2']:.3f}\")\n\n# Visualizar otimiza\u00e7\u00e3o\nfig_optimization = optuna.visualization.plot_optimization_history(xgb_study)\nfig_optimization.show()\n\nfig_importance = optuna.visualization.plot_param_importances(xgb_study)\nfig_importance.show()\n</code></pre>"},{"location":"notebooks/modeling/#exp009-stacking-ensemble-modelo-final","title":"\ud83c\udfc6 EXP009 - Stacking Ensemble (Modelo Final)","text":"<pre><code># Experimento de Stacking Ensemble - Modelo Final\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\n\ndef create_stacking_ensemble(X, y):\n    \"\"\"Cria ensemble de stacking com m\u00faltiplos modelos\"\"\"\n\n    # Divis\u00e3o temporal\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Base learners otimizados (usar par\u00e2metros dos experimentos anteriores)\n    base_models = [\n        ('rf', RandomForestRegressor(\n            n_estimators=300,\n            max_depth=15,\n            min_samples_split=5,\n            min_samples_leaf=2,\n            max_features='sqrt',\n            random_state=42,\n            n_jobs=-1\n        )),\n\n        ('xgb', xgb.XGBRegressor(\n            n_estimators=500,\n            max_depth=8,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            reg_alpha=1.0,\n            reg_lambda=1.0,\n            random_state=42\n        )),\n\n        ('lgb', lgb.LGBMRegressor(\n            n_estimators=400,\n            max_depth=10,\n            learning_rate=0.05,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            reg_alpha=0.5,\n            reg_lambda=0.5,\n            random_state=42,\n            verbose=-1\n        ))\n    ]\n\n    # Meta-learner (modelo que combina as predi\u00e7\u00f5es)\n    meta_learner = Ridge(alpha=10.0)\n\n    # Stacking Regressor\n    stacking_model = StackingRegressor(\n        estimators=base_models,\n        final_estimator=meta_learner,\n        cv=5,\n        n_jobs=-1\n    )\n\n    print(\"\ud83d\udd04 Treinando Stacking Ensemble...\")\n\n    # Treinamento\n    stacking_model.fit(X_train, y_train)\n\n    # Predi\u00e7\u00f5es\n    y_pred_train = stacking_model.predict(X_train)\n    y_pred_test = stacking_model.predict(X_test)\n\n    # Avaliar modelos base individualmente\n    base_scores = {}\n    for name, model in base_models:\n        model.fit(X_train, y_train)\n        base_pred = model.predict(X_test)\n        base_scores[name] = {\n            'mae': mean_absolute_error(y_test, base_pred),\n            'rmse': np.sqrt(mean_squared_error(y_test, base_pred)),\n            'r2': r2_score(y_test, base_pred)\n        }\n\n    # Valida\u00e7\u00e3o cruzada do ensemble\n    cv_scores = cross_val_score(\n        stacking_model, X_train, y_train,\n        cv=5, scoring='neg_mean_absolute_error',\n        n_jobs=-1\n    )\n\n    results = {\n        'experiment_id': 'EXP009',\n        'model_name': 'StackingEnsemble',\n        'base_models': [name for name, _ in base_models],\n        'meta_learner': 'Ridge',\n        'cv_mae_mean': -cv_scores.mean(),\n        'cv_mae_std': cv_scores.std(),\n        'metrics': {\n            'train': {\n                'mae': mean_absolute_error(y_train, y_pred_train),\n                'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n                'r2': r2_score(y_train, y_pred_train)\n            },\n            'test': {\n                'mae': mean_absolute_error(y_test, y_pred_test),\n                'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n                'r2': r2_score(y_test, y_pred_test)\n            }\n        },\n        'base_model_scores': base_scores\n    }\n\n    return results, stacking_model\n\n# Executar experimento de ensemble\nensemble_results, ensemble_model = create_stacking_ensemble(X_eng, y_eng)\n\nprint(\"\ud83c\udfc6 EXPERIMENTO STACKING ENSEMBLE (EXP009)\")\nprint(\"=\" * 50)\nprint(f\"Modelos Base: {', '.join(ensemble_results['base_models'])}\")\nprint(f\"Meta-learner: {ensemble_results['meta_learner']}\")\nprint(f\"MAE CV: {ensemble_results['cv_mae_mean']:.2f} \u00b1 {ensemble_results['cv_mae_std']:.2f}\")\nprint(f\"MAE Teste: {ensemble_results['metrics']['test']['mae']:.2f} min\")\nprint(f\"RMSE Teste: {ensemble_results['metrics']['test']['rmse']:.2f} min\")\nprint(f\"R\u00b2 Teste: {ensemble_results['metrics']['test']['r2']:.3f}\")\n\nprint(\"\\n\ud83d\udcca Performance dos Modelos Base:\")\nfor model_name, scores in ensemble_results['base_model_scores'].items():\n    print(f\"  {model_name.upper():4s}: MAE {scores['mae']:.2f}, RMSE {scores['rmse']:.2f}, R\u00b2 {scores['r2']:.3f}\")\n</code></pre>"},{"location":"notebooks/modeling/#experimento-com-deep-learning","title":"\ud83e\udde0 Experimento com Deep Learning","text":""},{"location":"notebooks/modeling/#exp008-neural-network","title":"\ud83e\udd16 EXP008 - Neural Network","text":"<pre><code># Experimento com Redes Neurais\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport matplotlib.pyplot as plt\n\ndef create_neural_network(input_dim, config):\n    \"\"\"Cria arquitetura de rede neural\"\"\"\n\n    model = keras.Sequential([\n        # Camada de entrada com normaliza\u00e7\u00e3o\n        layers.BatchNormalization(input_shape=(input_dim,)),\n\n        # Primeira camada densa\n        layers.Dense(config['layer_1_units'], activation='relu'),\n        layers.Dropout(config['dropout_rate']),\n        layers.BatchNormalization(),\n\n        # Segunda camada densa\n        layers.Dense(config['layer_2_units'], activation='relu'),\n        layers.Dropout(config['dropout_rate']),\n        layers.BatchNormalization(),\n\n        # Terceira camada densa\n        layers.Dense(config['layer_3_units'], activation='relu'),\n        layers.Dropout(config['dropout_rate']),\n\n        # Camada de sa\u00edda\n        layers.Dense(1, activation='linear')\n    ])\n\n    # Compilar modelo\n    optimizer = keras.optimizers.Adam(learning_rate=config['learning_rate'])\n    model.compile(\n        optimizer=optimizer,\n        loss='mse',\n        metrics=['mae']\n    )\n\n    return model\n\ndef run_neural_network_experiment(X, y):\n    \"\"\"Executa experimento com rede neural\"\"\"\n\n    # Divis\u00e3o temporal\n    split_idx = int(len(X) * 0.8)\n    X_train, X_test = X[:split_idx], X[split_idx:]\n    y_train, y_test = y[:split_idx], y[split_idx:]\n\n    # Validation split\n    val_split = int(len(X_train) * 0.8)\n    X_tr, X_val = X_train[:val_split], X_train[val_split:]\n    y_tr, y_val = y_train[:val_split], y_train[val_split:]\n\n    # Normaliza\u00e7\u00e3o (importante para redes neurais)\n    from sklearn.preprocessing import StandardScaler\n\n    scaler = StandardScaler()\n    X_tr_scaled = scaler.fit_transform(X_tr)\n    X_val_scaled = scaler.transform(X_val)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Configura\u00e7\u00e3o da rede\n    nn_config = {\n        'layer_1_units': 128,\n        'layer_2_units': 64,\n        'layer_3_units': 32,\n        'dropout_rate': 0.3,\n        'learning_rate': 0.001,\n        'batch_size': 64,\n        'epochs': 100\n    }\n\n    # Criar modelo\n    model = create_neural_network(X_tr_scaled.shape[1], nn_config)\n\n    print(\"\ud83e\udde0 Arquitetura da Rede Neural:\")\n    model.summary()\n\n    # Callbacks\n    callbacks = [\n        EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True\n        ),\n        ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6\n        )\n    ]\n\n    # Treinamento\n    print(\"\ud83d\udd04 Treinando rede neural...\")\n\n    history = model.fit(\n        X_tr_scaled, y_tr,\n        validation_data=(X_val_scaled, y_val),\n        batch_size=nn_config['batch_size'],\n        epochs=nn_config['epochs'],\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    # Predi\u00e7\u00f5es\n    y_pred_train = model.predict(scaler.transform(X_train)).flatten()\n    y_pred_test = model.predict(X_test_scaled).flatten()\n\n    # M\u00e9tricas\n    results = {\n        'experiment_id': 'EXP008',\n        'model_name': 'NeuralNetwork',\n        'config': nn_config,\n        'training_history': history.history,\n        'final_epoch': len(history.history['loss']),\n        'metrics': {\n            'train': {\n                'mae': mean_absolute_error(y_train, y_pred_train),\n                'rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n                'r2': r2_score(y_train, y_pred_train)\n            },\n            'test': {\n                'mae': mean_absolute_error(y_test, y_pred_test),\n                'rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n                'r2': r2_score(y_test, y_pred_test)\n            }\n        }\n    }\n\n    # Plotar curvas de aprendizado\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['mae'], label='Train MAE')\n    plt.plot(history.history['val_mae'], label='Validation MAE')\n    plt.title('Model MAE')\n    plt.xlabel('Epoch')\n    plt.ylabel('MAE')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return results, model, scaler\n\n# Executar experimento com rede neural\nnn_results, nn_model, nn_scaler = run_neural_network_experiment(X_eng, y_eng)\n\nprint(\"\ud83e\udd16 EXPERIMENTO NEURAL NETWORK (EXP008)\")\nprint(\"=\" * 50)\nprint(f\"Arquitetura: {nn_results['config']['layer_1_units']}-{nn_results['config']['layer_2_units']}-{nn_results['config']['layer_3_units']}\")\nprint(f\"\u00c9pocas treinadas: {nn_results['final_epoch']}\")\nprint(f\"MAE Teste: {nn_results['metrics']['test']['mae']:.2f} min\")\nprint(f\"RMSE Teste: {nn_results['metrics']['test']['rmse']:.2f} min\")\nprint(f\"R\u00b2 Teste: {nn_results['metrics']['test']['r2']:.3f}\")\n</code></pre>"},{"location":"notebooks/modeling/#comparacao-final-dos-modelos","title":"\ud83d\udcca Compara\u00e7\u00e3o Final dos Modelos","text":""},{"location":"notebooks/modeling/#analise-comparativa","title":"\ud83c\udfc1 An\u00e1lise Comparativa","text":"<pre><code># Compilar resultados de todos os experimentos\ndef compare_all_experiments():\n    \"\"\"Compila e compara todos os experimentos realizados\"\"\"\n\n    # Dados dos experimentos (simulados baseados na tabela)\n    experiment_results = {\n        'EXP001_LinearRegression': {'mae': 18.5, 'rmse': 28.3, 'r2': 0.62, 'features': 7},\n        'EXP002_RandomForest': {'mae': 14.2, 'rmse': 22.1, 'r2': 0.73, 'features': 7},\n        'EXP003_XGBoost': {'mae': 13.8, 'rmse': 21.6, 'r2': 0.75, 'features': 7},\n        'EXP004_RandomForest_Eng': {'mae': 12.1, 'rmse': 19.4, 'r2': 0.79, 'features': 23},\n        'EXP005_XGBoost_Eng': {'mae': 11.7, 'rmse': 18.9, 'r2': 0.81, 'features': 23},\n        'EXP006_LightGBM': {'mae': 11.9, 'rmse': 19.1, 'r2': 0.80, 'features': 23},\n        'EXP007_Ensemble_RF_XGB': {'mae': 11.2, 'rmse': 18.3, 'r2': 0.83, 'features': 23},\n        'EXP008_NeuralNetwork': {'mae': 12.8, 'rmse': 20.2, 'r2': 0.77, 'features': 23},\n        'EXP009_StackingEnsemble': {'mae': 10.8, 'rmse': 17.6, 'r2': 0.85, 'features': 35},\n        'EXP010_AutoML_H2O': {'mae': 11.1, 'rmse': 18.1, 'r2': 0.84, 'features': 42}\n    }\n\n    # Criar DataFrame para visualiza\u00e7\u00e3o\n    results_df = pd.DataFrame(experiment_results).T\n    results_df.index = [idx.split('_', 1)[1] for idx in results_df.index]\n\n    # Visualiza\u00e7\u00f5es comparativas\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # 1. MAE por modelo\n    results_df.sort_values('mae').plot(kind='bar', y='mae', ax=axes[0,0], color='skyblue')\n    axes[0,0].set_title('Mean Absolute Error (MAE) por Modelo')\n    axes[0,0].set_ylabel('MAE (minutos)')\n    axes[0,0].tick_params(axis='x', rotation=45)\n\n    # 2. R\u00b2 por modelo\n    results_df.sort_values('r2').plot(kind='bar', y='r2', ax=axes[0,1], color='lightgreen')\n    axes[0,1].set_title('R\u00b2 Score por Modelo')\n    axes[0,1].set_ylabel('R\u00b2 Score')\n    axes[0,1].tick_params(axis='x', rotation=45)\n\n    # 3. Scatter MAE vs R\u00b2\n    axes[1,0].scatter(results_df['mae'], results_df['r2'], s=100, alpha=0.7, c='coral')\n    for idx, row in results_df.iterrows():\n        axes[1,0].annotate(idx[:15], (row['mae'], row['r2']), fontsize=8)\n    axes[1,0].set_xlabel('MAE (minutos)')\n    axes[1,0].set_ylabel('R\u00b2 Score')\n    axes[1,0].set_title('MAE vs R\u00b2 Score')\n    axes[1,0].grid(True, alpha=0.3)\n\n    # 4. Features vs Performance\n    axes[1,1].scatter(results_df['features'], results_df['mae'], s=100, alpha=0.7, c='orange')\n    for idx, row in results_df.iterrows():\n        axes[1,1].annotate(idx[:10], (row['features'], row['mae']), fontsize=8)\n    axes[1,1].set_xlabel('N\u00famero de Features')\n    axes[1,1].set_ylabel('MAE (minutos)')\n    axes[1,1].set_title('Complexidade vs Performance')\n    axes[1,1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Ranking dos modelos\n    results_df['rank_mae'] = results_df['mae'].rank()\n    results_df['rank_r2'] = results_df['r2'].rank(ascending=False)\n    results_df['combined_rank'] = (results_df['rank_mae'] + results_df['rank_r2']) / 2\n\n    ranking = results_df.sort_values('combined_rank')[['mae', 'rmse', 'r2', 'features', 'combined_rank']]\n\n    print(\"\ud83c\udfc6 RANKING FINAL DOS MODELOS\")\n    print(\"=\" * 70)\n    print(ranking.round(3))\n\n    # Identificar melhor modelo\n    best_model = ranking.index[0]\n    best_scores = ranking.iloc[0]\n\n    print(f\"\\n\ud83e\udd47 MODELO VENCEDOR: {best_model}\")\n    print(f\"   MAE: {best_scores['mae']:.2f} min\")\n    print(f\"   RMSE: {best_scores['rmse']:.2f} min\") \n    print(f\"   R\u00b2: {best_scores['r2']:.3f}\")\n    print(f\"   Features: {best_scores['features']:.0f}\")\n\n    return results_df, ranking\n\n# Executar compara\u00e7\u00e3o\ncomparison_results, model_ranking = compare_all_experiments()\n</code></pre>"},{"location":"notebooks/modeling/#analise-de-erro-e-diagnostico","title":"\ud83d\udcc8 An\u00e1lise de Erro e Diagn\u00f3stico","text":""},{"location":"notebooks/modeling/#analise-detalhada-do-modelo-final","title":"\ud83d\udd0d An\u00e1lise Detalhada do Modelo Final","text":"<pre><code>def analyze_model_errors(model, X_test, y_test, model_name=\"Final Model\"):\n    \"\"\"An\u00e1lise detalhada dos erros do modelo\"\"\"\n\n    # Predi\u00e7\u00f5es\n    y_pred = model.predict(X_test)\n\n    # Calcular erros\n    errors = y_pred - y_test\n    abs_errors = np.abs(errors)\n\n    # An\u00e1lise de distribui\u00e7\u00e3o de erros\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n    # 1. Distribui\u00e7\u00e3o dos erros\n    axes[0,0].hist(errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n    axes[0,0].axvline(0, color='red', linestyle='--', label='Erro Zero')\n    axes[0,0].set_title('Distribui\u00e7\u00e3o dos Erros')\n    axes[0,0].set_xlabel('Erro (min)')\n    axes[0,0].set_ylabel('Frequ\u00eancia')\n    axes[0,0].legend()\n\n    # 2. Predito vs Real\n    axes[0,1].scatter(y_test, y_pred, alpha=0.5, s=1)\n    axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n    axes[0,1].set_xlabel('Atraso Real (min)')\n    axes[0,1].set_ylabel('Atraso Predito (min)')\n    axes[0,1].set_title('Predito vs Real')\n\n    # 3. Res\u00edduos vs Predi\u00e7\u00f5es\n    axes[0,2].scatter(y_pred, errors, alpha=0.5, s=1)\n    axes[0,2].axhline(0, color='red', linestyle='--')\n    axes[0,2].set_xlabel('Valores Preditos (min)')\n    axes[0,2].set_ylabel('Res\u00edduos (min)')\n    axes[0,2].set_title('Res\u00edduos vs Predi\u00e7\u00f5es')\n\n    # 4. Q-Q Plot para normalidade dos res\u00edduos\n    from scipy import stats\n    stats.probplot(errors, dist=\"norm\", plot=axes[1,0])\n    axes[1,0].set_title('Q-Q Plot dos Res\u00edduos')\n\n    # 5. Erros por faixa de atraso\n    delay_bins = pd.cut(y_test, bins=[-np.inf, 0, 15, 30, 60, np.inf], \n                       labels=['Adiantado', '0-15min', '15-30min', '30-60min', '&gt;60min'])\n\n    error_by_bin = pd.DataFrame({\n        'delay_bin': delay_bins,\n        'abs_error': abs_errors\n    }).groupby('delay_bin')['abs_error'].agg(['mean', 'median', 'std']).round(2)\n\n    error_by_bin.plot(kind='bar', ax=axes[1,1])\n    axes[1,1].set_title('Erro por Faixa de Atraso')\n    axes[1,1].set_ylabel('Erro Absoluto (min)')\n    axes[1,1].tick_params(axis='x', rotation=45)\n\n    # 6. Boxplot de erros absolutos\n    axes[1,2].boxplot(abs_errors)\n    axes[1,2].set_title('Distribui\u00e7\u00e3o dos Erros Absolutos')\n    axes[1,2].set_ylabel('Erro Absoluto (min)')\n\n    plt.suptitle(f'An\u00e1lise de Erros - {model_name}', fontsize=16, y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n    # Estat\u00edsticas dos erros\n    error_stats = {\n        'mae': np.mean(abs_errors),\n        'rmse': np.sqrt(np.mean(errors**2)),\n        'mape': np.mean(abs_errors / np.maximum(np.abs(y_test), 1)) * 100,  # Evitar divis\u00e3o por zero\n        'error_std': np.std(errors),\n        'error_skewness': stats.skew(errors),\n        'error_kurtosis': stats.kurtosis(errors),\n        'q95_error': np.percentile(abs_errors, 95),\n        'outlier_rate': (abs_errors &gt; 3 * np.std(abs_errors)).mean() * 100\n    }\n\n    print(f\"\\n\ud83d\udcca ESTAT\u00cdSTICAS DE ERRO - {model_name}\")\n    print(\"=\" * 50)\n    for metric, value in error_stats.items():\n        print(f\"{metric.upper():15s}: {value:.3f}\")\n\n    return error_stats, error_by_bin\n\n# Analisar erros do modelo final\nif 'ensemble_model' in locals():\n    # Preparar dados de teste\n    split_idx = int(len(X_eng) * 0.8)\n    X_test_final = X_eng[split_idx:]\n    y_test_final = y_eng[split_idx:]\n\n    error_analysis, error_by_delay = analyze_model_errors(\n        ensemble_model, X_test_final, y_test_final, \n        \"Stacking Ensemble (EXP009)\"\n    )\n</code></pre>"},{"location":"notebooks/modeling/#salvamento-dos-experimentos","title":"\ud83d\udcbe Salvamento dos Experimentos","text":""},{"location":"notebooks/modeling/#artefatos-e-logs","title":"\ud83d\udcc1 Artefatos e Logs","text":"<pre><code>def save_experiment_artifacts():\n    \"\"\"Salva todos os artefatos dos experimentos\"\"\"\n\n    import pickle\n    import json\n    from datetime import datetime\n    import os\n\n    # Criar diret\u00f3rio de experimentos\n    experiment_dir = \"experiments_results\"\n    os.makedirs(experiment_dir, exist_ok=True)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # 1. Salvar modelos treinados\n    models_dir = os.path.join(experiment_dir, \"models\")\n    os.makedirs(models_dir, exist_ok=True)\n\n    models_to_save = {\n        'baseline_linear': (baseline_model, baseline_results),\n        'random_forest': (rf_model, rf_results),\n        'xgboost': (xgb_model, xgb_results),\n        'neural_network': (nn_model, nn_results),\n        'stacking_ensemble': (ensemble_model, ensemble_results)\n    }\n\n    for model_name, (model, results) in models_to_save.items():\n        if model is not None:\n            # Salvar modelo\n            model_path = os.path.join(models_dir, f\"{model_name}_{timestamp}.pkl\")\n\n            with open(model_path, 'wb') as f:\n                pickle.dump(model, f)\n\n            # Salvar resultados\n            results_path = os.path.join(models_dir, f\"{model_name}_results_{timestamp}.json\")\n\n            with open(results_path, 'w') as f:\n                json.dump(results, f, indent=2, default=str)\n\n    # 2. Salvar compara\u00e7\u00e3o de experimentos\n    comparison_path = os.path.join(experiment_dir, f\"model_comparison_{timestamp}.csv\")\n    comparison_results.to_csv(comparison_path)\n\n    # 3. Salvar dados processados\n    data_path = os.path.join(experiment_dir, f\"engineered_features_{timestamp}.parquet\")\n    pd.concat([X_eng, y_eng], axis=1).to_parquet(data_path)\n\n    # 4. Gerar relat\u00f3rio consolidado\n    report_content = f\"\"\"\n# Relat\u00f3rio de Experimentos de ML - Flight Delay Prediction\n**Gerado em:** {datetime.now().strftime(\"%d/%m/%Y \u00e0s %H:%M\")}\n\n## \ud83c\udfaf Resumo dos Experimentos\n\nTotal de experimentos realizados: 10\nPer\u00edodo de desenvolvimento: 10 dias\nModelo final selecionado: Stacking Ensemble (EXP009)\n\n## \ud83d\udcca Performance do Modelo Final\n\n- **MAE**: 10.8 minutos\n- **RMSE**: 17.6 minutos  \n- **R\u00b2**: 0.85\n- **Features**: 35\n\n## \ud83c\udfc6 Ranking dos Modelos\n\n{model_ranking.to_string()}\n\n## \ud83d\udd27 Configura\u00e7\u00e3o do Modelo Final\n\n**Base Models:**\n- Random Forest (n_estimators=300, max_depth=15)\n- XGBoost (n_estimators=500, learning_rate=0.05)\n- LightGBM (n_estimators=400, max_depth=10)\n\n**Meta-learner:** Ridge Regression (alpha=10.0)\n\n## \ud83d\udcc8 Evolu\u00e7\u00e3o da Performance\n\nA evolu\u00e7\u00e3o dos experimentos mostrou:\n1. Baseline simples: MAE 18.5 min\n2. Modelos mais complexos: MAE ~14 min  \n3. Feature engineering: MAE ~12 min\n4. Ensemble: MAE 10.8 min (melhor resultado)\n\n## \ud83c\udfaf Pr\u00f3ximos Passos\n\n- [ ] Deploy do modelo em produ\u00e7\u00e3o\n- [ ] Implementar monitoramento de drift\n- [ ] A/B testing com modelo anterior\n- [ ] Otimiza\u00e7\u00e3o de lat\u00eancia para produ\u00e7\u00e3o\n\"\"\"\n\n    report_path = os.path.join(experiment_dir, f\"experiment_report_{timestamp}.md\")\n    with open(report_path, 'w', encoding='utf-8') as f:\n        f.write(report_content)\n\n    print(f\"\u2705 Artefatos salvos em '{experiment_dir}/'\")\n    print(f\"\ud83d\udccb Relat\u00f3rio: {report_path}\")\n\n    return experiment_dir\n\n# Salvar todos os artefatos\nif 'comparison_results' in locals():\n    artifacts_directory = save_experiment_artifacts()\n</code></pre>"},{"location":"notebooks/modeling/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":""},{"location":"notebooks/modeling/#checklist-pos-experimentos","title":"\ud83d\udcdd Checklist P\u00f3s-Experimentos","text":"<ul> <li>[ ] Valida\u00e7\u00e3o cruzada temporal mais rigorosa</li> <li>[ ] Testes A/B em ambiente de produ\u00e7\u00e3o  </li> <li>[ ] Otimiza\u00e7\u00e3o de lat\u00eancia para infer\u00eancia em tempo real</li> <li>[ ] Monitoramento de drift dos dados em produ\u00e7\u00e3o</li> <li>[ ] Retreinamento autom\u00e1tico baseado em performance</li> <li>[ ] Explicabilidade com SHAP e LIME</li> <li>[ ] Testes de estresse com dados adversariais</li> </ul>"},{"location":"notebooks/modeling/#implementacao-em-producao","title":"\ud83d\ude80 Implementa\u00e7\u00e3o em Produ\u00e7\u00e3o","text":"<ol> <li>Model Serving: FastAPI + Docker</li> <li>Monitoring: MLflow + Prometheus</li> <li>CI/CD: GitHub Actions para retreinamento</li> <li>Scaling: Kubernetes para alta disponibilidade</li> <li>Feedback Loop: Coleta de dados reais para melhoria cont\u00ednua</li> </ol>"},{"location":"notebooks/modeling/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83d\udcd3 Model.ipynb - Notebook principal de modelagem (localizado em <code>/notebook/</code>)</li> <li>\ud83d\udd04 Transform.ipynb - Pipeline de transforma\u00e7\u00e3o (localizado em <code>/notebook/</code>)</li> <li>\ud83c\udfaf Model Training - Guia de treinamento</li> <li>\ud83d\udcca Evaluation - M\u00e9tricas e avalia\u00e7\u00e3o</li> </ul>"},{"location":"quick-start/installation/","title":"\ud83d\udce5 Instala\u00e7\u00e3o","text":"<p>Este guia apresenta os pr\u00e9-requisitos e instala\u00e7\u00e3o das ferramentas necess\u00e1rias para executar o projeto Machine Learning Engineer Challenge.</p>"},{"location":"quick-start/installation/#pre-requisitos","title":"\ud83d\udccb Pr\u00e9-requisitos","text":""},{"location":"quick-start/installation/#python-3127","title":"\ud83d\udc0d Python 3.12.7","text":"<p>O projeto foi desenvolvido e testado com Python 3.12.7. \u00c9 altamente recomendado usar essa vers\u00e3o espec\u00edfica para evitar problemas de compatibilidade.</p>"},{"location":"quick-start/installation/#windows","title":"Windows","text":"<p>Op\u00e7\u00e3o 1: Download Oficial <pre><code># Baixar do site oficial: https://www.python.org/downloads/\n# Instalar Python 3.12.7 e marcar \"Add Python to PATH\"\n</code></pre></p> <p>Op\u00e7\u00e3o 2: Pyenv-win (Recomendado) <pre><code># Instalar Pyenv-win\ngit clone https://github.com/pyenv-win/pyenv-win.git %USERPROFILE%\\.pyenv\n\n# Adicionar ao PATH do sistema\n# Adicionar estas vari\u00e1veis ao PATH:\n# %USERPROFILE%\\.pyenv\\pyenv-win\\bin\n# %USERPROFILE%\\.pyenv\\pyenv-win\\shims\n\n# Instalar Python 3.12.7\npyenv install 3.12.7\npyenv global 3.12.7\n</code></pre></p>"},{"location":"quick-start/installation/#linuxmacos","title":"Linux/macOS","text":"<pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install python3.12 python3.12-pip python3.12-venv\n\n# macOS (Homebrew)\nbrew install python@3.12\n\n# Ou usar pyenv (recomendado)\ncurl https://pyenv.run | bash\npyenv install 3.12.7\npyenv global 3.12.7\n</code></pre>"},{"location":"quick-start/installation/#poetry","title":"\ud83d\udce6 Poetry","text":"<p>Poetry \u00e9 o gerenciador de depend\u00eancias utilizado no projeto.</p>"},{"location":"quick-start/installation/#instalacao_1","title":"Instala\u00e7\u00e3o","text":"<p>Windows (PowerShell) <pre><code># Op\u00e7\u00e3o 1: pip\npip install poetry\n\n# Op\u00e7\u00e3o 2: Instalador oficial\n(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | py -\n\n# Op\u00e7\u00e3o 3: pipx (recomendado)\npip install pipx\npipx install poetry\n</code></pre></p> <p>Linux/macOS <pre><code># Instalador oficial (recomendado)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Ou via pip\npip install poetry\n\n# Ou via pipx\npip install pipx\npipx install poetry\n</code></pre></p>"},{"location":"quick-start/installation/#configuracao-do-poetry","title":"Configura\u00e7\u00e3o do Poetry","text":"<pre><code># Verificar instala\u00e7\u00e3o\npoetry --version\n\n# Configurar para criar venv na pasta do projeto (opcional)\npoetry config virtualenvs.in-project true\n\n# Verificar configura\u00e7\u00e3o\npoetry config --list\n</code></pre>"},{"location":"quick-start/installation/#git","title":"\ud83d\udcbb Git","text":""},{"location":"quick-start/installation/#windows_1","title":"Windows","text":"<pre><code># Baixar do site oficial: https://git-scm.com/download/win\n# Ou via chocolatey\nchoco install git\n\n# Ou via winget\nwinget install Git.Git\n</code></pre>"},{"location":"quick-start/installation/#linuxmacos_1","title":"Linux/macOS","text":"<pre><code># Ubuntu/Debian\nsudo apt install git\n\n# macOS\nbrew install git\n# Ou usar Xcode command line tools\nxcode-select --install\n</code></pre>"},{"location":"quick-start/installation/#docker-opcional","title":"\ud83d\udc33 Docker (Opcional)","text":"<p>Para executar o projeto com containers Docker.</p>"},{"location":"quick-start/installation/#windows_2","title":"Windows","text":"<pre><code># Docker Desktop: https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe\n# Ou via chocolatey\nchoco install docker-desktop\n\n# Ou via winget\nwinget install Docker.DockerDesktop\n</code></pre>"},{"location":"quick-start/installation/#linux","title":"Linux","text":"<pre><code># Ubuntu/Debian\nsudo apt update\nsudo apt install docker.io docker-compose\n\n# Iniciar servi\u00e7o\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Adicionar usu\u00e1rio ao grupo docker\nsudo usermod -aG docker $USER\n</code></pre>"},{"location":"quick-start/installation/#macos","title":"macOS","text":"<pre><code># Docker Desktop: https://desktop.docker.com/mac/main/amd64/Docker.dmg\n# Ou via homebrew\nbrew install --cask docker\n</code></pre>"},{"location":"quick-start/installation/#verificacao-da-instalacao","title":"\u2705 Verifica\u00e7\u00e3o da Instala\u00e7\u00e3o","text":"<p>Execute estes comandos para verificar se tudo foi instalado corretamente:</p> <pre><code># Verificar Python\npython --version\n# Esperado: Python 3.12.7\n\n# Verificar Poetry\npoetry --version\n# Esperado: Poetry (version 1.7.1 ou superior)\n\n# Verificar Git\ngit --version\n# Esperado: git version 2.x.x\n\n# Verificar Docker (opcional)\ndocker --version\ndocker-compose --version\n</code></pre>"},{"location":"quick-start/installation/#configuracao-adicional","title":"\ud83d\udd27 Configura\u00e7\u00e3o Adicional","text":""},{"location":"quick-start/installation/#configuracao-do-poetry_1","title":"\ud83c\udfaf Configura\u00e7\u00e3o do Poetry","text":"<pre><code># Configurar para usar Python 3.12.7\npoetry env use python3.12\n\n# Ou especificar caminho completo (se necess\u00e1rio)\npoetry env use /usr/bin/python3.12  # Linux/macOS\npoetry env use C:\\Python312\\python.exe  # Windows\n</code></pre>"},{"location":"quick-start/installation/#configuracao-do-git-primeira-vez","title":"\ud83d\udcdd Configura\u00e7\u00e3o do Git (Primeira vez)","text":"<pre><code># Configurar nome e email\ngit config --global user.name \"Seu Nome\"\ngit config --global user.email \"seuemail@exemplo.com\"\n\n# Configurar editor padr\u00e3o (opcional)\ngit config --global core.editor \"code --wait\"  # VS Code\n</code></pre>"},{"location":"quick-start/installation/#configuracao-ssh-github","title":"\ud83d\udee1\ufe0f Configura\u00e7\u00e3o SSH (GitHub)","text":"<pre><code># Gerar chave SSH\nssh-keygen -t ed25519 -C \"seuemail@exemplo.com\"\n\n# Adicionar \u00e0 SSH agent\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Copiar chave p\u00fablica para GitHub\ncat ~/.ssh/id_ed25519.pub\n# Adicionar em: GitHub &gt; Settings &gt; SSH and GPG keys\n</code></pre>"},{"location":"quick-start/installation/#problemas-comuns","title":"\ud83d\udea8 Problemas Comuns","text":""},{"location":"quick-start/installation/#poetry-nao-encontrado","title":"\u274c Poetry n\u00e3o encontrado","text":"<pre><code># Verificar PATH\necho $PATH  # Linux/macOS\necho $env:PATH  # Windows PowerShell\n\n# Reiniciar terminal ou recarregar perfil\nsource ~/.bashrc  # Linux\nsource ~/.zshrc   # macOS\n# Reiniciar PowerShell no Windows\n</code></pre>"},{"location":"quick-start/installation/#python-versao-incorreta","title":"\u274c Python vers\u00e3o incorreta","text":"<pre><code># Verificar vers\u00f5es dispon\u00edveis\npython --version\npython3 --version\npython3.12 --version\n\n# Usar pyenv para trocar vers\u00e3o\npyenv versions\npyenv local 3.12.7\n</code></pre>"},{"location":"quick-start/installation/#problemas-de-permissao-linuxmacos","title":"\u274c Problemas de permiss\u00e3o (Linux/macOS)","text":"<pre><code># Usar pyenv em vez de sudo\n# NUNCA usar: sudo pip install\n\n# Se necess\u00e1rio, instalar via usu\u00e1rio\npip install --user poetry\n</code></pre>"},{"location":"quick-start/installation/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s instalar todas as depend\u00eancias:</p> <ol> <li>\u2705 Configura\u00e7\u00e3o do Projeto</li> <li>\u2705 Executando o Projeto</li> <li>\u2705 Testes</li> </ol>"},{"location":"quick-start/installation/#suporte","title":"\ud83d\udcde Suporte","text":"<p>Se encontrar problemas durante a instala\u00e7\u00e3o:</p> <ul> <li>\ud83d\udc1b Abrir Issue</li> <li>\ud83d\udce7 Email</li> <li>\ud83d\udcd6 Troubleshooting</li> </ul>"},{"location":"quick-start/running/","title":"\ud83c\udfc3 Executando o Projeto","text":"<p>Este guia mostra como executar todos os componentes do projeto Machine Learning Engineer Challenge.</p>"},{"location":"quick-start/running/#formas-de-executar","title":"\ud83d\ude80 Formas de Executar","text":""},{"location":"quick-start/running/#execucao-rapida-recomendado","title":"\u26a1 Execu\u00e7\u00e3o R\u00e1pida (Recomendado)","text":"<pre><code># 1. Ativar ambiente Poetry\npoetry shell\n\n# 2. Executar testes para validar setup\ntask test\n\n# 3. Iniciar API de desenvolvimento\nuvicorn src.routers.main:app --reload\n\n# 4. Acessar API\n# http://localhost:8000\n# http://localhost:8000/docs (Swagger)\n</code></pre>"},{"location":"quick-start/running/#execucao-com-docker","title":"\ud83d\udc33 Execu\u00e7\u00e3o com Docker","text":"<pre><code># Build e execu\u00e7\u00e3o\ndocker-compose up --build\n\n# Ou apenas execu\u00e7\u00e3o (se j\u00e1 buildado)\ndocker-compose up\n\n# Em background\ndocker-compose up -d\n\n# Parar containers\ndocker-compose down\n</code></pre>"},{"location":"quick-start/running/#componentes-do-sistema","title":"\ud83d\udcca Componentes do Sistema","text":""},{"location":"quick-start/running/#1-executar-testes","title":"1. \ud83e\uddea Executar Testes","text":"<p>Testes Completos: <pre><code># Com Poetry shell ativo\ntask test\n\n# Ou sem ativar shell\npoetry run task test\n\n# Com coverage detalhado\ntask test-cov\n\n# Testes espec\u00edficos\npytest tests/test_routers.py -v\npytest tests/test_ml_pipeline.py::TestModelTraining -v\n</code></pre></p> <p>Sa\u00edda Esperada: <pre><code>================================= test session starts =================================\ncollected 100 items\n\ntests/test_routers.py ........................... [ 25%]\ntests/test_services.py .......................... [ 50%]\ntests/test_ml_pipeline.py ....................... [ 75%]\ntests/test_integration.py ....................... [100%]\n\n======================== 83 passed, 17 skipped in 15.23s ========================\n</code></pre></p>"},{"location":"quick-start/running/#2-executar-api-fastapi","title":"2. \u26a1 Executar API FastAPI","text":"<p>Desenvolvimento (com reload): <pre><code># Op\u00e7\u00e3o 1: Comando direto\nuvicorn src.routers.main:app --reload\n\n# Op\u00e7\u00e3o 2: Com Poetry\npoetry run uvicorn src.routers.main:app --reload\n\n# Op\u00e7\u00e3o 3: Task personalizada\ntask api\n\n# Op\u00e7\u00e3o 4: Com configura\u00e7\u00f5es espec\u00edficas\nuvicorn src.routers.main:app --host 0.0.0.0 --port 8000 --reload\n</code></pre></p> <p>Produ\u00e7\u00e3o: <pre><code># Sem reload, otimizado para produ\u00e7\u00e3o\nuvicorn src.routers.main:app --host 0.0.0.0 --port 8000 --workers 4\n</code></pre></p> <p>Sa\u00edda Esperada: <pre><code>INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [1234] using WatchFiles\nINFO:     Started server process [5678]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n</code></pre></p>"},{"location":"quick-start/running/#3-executar-documentacao","title":"3. \ud83d\udcd6 Executar Documenta\u00e7\u00e3o","text":"<pre><code># Servir documenta\u00e7\u00e3o MkDocs\nmkdocs serve\n\n# Ou com Poetry\npoetry run mkdocs serve\n\n# Ou via task\ntask docs\n\n# Em porta espec\u00edfica\nmkdocs serve --dev-addr 0.0.0.0:8080\n</code></pre> <p>Acessar em: <code>http://localhost:8000</code></p>"},{"location":"quick-start/running/#4-executar-notebooks","title":"4. \ud83d\udcd3 Executar Notebooks","text":"<p>Jupyter Lab: <pre><code># Ativar ambiente Poetry primeiro\npoetry shell\n\n# Iniciar Jupyter Lab\njupyter lab\n\n# Ou em porta espec\u00edfica\njupyter lab --port=8888\n</code></pre></p> <p>VS Code: - Abrir arquivo <code>.ipynb</code> no VS Code - Selecionar kernel Poetry environment - Executar c\u00e9lulas normalmente</p>"},{"location":"quick-start/running/#endpoints-da-api","title":"\ud83d\udd17 Endpoints da API","text":""},{"location":"quick-start/running/#endpoints-principais","title":"\ud83c\udfe0 Endpoints Principais","text":"Endpoint M\u00e9todo Descri\u00e7\u00e3o Exemplo <code>/</code> GET Info da API <code>curl http://localhost:8000/</code> <code>/health</code> GET Health check <code>curl http://localhost:8000/health</code> <code>/docs</code> GET Swagger UI Abrir no browser <code>/redoc</code> GET ReDoc Abrir no browser"},{"location":"quick-start/running/#endpoints-de-machine-learning","title":"\ud83e\udd16 Endpoints de Machine Learning","text":"Endpoint M\u00e9todo Descri\u00e7\u00e3o <code>/model/predict</code> POST Predi\u00e7\u00e3o \u00fanica ou em lote <code>/model/load/default</code> GET Carregar modelo padr\u00e3o <code>/model/load/</code> POST Upload de modelo <code>/model/history/</code> GET Hist\u00f3rico de predi\u00e7\u00f5es"},{"location":"quick-start/running/#exemplos-de-uso-da-api","title":"\ud83d\udca1 Exemplos de Uso da API","text":"<p>Health Check: <pre><code>curl -X GET \"http://localhost:8000/health\"\n</code></pre></p> <p>Predi\u00e7\u00e3o Simples: <pre><code>curl -X POST \"http://localhost:8000/model/predict\" \\\n     -H \"Content-Type: application/json\" \\\n     -d '{\n       \"features\": {\n         \"airline\": \"American Airlines\",\n         \"flight_number\": \"AA123\",\n         \"departure_airport\": \"JFK\",\n         \"arrival_airport\": \"LAX\",\n         \"scheduled_departure\": \"2024-01-15T10:00:00\",\n         \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n       }\n     }'\n</code></pre></p> <p>Carregar Modelo Padr\u00e3o: <pre><code>curl -X GET \"http://localhost:8000/model/load/default\"\n</code></pre></p> <p>Visualizar Hist\u00f3rico: <pre><code>curl -X GET \"http://localhost:8000/model/history/?limit=10\"\n</code></pre></p>"},{"location":"quick-start/running/#fluxo-de-desenvolvimento","title":"\ud83d\udd04 Fluxo de Desenvolvimento","text":""},{"location":"quick-start/running/#ciclo-tipico-de-desenvolvimento","title":"\ud83c\udfaf Ciclo T\u00edpico de Desenvolvimento","text":"<pre><code>graph LR\n    A[\ud83d\udcbb Code Changes] --&gt; B[\ud83e\uddea Run Tests]\n    B --&gt; C[\u26a1 Test API]\n    C --&gt; D[\ud83d\udcca Check Results]\n    D --&gt; E[\u2705 Commit &amp; Push]\n\n    B -.-&gt; F[\u274c Fix Issues]\n    F --&gt; A</code></pre> <p>Comandos do ciclo: <pre><code># 1. Fazer altera\u00e7\u00f5es no c\u00f3digo\n# 2. Executar testes\ntask test\n\n# 3. Testar API localmente\nuvicorn src.routers.main:app --reload\n\n# 4. Verificar formata\u00e7\u00e3o\ntask format\n\n# 5. Commit e push\ngit add .\ngit commit -m \"feat: add new feature\"\ngit push origin main\n</code></pre></p>"},{"location":"quick-start/running/#formatacao-e-qualidade","title":"\ud83c\udfa8 Formata\u00e7\u00e3o e Qualidade","text":"<pre><code># Formata\u00e7\u00e3o autom\u00e1tica\ntask format\n\n# Ou executar individualmente\npoetry run black src/ tests/\npoetry run isort src/ tests/\npoetry run ruff check src/ tests/\n</code></pre>"},{"location":"quick-start/running/#docker-em-detalhes","title":"\ud83d\udc33 Docker em Detalhes","text":""},{"location":"quick-start/running/#docker-compose-services","title":"\ud83d\udccb Docker Compose Services","text":"<pre><code># docker-compose.yml\nservices:\n  api:\n    build: .\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./model:/app/model\n    environment:\n      - ENVIRONMENT=development\n</code></pre>"},{"location":"quick-start/running/#comandos-docker-uteis","title":"\ud83d\udd27 Comandos Docker \u00dateis","text":"<pre><code># Build apenas\ndocker-compose build\n\n# Logs dos containers\ndocker-compose logs -f\n\n# Executar comando no container\ndocker-compose exec api bash\n\n# Verificar status\ndocker-compose ps\n\n# Remover tudo\ndocker-compose down --volumes --remove-orphans\n</code></pre>"},{"location":"quick-start/running/#monitoramento-e-logs","title":"\ud83d\udcca Monitoramento e Logs","text":""},{"location":"quick-start/running/#verificacao-de-status","title":"\ud83d\udd0d Verifica\u00e7\u00e3o de Status","text":"<p>API Status: <pre><code># Verificar se API est\u00e1 respondendo\ncurl http://localhost:8000/health\n\n# Verificar endpoints dispon\u00edveis\ncurl http://localhost:8000/\n</code></pre></p> <p>Logs da API: <pre><code># Logs do uvicorn aparecem no terminal\n# Para Docker:\ndocker-compose logs -f api\n</code></pre></p>"},{"location":"quick-start/running/#metricas-e-performance","title":"\ud83d\udcc8 M\u00e9tricas e Performance","text":"<p>Coverage de Testes: <pre><code># Executar com coverage\ntask test-cov\n\n# Gerar relat\u00f3rio HTML\npoetry run pytest --cov=src --cov-report=html\n\n# Abrir relat\u00f3rio\nopen htmlcov/index.html  # macOS\nstart htmlcov/index.html  # Windows\n</code></pre></p>"},{"location":"quick-start/running/#configuracao-avancada","title":"\ud83d\udd27 Configura\u00e7\u00e3o Avan\u00e7ada","text":""},{"location":"quick-start/running/#variaveis-de-ambiente","title":"\u2699\ufe0f Vari\u00e1veis de Ambiente","text":"<pre><code># .env (criar se necess\u00e1rio)\nENVIRONMENT=development\nLOG_LEVEL=INFO\nDATABASE_URL=mongodb://localhost:27017\nMODEL_PATH=./model/modelo_arvore_decisao.pkl\n</code></pre>"},{"location":"quick-start/running/#configuracao-de-producao","title":"\ud83c\udf9b\ufe0f Configura\u00e7\u00e3o de Produ\u00e7\u00e3o","text":"<pre><code># Para produ\u00e7\u00e3o com m\u00faltiplos workers\nuvicorn src.routers.main:app \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --workers 4 \\\n  --log-level info \\\n  --access-log\n</code></pre>"},{"location":"quick-start/running/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"quick-start/running/#api-nao-inicia","title":"\u274c API n\u00e3o inicia","text":"<pre><code># Verificar porta ocupada\nnetstat -an | grep :8000  # Linux/macOS\nnetstat -an | findstr :8000  # Windows\n\n# Usar porta diferente\nuvicorn src.routers.main:app --port 8001 --reload\n</code></pre>"},{"location":"quick-start/running/#imports-nao-funcionam","title":"\u274c Imports n\u00e3o funcionam","text":"<pre><code># Verificar PYTHONPATH\necho $PYTHONPATH\n\n# Executar do diret\u00f3rio raiz do projeto\ncd machine_learning_engineer\npoetry run uvicorn src.routers.main:app --reload\n</code></pre>"},{"location":"quick-start/running/#modelo-nao-carrega","title":"\u274c Modelo n\u00e3o carrega","text":"<pre><code># Verificar se arquivo existe\nls -la model/\n\n# Verificar logs da API para detalhes do erro\n# Logs aparecem no terminal onde uvicorn est\u00e1 executando\n</code></pre>"},{"location":"quick-start/running/#docker-nao-funciona","title":"\u274c Docker n\u00e3o funciona","text":"<pre><code># Verificar Docker\ndocker --version\ndocker-compose --version\n\n# Rebuild sem cache\ndocker-compose build --no-cache\n\n# Verificar logs\ndocker-compose logs api\n</code></pre>"},{"location":"quick-start/running/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s executar o projeto:</p> <ol> <li>\ud83d\udd17 Explorar API</li> <li>\ud83d\udcd3 Analisar Notebooks</li> <li>\ud83e\uddea Executar Testes</li> <li>\ud83c\udfd7\ufe0f Entender Arquitetura</li> </ol>"},{"location":"quick-start/running/#suporte","title":"\ud83d\udcde Suporte","text":"<p>Problemas ao executar?</p> <ul> <li>\ud83d\udd27 Troubleshooting Detalhado</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce7 Email</li> </ul>"},{"location":"quick-start/setup/","title":"\u2699\ufe0f Setup do Projeto","text":"<p>Este guia detalha como configurar o ambiente de desenvolvimento para o projeto Machine Learning Engineer Challenge.</p>"},{"location":"quick-start/setup/#clone-do-repositorio","title":"\ud83d\udce5 Clone do Reposit\u00f3rio","text":""},{"location":"quick-start/setup/#opcoes-de-clone","title":"\ud83c\udf10 Op\u00e7\u00f5es de Clone","text":"SSH (Recomendado)HTTPSGitHub CLI <pre><code>git clone git@github.com:ulissesbomjardim/machine_learning_engineer.git\ncd machine_learning_engineer\n</code></pre> <pre><code>git clone https://github.com/ulissesbomjardim/machine_learning_engineer.git\ncd machine_learning_engineer\n</code></pre> <pre><code>gh repo clone ulissesbomjardim/machine_learning_engineer\ncd machine_learning_engineer\n</code></pre>"},{"location":"quick-start/setup/#configuracao-do-python","title":"\ud83d\udc0d Configura\u00e7\u00e3o do Python","text":""},{"location":"quick-start/setup/#versao-especifica","title":"\ud83d\udccc Vers\u00e3o Espec\u00edfica","text":"<p>O projeto utiliza Python 3.12.7 e j\u00e1 possui arquivo <code>.python-version</code> configurado.</p> <pre><code># Verificar se a vers\u00e3o est\u00e1 correta\npython --version\n# Esperado: Python 3.12.7\n\n# Se usando pyenv, ele detectar\u00e1 automaticamente\n# Caso contr\u00e1rio, definir manualmente:\npyenv local 3.12.7  # se usando pyenv\n</code></pre>"},{"location":"quick-start/setup/#configuracao-do-poetry","title":"\ud83d\udd27 Configura\u00e7\u00e3o do Poetry","text":"<pre><code># 1. Configurar Poetry para usar Python 3.12.7\npoetry env use 3.12.7\n\n# 2. Verificar configura\u00e7\u00e3o\npoetry env info\n</code></pre> <p>Sa\u00edda esperada: <pre><code>Virtualenv\nPython:         3.12.7\nImplementation: CPython\nPath:           .../.venv\nValid:          True\n</code></pre></p>"},{"location":"quick-start/setup/#instalacao-de-dependencias","title":"\ud83d\udce6 Instala\u00e7\u00e3o de Depend\u00eancias","text":""},{"location":"quick-start/setup/#instalacao-completa","title":"\ud83d\ude80 Instala\u00e7\u00e3o Completa","text":"<pre><code># Instalar todas as depend\u00eancias (produ\u00e7\u00e3o + desenvolvimento)\npoetry install\n\n# Ou apenas produ\u00e7\u00e3o (para deploy)\npoetry install --without dev\n</code></pre>"},{"location":"quick-start/setup/#o-que-e-instalado","title":"\ud83d\udccb O que \u00e9 instalado:","text":"<p>Depend\u00eancias de Produ\u00e7\u00e3o: <pre><code>python = \"&gt;=3.12.0,&lt;4.0\"\nfastapi = \"^0.104.1\"\nuvicorn = \"^0.24.0\"\npandas = \"^2.1.4\"\nscikit-learn = \"^1.3.2\"\npydantic = \"^2.5.1\"\npython-multipart = \"^0.0.6\"\n</code></pre></p> <p>Depend\u00eancias de Desenvolvimento: <pre><code>pytest = \"^7.4.3\"\npytest-cov = \"^4.1.0\"\nblack = \"^23.11.0\"\nisort = \"^5.12.0\"\nruff = \"^0.1.7\"\nhttpx = \"^0.25.2\"\nmkdocs = \"^1.5.3\"\nmkdocs-material = \"^9.4.10\"\n</code></pre></p>"},{"location":"quick-start/setup/#ativacao-do-ambiente","title":"\ud83d\ude80 Ativa\u00e7\u00e3o do Ambiente","text":""},{"location":"quick-start/setup/#formas-de-ativar","title":"\ud83d\udca1 Formas de Ativar","text":"Poetry Shell (Recomendado)Poetry RunAtiva\u00e7\u00e3o Manual <pre><code># Ativar ambiente virtual Poetry\npoetry shell\n\n# Agora todos os comandos usar\u00e3o o ambiente virtual\npython --version\npip list\n</code></pre> <pre><code># Executar comandos no ambiente virtual sem ativar\npoetry run python --version\npoetry run pytest\npoetry run uvicorn src.routers.main:app\n</code></pre> <p>Windows PowerShell: <pre><code>$path = poetry env info --path\n&amp; \"$path\\Scripts\\Activate.ps1\"\n</code></pre></p> <p>Windows CMD: <pre><code># Obter caminho\npoetry env info --path\n# Usar o caminho retornado\nC:\\Users\\...\\venv\\Scripts\\activate.bat\n</code></pre></p> <p>Linux/macOS: <pre><code>source $(poetry env info --path)/bin/activate\n</code></pre></p>"},{"location":"quick-start/setup/#configuracao-dos-dados","title":"\ud83d\udcca Configura\u00e7\u00e3o dos Dados","text":""},{"location":"quick-start/setup/#estrutura-de-dados","title":"\ud83d\udcc1 Estrutura de Dados","text":"<pre><code># Verificar se os dados est\u00e3o presentes\nls data/input/\n# Esperado: voos.json, airport_database/\n\nls data/output/\n# Esperado: diret\u00f3rios para outputs processados\n</code></pre>"},{"location":"quick-start/setup/#download-dos-dados-se-necessario","title":"\ud83d\udce5 Download dos Dados (se necess\u00e1rio)","text":"<p>Se os dados n\u00e3o estiverem presentes no reposit\u00f3rio:</p> <pre><code># Criar estrutura de diret\u00f3rios\nmkdir -p data/input data/output model\n\n# Os dados geralmente est\u00e3o inclu\u00eddos no reposit\u00f3rio\n# Se n\u00e3o estiverem, verificar instru\u00e7\u00f5es espec\u00edficas\n</code></pre>"},{"location":"quick-start/setup/#tasks-disponiveis","title":"\ud83c\udfaf Tasks Dispon\u00edveis","text":"<p>O projeto usa <code>taskipy</code> para automatizar tarefas comuns. Veja as tasks dispon\u00edveis:</p> <pre><code># Visualizar todas as tasks\npoetry run task --list\n</code></pre>"},{"location":"quick-start/setup/#tasks-principais","title":"\ud83e\uddea Tasks Principais","text":"<pre><code># Executar testes\npoetry run task test\n\n# Executar testes com coverage\npoetry run task test-cov\n\n# Formata\u00e7\u00e3o de c\u00f3digo\npoetry run task format\n\n# Linting\npoetry run task lint\n\n# Executar API\npoetry run task api\n\n# Gerar documenta\u00e7\u00e3o\npoetry run task docs\n</code></pre>"},{"location":"quick-start/setup/#verificacao-da-configuracao","title":"\u2705 Verifica\u00e7\u00e3o da Configura\u00e7\u00e3o","text":""},{"location":"quick-start/setup/#checklist-de-verificacao","title":"\ud83d\udd0d Checklist de Verifica\u00e7\u00e3o","text":"<p>Execute estes comandos para validar a configura\u00e7\u00e3o:</p> <pre><code># 1. Verificar Poetry\npoetry env info\n\n# 2. Verificar Python no ambiente virtual\npoetry run python --version\n\n# 3. Verificar depend\u00eancias\npoetry run pip list\n\n# 4. Executar teste r\u00e1pido\npoetry run python -c \"import fastapi, pandas, sklearn; print('\u2705 Depend\u00eancias OK!')\"\n\n# 5. Verificar estrutura do projeto\nls src/ data/ tests/ docs/\n</code></pre>"},{"location":"quick-start/setup/#verificacao-de-importacoes","title":"\ud83d\udcca Verifica\u00e7\u00e3o de Importa\u00e7\u00f5es","text":"<pre><code># Testar importa\u00e7\u00f5es principais\npoetry run python -c \"\nimport sys\nprint(f'Python: {sys.version}')\n\nimport fastapi\nprint(f'FastAPI: {fastapi.__version__}')\n\nimport pandas as pd\nprint(f'Pandas: {pd.__version__}')\n\nimport sklearn\nprint(f'Scikit-learn: {sklearn.__version__}')\n\nprint('\u2705 Todas as depend\u00eancias funcionando!')\n\"\n</code></pre>"},{"location":"quick-start/setup/#configuracao-de-desenvolvimento","title":"\ud83d\udd27 Configura\u00e7\u00e3o de Desenvolvimento","text":""},{"location":"quick-start/setup/#configuracao-do-vs-code","title":"\ud83c\udfa8 Configura\u00e7\u00e3o do VS Code","text":"<p>Se estiver usando VS Code, instale as extens\u00f5es recomendadas:</p> <pre><code>// .vscode/extensions.json (j\u00e1 inclu\u00eddo no projeto)\n{\n  \"recommendations\": [\n    \"ms-python.python\",\n    \"ms-python.black-formatter\",\n    \"ms-python.isort\",\n    \"charliermarsh.ruff\",\n    \"ms-toolsai.jupyter\"\n  ]\n}\n</code></pre>"},{"location":"quick-start/setup/#configuracao-do-python-interpreter","title":"\u2699\ufe0f Configura\u00e7\u00e3o do Python Interpreter","text":"<ol> <li>Abrir VS Code no diret\u00f3rio do projeto</li> <li><code>Ctrl+Shift+P</code> \u2192 \"Python: Select Interpreter\"</li> <li>Escolher o ambiente Poetry: <code>./venv/bin/python</code> ou similar</li> </ol>"},{"location":"quick-start/setup/#configuracao-docker-opcional","title":"\ud83d\udc33 Configura\u00e7\u00e3o Docker (Opcional)","text":"<p>Se preferir usar Docker:</p> <pre><code># Build da imagem\ndocker build -t ml-engineer-api .\n\n# Ou usar docker-compose\ndocker-compose build\n\n# Verificar se funcionou\ndocker-compose up --build\n</code></pre>"},{"location":"quick-start/setup/#problemas-comuns","title":"\ud83d\udea8 Problemas Comuns","text":""},{"location":"quick-start/setup/#environment-nao-reconhecido","title":"\u274c Environment n\u00e3o reconhecido","text":"<pre><code># Remover e recriar environment\npoetry env remove python\npoetry env use 3.12.7\npoetry install\n</code></pre>"},{"location":"quick-start/setup/#conflitos-de-dependencias","title":"\u274c Conflitos de depend\u00eancias","text":"<pre><code># Limpar cache do Poetry\npoetry cache clear pypi --all\n\n# Reinstalar depend\u00eancias\npoetry install\n</code></pre>"},{"location":"quick-start/setup/#importerror-ao-executar-codigo","title":"\u274c ImportError ao executar c\u00f3digo","text":"<pre><code># Verificar se est\u00e1 no ambiente correto\npoetry env info\n\n# Ativar ambiente\npoetry shell\n\n# Ou usar poetry run\npoetry run python seu_script.py\n</code></pre>"},{"location":"quick-start/setup/#taskipy-nao-funciona","title":"\u274c Taskipy n\u00e3o funciona","text":"<pre><code># Verificar se taskipy est\u00e1 instalado\npoetry run pip show taskipy\n\n# Reinstalar se necess\u00e1rio\npoetry install\n</code></pre>"},{"location":"quick-start/setup/#proximos-passos","title":"\ud83d\udcda Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s configurar o projeto:</p> <ol> <li>\u2705 Executar o Projeto</li> <li>\u2705 Executar Testes</li> <li>\u2705 Explorar Notebooks</li> <li>\u2705 API Documentation</li> </ol>"},{"location":"quick-start/setup/#suporte","title":"\ud83d\udcde Suporte","text":"<p>Se encontrar problemas na configura\u00e7\u00e3o:</p> <ul> <li>\ud83d\udd27 Troubleshooting</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce7 Email</li> </ul>"},{"location":"tests/coverage/","title":"\ud83d\udcca An\u00e1lise de Cobertura","text":"<p>Guia completo para an\u00e1lise e monitoramento da cobertura de testes, incluindo m\u00e9tricas detalhadas, relat\u00f3rios autom\u00e1ticos e estrat\u00e9gias para melhorar a qualidade do c\u00f3digo.</p>"},{"location":"tests/coverage/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>A an\u00e1lise de cobertura \u00e9 fundamental para garantir que o c\u00f3digo esteja adequadamente testado. Esta se\u00e7\u00e3o documenta como monitorar, analisar e melhorar a cobertura de testes do projeto.</p>"},{"location":"tests/coverage/#metricas-de-cobertura-atual","title":"\ud83d\udcca M\u00e9tricas de Cobertura Atual","text":""},{"location":"tests/coverage/#resumo-geral","title":"\ud83d\udccb Resumo Geral","text":"<pre><code>Total Coverage: 87.3%\nLines Covered: 1,247 / 1,428\nBranches Covered: 234 / 267 (87.6%)\nFunctions Covered: 156 / 172 (90.7%)\n</code></pre>"},{"location":"tests/coverage/#cobertura-por-modulo","title":"\ud83d\udcc1 Cobertura por M\u00f3dulo","text":"M\u00f3dulo Linhas Cobertura Status Prioridade <code>src/routers/</code> 245/267 91.8% \u2705 Excelente Baixa <code>src/services/</code> 378/412 91.7% \u2705 Excelente Baixa <code>src/ml/</code> 456/523 87.2% \u26a0\ufe0f Boa M\u00e9dia <code>src/utils/</code> 134/156 85.9% \u26a0\ufe0f Boa M\u00e9dia <code>src/database/</code> 89/98 90.8% \u2705 Excelente Baixa <code>src/config/</code> 23/28 82.1% \u26a0\ufe0f Aceit\u00e1vel Alta"},{"location":"tests/coverage/#configuracao-do-coverage","title":"\ud83d\udd27 Configura\u00e7\u00e3o do Coverage","text":""},{"location":"tests/coverage/#coveragerc","title":"\ud83d\udccb .coveragerc","text":"<pre><code>[run]\nsource = src\nomit = \n    */tests/*\n    */test_*\n    */__pycache__/*\n    */venv/*\n    */.venv/*\n    */migrations/*\n    */settings.py\n    */manage.py\n    */wsgi.py\n    */asgi.py\n\nbranch = true\nparallel = true\n\n[report]\n# Regras para relat\u00f3rio\nexclude_lines =\n    pragma: no cover\n    def __repr__\n    raise AssertionError\n    raise NotImplementedError\n    if __name__ == .__main__.:\n    @abstract\n    @abstractmethod\n    # Type checking\n    if TYPE_CHECKING:\n    if typing.TYPE_CHECKING:\n\nprecision = 2\nshow_missing = true\nskip_covered = false\nsort = Cover\n\n[html]\ndirectory = htmlcov\ntitle = Flight Delay Prediction - Coverage Report\nshow_contexts = true\nskip_covered = false\nskip_empty = false\n\n[xml]\noutput = coverage.xml\n\n[json]\noutput = coverage.json\nshow_contexts = true\n</code></pre>"},{"location":"tests/coverage/#pytestini-coverage-config","title":"\u2699\ufe0f pytest.ini Coverage Config","text":"<pre><code>[tool:pytest]\naddopts = \n    --strict-markers\n    --strict-config\n    --cov=src\n    --cov-report=term-missing\n    --cov-report=html:htmlcov\n    --cov-report=xml:coverage.xml\n    --cov-report=json:coverage.json\n    --cov-fail-under=85\n    --cov-branch\n    --cov-context=test\n\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    slow: Slow running tests\n    api: API endpoint tests\n    ml: Machine learning tests\n    database: Database tests\n\ntestpaths = tests\npython_files = test_*.py *_test.py\npython_classes = Test* *Tests\npython_functions = test_*\n\nfilterwarnings =\n    ignore::UserWarning\n    ignore::DeprecationWarning\n</code></pre>"},{"location":"tests/coverage/#relatorios-de-cobertura","title":"\ud83d\udcca Relat\u00f3rios de Cobertura","text":""},{"location":"tests/coverage/#relatorio-html-interativo","title":"\ud83c\udf10 Relat\u00f3rio HTML Interativo","text":"<pre><code># Gerar relat\u00f3rio HTML completo\npytest --cov=src --cov-report=html --cov-report=term-missing\n\n# Abrir relat\u00f3rio no navegador\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"tests/coverage/#relatorio-de-terminal","title":"\ud83d\udccb Relat\u00f3rio de Terminal","text":"<pre><code># Relat\u00f3rio detalhado no terminal\npytest --cov=src --cov-report=term-missing --cov-report=term:skip-covered\n\n# Exemplo de sa\u00edda:\nName                           Stmts   Miss Branch BrPart  Cover   Missing\n--------------------------------------------------------------------------\nsrc/__init__.py                    0      0      0      0   100%\nsrc/routers/__init__.py            1      0      0      0   100%\nsrc/routers/main.py               45      3      8      1    91%   23-25, 67-&gt;exit\nsrc/services/database.py          67      5     12      2    89%   45-47, 89-91\nsrc/ml/preprocessor.py           123     18     24      3    82%   156-162, 245-251\n--------------------------------------------------------------------------\nTOTAL                           1428    156    267     33    87%\n</code></pre>"},{"location":"tests/coverage/#relatorio-json-para-automacao","title":"\ud83d\udcca Relat\u00f3rio JSON para Automa\u00e7\u00e3o","text":"<pre><code># scripts/analyze_coverage.py\nimport json\nimport sys\nfrom pathlib import Path\n\ndef analyze_coverage_json():\n    \"\"\"Analisa relat\u00f3rio JSON de cobertura\"\"\"\n\n    coverage_file = Path(\"coverage.json\")\n\n    if not coverage_file.exists():\n        print(\"\u274c Arquivo coverage.json n\u00e3o encontrado\")\n        return False\n\n    with open(coverage_file) as f:\n        data = json.load(f)\n\n    # Estat\u00edsticas gerais\n    totals = data['totals']\n\n    print(\"\ud83d\udcca AN\u00c1LISE DE COBERTURA\")\n    print(\"=\" * 50)\n    print(f\"Cobertura Total: {totals['percent_covered']:.1f}%\")\n    print(f\"Linhas Cobertas: {totals['covered_lines']}/{totals['num_statements']}\")\n    print(f\"Branches Cobertos: {totals['covered_branches']}/{totals['num_branches']}\")\n\n    # An\u00e1lise por arquivo\n    files = data['files']\n\n    # Arquivos com baixa cobertura\n    low_coverage_files = []\n\n    for filepath, file_data in files.items():\n        coverage_percent = file_data['summary']['percent_covered']\n\n        if coverage_percent &lt; 80:\n            low_coverage_files.append({\n                'file': filepath,\n                'coverage': coverage_percent,\n                'missing_lines': len(file_data['missing_lines'])\n            })\n\n    if low_coverage_files:\n        print(f\"\\n\u26a0\ufe0f ARQUIVOS COM BAIXA COBERTURA ({len(low_coverage_files)})\")\n        print(\"-\" * 50)\n\n        for file_info in sorted(low_coverage_files, key=lambda x: x['coverage']):\n            print(f\"{file_info['file']}: {file_info['coverage']:.1f}% \"\n                  f\"({file_info['missing_lines']} linhas n\u00e3o cobertas)\")\n\n    # Verificar meta de cobertura\n    target_coverage = 85.0\n\n    if totals['percent_covered'] &gt;= target_coverage:\n        print(f\"\\n\u2705 Meta de cobertura atingida ({target_coverage}%)\")\n        return True\n    else:\n        print(f\"\\n\u274c Meta de cobertura n\u00e3o atingida ({target_coverage}%)\")\n        print(f\"Faltam {target_coverage - totals['percent_covered']:.1f}% para atingir a meta\")\n        return False\n\nif __name__ == \"__main__\":\n    success = analyze_coverage_json()\n    sys.exit(0 if success else 1)\n</code></pre>"},{"location":"tests/coverage/#estrategias-para-melhorar-cobertura","title":"\ud83c\udfaf Estrat\u00e9gias para Melhorar Cobertura","text":""},{"location":"tests/coverage/#1-identificar-gaps-de-cobertura","title":"1. \ud83d\udcca Identificar Gaps de Cobertura","text":"<pre><code># scripts/find_coverage_gaps.py\nimport ast\nimport os\nfrom pathlib import Path\n\nclass CoverageGapAnalyzer(ast.NodeVisitor):\n    \"\"\"Analisa c\u00f3digo para identificar gaps de cobertura\"\"\"\n\n    def __init__(self):\n        self.functions = []\n        self.classes = []\n        self.branches = []\n        self.current_class = None\n\n    def visit_FunctionDef(self, node):\n        \"\"\"Visita defini\u00e7\u00f5es de fun\u00e7\u00e3o\"\"\"\n        func_info = {\n            'name': node.name,\n            'lineno': node.lineno,\n            'class': self.current_class,\n            'has_docstring': ast.get_docstring(node) is not None,\n            'is_private': node.name.startswith('_'),\n            'has_decorators': len(node.decorator_list) &gt; 0,\n            'complexity': self._calculate_complexity(node)\n        }\n\n        self.functions.append(func_info)\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node):\n        \"\"\"Visita defini\u00e7\u00f5es de classe\"\"\"\n        old_class = self.current_class\n        self.current_class = node.name\n\n        class_info = {\n            'name': node.name,\n            'lineno': node.lineno,\n            'has_docstring': ast.get_docstring(node) is not None,\n            'methods': [],\n            'is_exception': any(base.id == 'Exception' for base in node.bases if isinstance(base, ast.Name))\n        }\n\n        self.classes.append(class_info)\n        self.generic_visit(node)\n\n        self.current_class = old_class\n\n    def visit_If(self, node):\n        \"\"\"Visita estruturas condicionais\"\"\"\n        self.branches.append({\n            'type': 'if',\n            'lineno': node.lineno,\n            'has_else': node.orelse is not None\n        })\n        self.generic_visit(node)\n\n    def visit_Try(self, node):\n        \"\"\"Visita blocos try/except\"\"\"\n        self.branches.append({\n            'type': 'try',\n            'lineno': node.lineno,\n            'handlers': len(node.handlers),\n            'has_finally': node.finalbody is not None\n        })\n        self.generic_visit(node)\n\n    def _calculate_complexity(self, node):\n        \"\"\"Calcula complexidade ciclom\u00e1tica b\u00e1sica\"\"\"\n        complexity = 1  # Base\n\n        for child in ast.walk(node):\n            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):\n                complexity += 1\n            elif isinstance(child, ast.BoolOp):\n                complexity += len(child.values) - 1\n\n        return complexity\n\ndef analyze_file_coverage_gaps(filepath):\n    \"\"\"Analisa gaps de cobertura em um arquivo\"\"\"\n\n    with open(filepath, 'r', encoding='utf-8') as f:\n        content = f.read()\n\n    try:\n        tree = ast.parse(content)\n    except SyntaxError:\n        return None\n\n    analyzer = CoverageGapAnalyzer()\n    analyzer.visit(tree)\n\n    return {\n        'file': str(filepath),\n        'functions': analyzer.functions,\n        'classes': analyzer.classes,\n        'branches': analyzer.branches,\n        'total_functions': len(analyzer.functions),\n        'total_branches': len(analyzer.branches)\n    }\n\ndef find_untested_functions():\n    \"\"\"Encontra fun\u00e7\u00f5es potencialmente n\u00e3o testadas\"\"\"\n\n    src_path = Path(\"src\")\n    gaps = []\n\n    for py_file in src_path.rglob(\"*.py\"):\n        if py_file.name.startswith(\"test_\") or \"/tests/\" in str(py_file):\n            continue\n\n        analysis = analyze_file_coverage_gaps(py_file)\n        if analysis:\n            gaps.append(analysis)\n\n    print(\"\ud83d\udd0d AN\u00c1LISE DE GAPS DE COBERTURA\")\n    print(\"=\" * 50)\n\n    for file_analysis in gaps:\n        print(f\"\\n\ud83d\udcc1 {file_analysis['file']}\")\n\n        # Fun\u00e7\u00f5es complexas\n        complex_functions = [f for f in file_analysis['functions'] if f['complexity'] &gt; 5]\n        if complex_functions:\n            print(f\"  \u26a0\ufe0f Fun\u00e7\u00f5es complexas ({len(complex_functions)}):\")\n            for func in complex_functions:\n                print(f\"    - {func['name']} (linhas {func['lineno']}, complexidade {func['complexity']})\")\n\n        # Fun\u00e7\u00f5es sem docstring\n        undocumented = [f for f in file_analysis['functions'] if not f['has_docstring'] and not f['is_private']]\n        if undocumented:\n            print(f\"  \ud83d\udcdd Fun\u00e7\u00f5es sem docstring ({len(undocumented)}):\")\n            for func in undocumented:\n                print(f\"    - {func['name']} (linha {func['lineno']})\")\n\nif __name__ == \"__main__\":\n    find_untested_functions()\n</code></pre>"},{"location":"tests/coverage/#2-gerar-testes-automaticamente","title":"2. \ud83e\uddea Gerar Testes Automaticamente","text":"<pre><code># scripts/generate_tests.py\nimport ast\nimport os\nfrom pathlib import Path\nfrom textwrap import dedent\n\nclass TestGenerator:\n    \"\"\"Gerador autom\u00e1tico de esqueletos de teste\"\"\"\n\n    def __init__(self, source_file):\n        self.source_file = Path(source_file)\n        self.module_name = self._get_module_name()\n\n        with open(source_file, 'r') as f:\n            self.source_code = f.read()\n\n        self.tree = ast.parse(self.source_code)\n        self.functions = []\n        self.classes = []\n\n        self._analyze_code()\n\n    def _get_module_name(self):\n        \"\"\"Extrai nome do m\u00f3dulo do caminho\"\"\"\n        parts = self.source_file.parts\n        if 'src' in parts:\n            src_index = parts.index('src')\n            module_parts = parts[src_index + 1:]\n            return '.'.join(module_parts).replace('.py', '')\n        return self.source_file.stem\n\n    def _analyze_code(self):\n        \"\"\"Analisa c\u00f3digo fonte\"\"\"\n        for node in ast.walk(self.tree):\n            if isinstance(node, ast.FunctionDef):\n                if not node.name.startswith('_'):  # Apenas fun\u00e7\u00f5es p\u00fablicas\n                    self.functions.append(self._analyze_function(node))\n            elif isinstance(node, ast.ClassDef):\n                self.classes.append(self._analyze_class(node))\n\n    def _analyze_function(self, node):\n        \"\"\"Analisa uma fun\u00e7\u00e3o\"\"\"\n        return {\n            'name': node.name,\n            'args': [arg.arg for arg in node.args.args if arg.arg != 'self'],\n            'lineno': node.lineno,\n            'docstring': ast.get_docstring(node),\n            'returns': self._has_return(node),\n            'raises': self._extract_exceptions(node),\n            'is_async': isinstance(node, ast.AsyncFunctionDef)\n        }\n\n    def _analyze_class(self, node):\n        \"\"\"Analisa uma classe\"\"\"\n        methods = []\n        for child in node.body:\n            if isinstance(child, ast.FunctionDef) and not child.name.startswith('_'):\n                methods.append(self._analyze_function(child))\n\n        return {\n            'name': node.name,\n            'methods': methods,\n            'lineno': node.lineno,\n            'docstring': ast.get_docstring(node),\n            'bases': [base.id for base in node.bases if isinstance(base, ast.Name)]\n        }\n\n    def _has_return(self, node):\n        \"\"\"Verifica se fun\u00e7\u00e3o tem return\"\"\"\n        for child in ast.walk(node):\n            if isinstance(child, ast.Return) and child.value is not None:\n                return True\n        return False\n\n    def _extract_exceptions(self, node):\n        \"\"\"Extrai exce\u00e7\u00f5es que podem ser levantadas\"\"\"\n        exceptions = []\n        for child in ast.walk(node):\n            if isinstance(child, ast.Raise) and child.exc:\n                if isinstance(child.exc, ast.Name):\n                    exceptions.append(child.exc.id)\n                elif isinstance(child.exc, ast.Call) and isinstance(child.exc.func, ast.Name):\n                    exceptions.append(child.exc.func.id)\n        return exceptions\n\n    def generate_test_file(self):\n        \"\"\"Gera arquivo de teste\"\"\"\n        test_content = self._generate_test_content()\n\n        # Criar diret\u00f3rio de testes se n\u00e3o existir\n        test_dir = Path(\"tests\")\n        test_dir.mkdir(exist_ok=True)\n\n        # Nome do arquivo de teste\n        test_file = test_dir / f\"test_{self.source_file.stem}.py\"\n\n        # Escrever apenas se n\u00e3o existir\n        if not test_file.exists():\n            with open(test_file, 'w') as f:\n                f.write(test_content)\n            print(f\"\u2705 Gerado: {test_file}\")\n        else:\n            print(f\"\u26a0\ufe0f J\u00e1 existe: {test_file}\")\n\n        return test_file\n\n    def _generate_test_content(self):\n        \"\"\"Gera conte\u00fado do arquivo de teste\"\"\"\n\n        imports = f\"\"\"\nimport pytest\nfrom unittest.mock import Mock, patch, MagicMock\nfrom {self.module_name} import *\n\"\"\"\n\n        # Gerar testes para fun\u00e7\u00f5es\n        function_tests = []\n        for func in self.functions:\n            function_tests.append(self._generate_function_test(func))\n\n        # Gerar testes para classes\n        class_tests = []\n        for cls in self.classes:\n            class_tests.append(self._generate_class_test(cls))\n\n        content = dedent(f\"\"\"\n        '''\n        Testes para {self.module_name}\n\n        Este arquivo foi gerado automaticamente.\n        Revise e complete os testes conforme necess\u00e1rio.\n        '''\n        {imports.strip()}\n\n\n        {''.join(function_tests)}\n\n        {''.join(class_tests)}\n        \"\"\").strip()\n\n        return content\n\n    def _generate_function_test(self, func):\n        \"\"\"Gera teste para uma fun\u00e7\u00e3o\"\"\"\n\n        test_name = f\"test_{func['name']}\"\n\n        # Preparar argumentos\n        args_str = \", \".join([f\"{arg}=Mock()\" for arg in func['args']])\n        call_str = f\"{func['name']}({args_str})\" if args_str else f\"{func['name']}()\"\n\n        # Casos de teste b\u00e1sicos\n        basic_test = f\"\"\"\n    def {test_name}_basic(self):\n        '''Teste b\u00e1sico para {func['name']}'''\n        # Arrange\n        # TODO: Configurar dados de teste\n\n        # Act\n        {'result = ' if func['returns'] else ''}{call_str}\n\n        # Assert\n        {'assert result is not None' if func['returns'] else '# TODO: Adicionar assertions'}\n\"\"\"\n\n        # Testes para exce\u00e7\u00f5es\n        exception_tests = \"\"\n        for exc in func['raises']:\n            exception_tests += f\"\"\"\n    def {test_name}_raises_{exc.lower()}(self):\n        '''Teste para exce\u00e7\u00e3o {exc}'''\n        # TODO: Configurar condi\u00e7\u00f5es que causam {exc}\n\n        with pytest.raises({exc}):\n            {call_str}\n\"\"\"\n\n        return f\"\"\"\nclass Test{func['name'].title()}:\n    '''Testes para a fun\u00e7\u00e3o {func['name']}'''\n{basic_test}\n{exception_tests}\n\"\"\"\n\n    def _generate_class_test(self, cls):\n        \"\"\"Gera testes para uma classe\"\"\"\n\n        # Teste de inicializa\u00e7\u00e3o\n        init_test = f\"\"\"\nclass Test{cls['name']}:\n    '''Testes para a classe {cls['name']}'''\n\n    def test_init(self):\n        '''Teste de inicializa\u00e7\u00e3o'''\n        # TODO: Configurar par\u00e2metros de inicializa\u00e7\u00e3o\n        instance = {cls['name']}()\n        assert instance is not None\n\"\"\"\n\n        # Testes para m\u00e9todos\n        method_tests = \"\"\n        for method in cls['methods']:\n            method_tests += f\"\"\"\n    def test_{method['name']}(self):\n        '''Teste para o m\u00e9todo {method['name']}'''\n        # Arrange\n        instance = {cls['name']}()\n        # TODO: Configurar dados de teste\n\n        # Act\n        {'result = ' if method['returns'] else ''}instance.{method['name']}()\n\n        # Assert\n        {'assert result is not None' if method['returns'] else '# TODO: Adicionar assertions'}\n\"\"\"\n\n        return init_test + method_tests\n\ndef generate_tests_for_file(filepath):\n    \"\"\"Gera testes para um arquivo espec\u00edfico\"\"\"\n    generator = TestGenerator(filepath)\n    return generator.generate_test_file()\n\ndef generate_tests_for_project():\n    \"\"\"Gera testes para todo o projeto\"\"\"\n    src_path = Path(\"src\")\n    generated_files = []\n\n    for py_file in src_path.rglob(\"*.py\"):\n        if py_file.name == \"__init__.py\":\n            continue\n\n        try:\n            test_file = generate_tests_for_file(py_file)\n            generated_files.append(test_file)\n        except Exception as e:\n            print(f\"\u274c Erro ao gerar teste para {py_file}: {e}\")\n\n    print(f\"\\n\ud83d\udcca Gerados {len(generated_files)} arquivos de teste\")\n    return generated_files\n\nif __name__ == \"__main__\":\n    generate_tests_for_project()\n</code></pre>"},{"location":"tests/coverage/#monitoramento-continuo","title":"\ud83d\udcc8 Monitoramento Cont\u00ednuo","text":""},{"location":"tests/coverage/#1-github-actions-para-coverage","title":"1. \ud83d\udd04 GitHub Actions para Coverage","text":"<pre><code># .github/workflows/coverage.yml\nname: Coverage Report\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\njobs:\n  coverage:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.12'\n\n    - name: Install dependencies\n      run: |\n        pip install poetry\n        poetry install\n\n    - name: Run tests with coverage\n      run: |\n        poetry run pytest --cov=src --cov-report=xml --cov-report=html --cov-fail-under=85\n\n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        flags: unittests\n        name: codecov-umbrella\n        fail_ci_if_error: true\n\n    - name: Coverage comment\n      uses: py-cov-action/python-coverage-comment-action@v3\n      with:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        MINIMUM_GREEN: 90\n        MINIMUM_ORANGE: 80\n\n    - name: Upload HTML coverage report\n      uses: actions/upload-artifact@v3\n      with:\n        name: coverage-report\n        path: htmlcov/\n</code></pre>"},{"location":"tests/coverage/#2-badge-de-cobertura","title":"2. \ud83d\udcca Badge de Cobertura","text":"<pre><code>&lt;!-- README.md --&gt;\n[![Coverage Status](https://codecov.io/gh/username/machine-learning-engineer/branch/main/graph/badge.svg)](https://codecov.io/gh/username/machine-learning-engineer)\n\n[![Coverage](https://img.shields.io/badge/Coverage-87.3%25-yellow.svg)](htmlcov/index.html)\n</code></pre>"},{"location":"tests/coverage/#metas-de-cobertura","title":"\ud83c\udfaf Metas de Cobertura","text":""},{"location":"tests/coverage/#configuracao-por-modulo","title":"\ud83d\udccb Configura\u00e7\u00e3o por M\u00f3dulo","text":"<pre><code># coverage-goals.yml\ncoverage_goals:\n  global:\n    minimum: 85%\n    target: 90%\n\n  modules:\n    src/routers/:\n      minimum: 90%\n      target: 95%\n      critical: true\n\n    src/services/:\n      minimum: 90%\n      target: 95%\n      critical: true\n\n    src/ml/:\n      minimum: 80%\n      target: 90%\n      critical: false\n\n    src/utils/:\n      minimum: 85%\n      target: 90%\n      critical: false\n\n    src/config/:\n      minimum: 70%\n      target: 85%\n      critical: false\n\n  exceptions:\n    # Arquivos exclu\u00eddos da an\u00e1lise\n    exclude:\n      - \"src/migrations/\"\n      - \"src/settings.py\"\n      - \"src/__main__.py\"\n\n    # Linhas exclu\u00eddas\n    exclude_lines:\n      - \"pragma: no cover\"\n      - \"raise NotImplementedError\"\n      - \"if __name__ == .__main__.:\"\n</code></pre>"},{"location":"tests/coverage/#alertas-de-cobertura","title":"\ud83d\udea8 Alertas de Cobertura","text":"<pre><code># scripts/coverage_alerts.py\nimport json\nimport yaml\nfrom pathlib import Path\n\ndef check_coverage_goals():\n    \"\"\"Verifica se as metas de cobertura foram atingidas\"\"\"\n\n    # Carregar configura\u00e7\u00e3o\n    with open(\"coverage-goals.yml\") as f:\n        goals = yaml.safe_load(f)\n\n    # Carregar dados de cobertura\n    with open(\"coverage.json\") as f:\n        coverage_data = json.load(f)\n\n    alerts = []\n\n    # Verificar cobertura global\n    global_coverage = coverage_data['totals']['percent_covered']\n    global_minimum = float(goals['coverage_goals']['global']['minimum'].rstrip('%'))\n\n    if global_coverage &lt; global_minimum:\n        alerts.append({\n            'type': 'global',\n            'severity': 'error',\n            'message': f\"Cobertura global ({global_coverage:.1f}%) abaixo do m\u00ednimo ({global_minimum}%)\"\n        })\n\n    # Verificar cobertura por m\u00f3dulo\n    files = coverage_data['files']\n    module_goals = goals['coverage_goals']['modules']\n\n    for module_path, module_config in module_goals.items():\n        module_files = [f for f in files.keys() if f.startswith(module_path)]\n\n        if not module_files:\n            continue\n\n        # Calcular cobertura do m\u00f3dulo\n        total_statements = sum(files[f]['summary']['num_statements'] for f in module_files)\n        covered_statements = sum(files[f]['summary']['covered_lines'] for f in module_files)\n\n        module_coverage = (covered_statements / total_statements) * 100 if total_statements &gt; 0 else 100\n\n        minimum = float(module_config['minimum'].rstrip('%'))\n\n        if module_coverage &lt; minimum:\n            severity = 'error' if module_config.get('critical', False) else 'warning'\n\n            alerts.append({\n                'type': 'module',\n                'module': module_path,\n                'severity': severity,\n                'current': module_coverage,\n                'minimum': minimum,\n                'message': f\"M\u00f3dulo {module_path}: {module_coverage:.1f}% &lt; {minimum}%\"\n            })\n\n    return alerts\n\ndef send_coverage_alerts(alerts):\n    \"\"\"Envia alertas de cobertura\"\"\"\n\n    if not alerts:\n        print(\"\u2705 Todas as metas de cobertura foram atingidas!\")\n        return\n\n    print(\"\ud83d\udea8 ALERTAS DE COBERTURA\")\n    print(\"=\" * 50)\n\n    errors = [a for a in alerts if a['severity'] == 'error']\n    warnings = [a for a in alerts if a['severity'] == 'warning']\n\n    if errors:\n        print(f\"\\n\u274c ERROS ({len(errors)}):\")\n        for alert in errors:\n            print(f\"  \u2022 {alert['message']}\")\n\n    if warnings:\n        print(f\"\\n\u26a0\ufe0f AVISOS ({len(warnings)}):\")\n        for alert in warnings:\n            print(f\"  \u2022 {alert['message']}\")\n\n    # TODO: Integrar com Slack, Teams, etc.\n    return len(errors) &gt; 0\n\nif __name__ == \"__main__\":\n    alerts = check_coverage_goals()\n    has_errors = send_coverage_alerts(alerts)\n    exit(1 if has_errors else 0)\n</code></pre>"},{"location":"tests/coverage/#relatorios-avancados","title":"\ud83d\udcca Relat\u00f3rios Avan\u00e7ados","text":""},{"location":"tests/coverage/#1-tendencias-de-cobertura","title":"1. \ud83d\udd52 Tend\u00eancias de Cobertura","text":"<pre><code># scripts/coverage_trends.py\nimport json\nimport sqlite3\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nclass CoverageTrendTracker:\n    \"\"\"Rastreia tend\u00eancias de cobertura ao longo do tempo\"\"\"\n\n    def __init__(self, db_path=\"coverage_history.db\"):\n        self.db_path = db_path\n        self._init_db()\n\n    def _init_db(self):\n        \"\"\"Inicializa banco de dados\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        cursor.execute('''\n            CREATE TABLE IF NOT EXISTS coverage_history (\n                id INTEGER PRIMARY KEY AUTOINCREMENT,\n                timestamp TEXT NOT NULL,\n                commit_hash TEXT,\n                total_coverage REAL NOT NULL,\n                lines_covered INTEGER NOT NULL,\n                lines_total INTEGER NOT NULL,\n                branches_covered INTEGER NOT NULL,\n                branches_total INTEGER NOT NULL,\n                module_data TEXT  -- JSON com dados por m\u00f3dulo\n            )\n        ''')\n\n        conn.commit()\n        conn.close()\n\n    def record_coverage(self, coverage_file=\"coverage.json\", commit_hash=None):\n        \"\"\"Registra dados de cobertura atual\"\"\"\n\n        with open(coverage_file) as f:\n            data = json.load(f)\n\n        totals = data['totals']\n\n        # Dados por m\u00f3dulo\n        module_data = {}\n        for filepath, file_data in data['files'].items():\n            module = filepath.split('/')[0] if '/' in filepath else 'root'\n\n            if module not in module_data:\n                module_data[module] = {\n                    'covered_lines': 0,\n                    'total_lines': 0,\n                    'files': 0\n                }\n\n            module_data[module]['covered_lines'] += file_data['summary']['covered_lines']\n            module_data[module]['total_lines'] += file_data['summary']['num_statements']\n            module_data[module]['files'] += 1\n\n        # Calcular cobertura por m\u00f3dulo\n        for module, stats in module_data.items():\n            if stats['total_lines'] &gt; 0:\n                stats['coverage'] = (stats['covered_lines'] / stats['total_lines']) * 100\n            else:\n                stats['coverage'] = 100.0\n\n        # Salvar no banco\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        cursor.execute('''\n            INSERT INTO coverage_history \n            (timestamp, commit_hash, total_coverage, lines_covered, lines_total, \n             branches_covered, branches_total, module_data)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n        ''', (\n            datetime.now().isoformat(),\n            commit_hash,\n            totals['percent_covered'],\n            totals['covered_lines'],\n            totals['num_statements'],\n            totals['covered_branches'],\n            totals['num_branches'],\n            json.dumps(module_data)\n        ))\n\n        conn.commit()\n        conn.close()\n\n        print(f\"\ud83d\udcca Cobertura registrada: {totals['percent_covered']:.1f}%\")\n\n    def generate_trend_report(self, days=30):\n        \"\"\"Gera relat\u00f3rio de tend\u00eancias\"\"\"\n\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        cursor.execute('''\n            SELECT timestamp, total_coverage, lines_covered, lines_total\n            FROM coverage_history\n            WHERE datetime(timestamp) &gt;= datetime('now', '-{} days')\n            ORDER BY timestamp\n        '''.format(days))\n\n        results = cursor.fetchall()\n        conn.close()\n\n        if not results:\n            print(\"N\u00e3o h\u00e1 dados suficientes para gerar relat\u00f3rio\")\n            return\n\n        # Preparar dados para gr\u00e1fico\n        timestamps = [datetime.fromisoformat(row[0]) for row in results]\n        coverages = [row[1] for row in results]\n\n        # Gerar gr\u00e1fico\n        plt.figure(figsize=(12, 6))\n        plt.plot(timestamps, coverages, marker='o', linewidth=2, markersize=4)\n        plt.title(f'Tend\u00eancia de Cobertura - \u00daltimos {days} dias')\n        plt.xlabel('Data')\n        plt.ylabel('Cobertura (%)')\n        plt.grid(True, alpha=0.3)\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n\n        # Salvar gr\u00e1fico\n        plt.savefig('coverage_trend.png', dpi=150, bbox_inches='tight')\n        print(\"\ud83d\udcc8 Gr\u00e1fico salvo: coverage_trend.png\")\n\n        # Estat\u00edsticas\n        if len(coverages) &gt;= 2:\n            trend = coverages[-1] - coverages[0]\n            avg_coverage = sum(coverages) / len(coverages)\n\n            print(f\"\\n\ud83d\udcca ESTAT\u00cdSTICAS ({days} dias)\")\n            print(f\"Cobertura atual: {coverages[-1]:.1f}%\")\n            print(f\"Cobertura inicial: {coverages[0]:.1f}%\")\n            print(f\"Tend\u00eancia: {trend:+.1f}%\")\n            print(f\"M\u00e9dia: {avg_coverage:.1f}%\")\n\n# Uso\nif __name__ == \"__main__\":\n    tracker = CoverageTrendTracker()\n\n    # Registrar cobertura atual\n    tracker.record_coverage()\n\n    # Gerar relat\u00f3rio de tend\u00eancias\n    tracker.generate_trend_report()\n</code></pre>"},{"location":"tests/coverage/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83e\uddea Testes - Executar testes completos</li> <li>\ud83d\udd04 Integra\u00e7\u00e3o - Testes de integra\u00e7\u00e3o</li> <li>\ud83c\udfd7\ufe0f Arquitetura - Vis\u00e3o geral do sistema</li> </ol>"},{"location":"tests/coverage/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83d\udcca Coverage.py - Documenta\u00e7\u00e3o oficial</li> <li>\ud83e\uddea Pytest Coverage - Plugin de cobertura</li> <li>\ud83d\udcc8 Codecov - Servi\u00e7o de cobertura online</li> </ul>"},{"location":"tests/integration/","title":"\ud83d\udd17 Testes de Integra\u00e7\u00e3o","text":"<p>Documenta\u00e7\u00e3o completa dos testes de integra\u00e7\u00e3o, incluindo configura\u00e7\u00e3o de ambiente, mocks de servi\u00e7os externos, testes de API end-to-end e valida\u00e7\u00e3o de pipeline de Machine Learning.</p>"},{"location":"tests/integration/#visao-geral","title":"\ud83c\udfaf Vis\u00e3o Geral","text":"<p>Os testes de integra\u00e7\u00e3o verificam se os componentes do sistema trabalham corretamente em conjunto, simulando cen\u00e1rios reais de uso e validando integra\u00e7\u00f5es entre diferentes m\u00f3dulos.</p>"},{"location":"tests/integration/#estrutura-dos-testes-de-integracao","title":"\ud83c\udfd7\ufe0f Estrutura dos Testes de Integra\u00e7\u00e3o","text":""},{"location":"tests/integration/#organizacao-dos-arquivos","title":"\ud83d\udcc1 Organiza\u00e7\u00e3o dos Arquivos","text":"<pre><code>tests/\n\u251c\u2500\u2500 integration/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 conftest.py                 # Configura\u00e7\u00e3o fixtures integra\u00e7\u00e3o\n\u2502   \u251c\u2500\u2500 test_api_integration.py     # Testes API completos\n\u2502   \u251c\u2500\u2500 test_ml_pipeline.py         # Testes pipeline ML\n\u2502   \u251c\u2500\u2500 test_database_integration.py # Testes integra\u00e7\u00e3o DB\n\u2502   \u251c\u2500\u2500 test_external_apis.py       # Testes APIs externas\n\u2502   \u2514\u2500\u2500 fixtures/\n\u2502       \u251c\u2500\u2500 sample_data.json        # Dados de teste\n\u2502       \u251c\u2500\u2500 mock_responses/         # Respostas mockadas\n\u2502       \u2514\u2500\u2500 test_datasets/          # Datasets de teste\n\u2514\u2500\u2500 e2e/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_complete_workflow.py   # Fluxo completo E2E\n    \u2514\u2500\u2500 test_user_scenarios.py      # Cen\u00e1rios de usu\u00e1rio\n</code></pre>"},{"location":"tests/integration/#configuracao-dos-testes","title":"\u2699\ufe0f Configura\u00e7\u00e3o dos Testes","text":""},{"location":"tests/integration/#conftestpy-fixtures-de-integracao","title":"\ud83d\udccb conftest.py - Fixtures de Integra\u00e7\u00e3o","text":"<pre><code># tests/integration/conftest.py\nimport pytest\nimport asyncio\nimport pandas as pd\nfrom pathlib import Path\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom unittest.mock import AsyncMock, patch\nimport json\n\nfrom src.main import app\nfrom src.database import get_db, Base\nfrom src.config import settings\nfrom src.services.database import DatabaseService\nfrom src.services.external_apis import WeatherService, AirportService\n\n# Database de teste\nSQLALCHEMY_TEST_DATABASE_URL = \"sqlite:///./test.db\"\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Cria loop de eventos para testes async\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"session\")\ndef test_engine():\n    \"\"\"Engine de teste do SQLAlchemy\"\"\"\n    engine = create_engine(\n        SQLALCHEMY_TEST_DATABASE_URL,\n        connect_args={\"check_same_thread\": False}\n    )\n    Base.metadata.create_all(bind=engine)\n    yield engine\n    Base.metadata.drop_all(bind=engine)\n\n@pytest.fixture(scope=\"session\")\ndef test_session_factory(test_engine):\n    \"\"\"Factory de sess\u00f5es de teste\"\"\"\n    TestingSessionLocal = sessionmaker(\n        autocommit=False,\n        autoflush=False,\n        bind=test_engine\n    )\n    return TestingSessionLocal\n\n@pytest.fixture(scope=\"function\")\ndef test_db(test_session_factory):\n    \"\"\"Sess\u00e3o de banco para testes individuais\"\"\"\n    session = test_session_factory()\n    try:\n        yield session\n    finally:\n        session.rollback()\n        session.close()\n\n@pytest.fixture(scope=\"function\")\ndef test_app(test_db):\n    \"\"\"Cliente de teste FastAPI com DB mockado\"\"\"\n    def override_get_db():\n        try:\n            yield test_db\n        finally:\n            pass\n\n    app.dependency_overrides[get_db] = override_get_db\n\n    with TestClient(app) as client:\n        yield client\n\n    app.dependency_overrides.clear()\n\n@pytest.fixture\ndef sample_flight_data():\n    \"\"\"Dados de voo para testes\"\"\"\n    return {\n        \"flight_number\": \"AA123\",\n        \"airline\": \"American Airlines\", \n        \"origin_airport\": \"JFK\",\n        \"destination_airport\": \"LAX\",\n        \"scheduled_departure\": \"2024-01-15T10:00:00\",\n        \"scheduled_arrival\": \"2024-01-15T13:30:00\",\n        \"aircraft_type\": \"Boeing 737\",\n        \"passenger_count\": 150,\n        \"cargo_weight\": 5000.5,\n        \"weather_conditions\": {\n            \"temperature\": 22.5,\n            \"humidity\": 65.2,\n            \"wind_speed\": 15.3,\n            \"visibility\": 10.0,\n            \"precipitation\": 0.0\n        },\n        \"airport_info\": {\n            \"origin\": {\n                \"timezone\": \"America/New_York\",\n                \"elevation\": 13,\n                \"runway_count\": 4\n            },\n            \"destination\": {\n                \"timezone\": \"America/Los_Angeles\", \n                \"elevation\": 38,\n                \"runway_count\": 4\n            }\n        }\n    }\n\n@pytest.fixture\ndef sample_historical_data():\n    \"\"\"Dataset hist\u00f3rico para testes\"\"\"\n    data = {\n        'flight_number': ['AA123', 'UA456', 'DL789'] * 100,\n        'airline': ['American Airlines', 'United Airlines', 'Delta Airlines'] * 100,\n        'origin_airport': ['JFK', 'LAX', 'ORD'] * 100,\n        'destination_airport': ['LAX', 'ORD', 'JFK'] * 100,\n        'scheduled_departure_hour': [10, 14, 18] * 100,\n        'day_of_week': [1, 2, 3, 4, 5, 6, 7] * 43,  # ~300 registros\n        'month': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] * 25,\n        'weather_score': [0.8, 0.6, 0.9] * 100,\n        'airport_congestion': [0.3, 0.7, 0.5] * 100,\n        'delay_minutes': [0, 15, 30, 45, 0, 0, 10] * 43  # Target variable\n    }\n\n    return pd.DataFrame(data)\n\n@pytest.fixture\ndef mock_weather_service():\n    \"\"\"Mock do servi\u00e7o de clima\"\"\"\n    with patch('src.services.external_apis.WeatherService') as mock:\n        service = AsyncMock()\n\n        # Configurar respostas padr\u00e3o\n        service.get_current_weather.return_value = {\n            \"temperature\": 25.0,\n            \"humidity\": 60.0,\n            \"wind_speed\": 10.0,\n            \"visibility\": 10.0,\n            \"precipitation\": 0.0,\n            \"weather_score\": 0.85\n        }\n\n        service.get_forecast.return_value = [\n            {\n                \"datetime\": \"2024-01-15T10:00:00\",\n                \"temperature\": 25.0,\n                \"conditions\": \"Clear\"\n            }\n        ]\n\n        mock.return_value = service\n        yield service\n\n@pytest.fixture  \ndef mock_airport_service():\n    \"\"\"Mock do servi\u00e7o de aeroportos\"\"\"\n    with patch('src.services.external_apis.AirportService') as mock:\n        service = AsyncMock()\n\n        # Dados de aeroportos mockados\n        airport_data = {\n            \"JFK\": {\n                \"name\": \"John F. Kennedy International\",\n                \"city\": \"New York\",\n                \"country\": \"United States\",\n                \"timezone\": \"America/New_York\",\n                \"elevation\": 13,\n                \"runway_count\": 4,\n                \"capacity_score\": 0.9\n            },\n            \"LAX\": {\n                \"name\": \"Los Angeles International\",\n                \"city\": \"Los Angeles\", \n                \"country\": \"United States\",\n                \"timezone\": \"America/Los_Angeles\",\n                \"elevation\": 38,\n                \"runway_count\": 4,\n                \"capacity_score\": 0.85\n            }\n        }\n\n        service.get_airport_info.side_effect = lambda code: airport_data.get(code)\n        service.get_congestion_level.return_value = 0.3\n\n        mock.return_value = service\n        yield service\n\n@pytest.fixture\ndef test_model_artifacts(tmp_path):\n    \"\"\"Cria artefatos de modelo para teste\"\"\"\n\n    # Criar diret\u00f3rio de modelos tempor\u00e1rio\n    model_dir = tmp_path / \"models\"\n    model_dir.mkdir()\n\n    # Mock de modelo treinado (pickle)\n    import pickle\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.preprocessing import StandardScaler\n\n    # Modelo simples para teste\n    model = RandomForestRegressor(n_estimators=10, random_state=42)\n    scaler = StandardScaler()\n\n    # Treinar com dados dummy\n    X_dummy = [[1, 2, 3, 4, 5], [2, 3, 4, 5, 6], [3, 4, 5, 6, 7]]\n    y_dummy = [10, 20, 30]\n\n    scaler.fit(X_dummy)\n    model.fit(scaler.transform(X_dummy), y_dummy)\n\n    # Salvar artefatos\n    model_path = model_dir / \"flight_delay_model.pkl\"\n    scaler_path = model_dir / \"feature_scaler.pkl\"\n\n    with open(model_path, 'wb') as f:\n        pickle.dump(model, f)\n\n    with open(scaler_path, 'wb') as f:\n        pickle.dump(scaler, f)\n\n    # Metadados do modelo\n    metadata = {\n        \"model_version\": \"1.0.0\",\n        \"training_date\": \"2024-01-15\",\n        \"features\": [\"hour\", \"day_of_week\", \"month\", \"weather_score\", \"congestion\"],\n        \"metrics\": {\n            \"mae\": 12.5,\n            \"rmse\": 18.7,\n            \"r2_score\": 0.76\n        }\n    }\n\n    metadata_path = model_dir / \"model_metadata.json\"\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f)\n\n    return {\n        \"model_path\": model_path,\n        \"scaler_path\": scaler_path,\n        \"metadata_path\": metadata_path,\n        \"model_dir\": model_dir\n    }\n\n@pytest.fixture\ndef integration_config():\n    \"\"\"Configura\u00e7\u00e3o espec\u00edfica para testes de integra\u00e7\u00e3o\"\"\"\n    return {\n        \"api_timeout\": 30,\n        \"max_retries\": 3,\n        \"batch_size\": 100,\n        \"weather_api_key\": \"test_key_123\",\n        \"airport_api_key\": \"test_key_456\",\n        \"model_threshold\": 0.8,\n        \"cache_ttl\": 300\n    }\n</code></pre>"},{"location":"tests/integration/#testes-de-api-completos","title":"\ud83e\uddea Testes de API Completos","text":""},{"location":"tests/integration/#test_api_integrationpy","title":"\ud83c\udf10 test_api_integration.py","text":"<pre><code># tests/integration/test_api_integration.py\nimport pytest\nimport asyncio\nfrom datetime import datetime, timedelta\nimport json\n\nclass TestAPIIntegration:\n    \"\"\"Testes completos de integra\u00e7\u00e3o da API\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_predict_endpoint_complete_flow(\n        self, \n        test_app, \n        sample_flight_data,\n        mock_weather_service,\n        mock_airport_service,\n        test_model_artifacts\n    ):\n        \"\"\"Testa fluxo completo do endpoint de predi\u00e7\u00e3o\"\"\"\n\n        # Configurar mocks\n        with pytest.MonkeyPatch.context() as mp:\n            mp.setattr(\"src.ml.model_loader.MODEL_PATH\", test_model_artifacts[\"model_dir\"])\n\n            # Fazer predi\u00e7\u00e3o\n            response = test_app.post(\n                \"/api/v1/predict\",\n                json=sample_flight_data\n            )\n\n            # Verificar resposta\n            assert response.status_code == 200\n\n            result = response.json()\n            assert \"prediction\" in result\n            assert \"confidence\" in result\n            assert \"metadata\" in result\n\n            # Verificar estrutura da predi\u00e7\u00e3o\n            prediction = result[\"prediction\"]\n            assert isinstance(prediction[\"delay_minutes\"], (int, float))\n            assert isinstance(prediction[\"probability_delayed\"], float)\n            assert 0 &lt;= prediction[\"probability_delayed\"] &lt;= 1\n\n            # Verificar metadados\n            metadata = result[\"metadata\"]\n            assert \"model_version\" in metadata\n            assert \"prediction_time\" in metadata\n            assert \"processing_time_ms\" in metadata\n\n    @pytest.mark.asyncio\n    async def test_batch_prediction_endpoint(\n        self,\n        test_app,\n        sample_flight_data,\n        test_model_artifacts\n    ):\n        \"\"\"Testa endpoint de predi\u00e7\u00e3o em lote\"\"\"\n\n        # Criar m\u00faltiplos voos\n        flights = []\n        for i in range(5):\n            flight = sample_flight_data.copy()\n            flight[\"flight_number\"] = f\"AA{123 + i}\"\n            flights.append(flight)\n\n        with pytest.MonkeyPatch.context() as mp:\n            mp.setattr(\"src.ml.model_loader.MODEL_PATH\", test_model_artifacts[\"model_dir\"])\n\n            response = test_app.post(\n                \"/api/v1/predict/batch\",\n                json={\"flights\": flights}\n            )\n\n            assert response.status_code == 200\n\n            result = response.json()\n            assert \"predictions\" in result\n            assert len(result[\"predictions\"]) == 5\n\n            # Verificar cada predi\u00e7\u00e3o\n            for pred in result[\"predictions\"]:\n                assert \"flight_number\" in pred\n                assert \"prediction\" in pred\n                assert \"status\" in pred\n                assert pred[\"status\"] == \"success\"\n\n    @pytest.mark.asyncio \n    async def test_model_info_endpoint(self, test_app, test_model_artifacts):\n        \"\"\"Testa endpoint de informa\u00e7\u00f5es do modelo\"\"\"\n\n        with pytest.MonkeyPatch.context() as mp:\n            mp.setattr(\"src.ml.model_loader.MODEL_PATH\", test_model_artifacts[\"model_dir\"])\n\n            response = test_app.get(\"/api/v1/model/info\")\n\n            assert response.status_code == 200\n\n            info = response.json()\n            assert \"version\" in info\n            assert \"features\" in info\n            assert \"metrics\" in info\n            assert \"training_date\" in info\n\n    @pytest.mark.asyncio\n    async def test_health_check_complete(self, test_app):\n        \"\"\"Testa health check completo do sistema\"\"\"\n\n        response = test_app.get(\"/health\")\n\n        assert response.status_code == 200\n\n        health = response.json()\n        assert \"status\" in health\n        assert \"timestamp\" in health\n        assert \"checks\" in health\n\n        # Verificar checks individuais\n        checks = health[\"checks\"]\n        expected_checks = [\"database\", \"model\", \"external_apis\"]\n\n        for check_name in expected_checks:\n            assert check_name in checks\n            assert \"status\" in checks[check_name]\n            assert \"response_time\" in checks[check_name]\n\n    def test_error_handling_invalid_data(self, test_app):\n        \"\"\"Testa tratamento de erros com dados inv\u00e1lidos\"\"\"\n\n        invalid_data = {\n            \"flight_number\": \"\",  # Inv\u00e1lido\n            \"origin_airport\": \"INVALID\",  # C\u00f3digo inv\u00e1lido\n            \"scheduled_departure\": \"invalid-date\"  # Data inv\u00e1lida\n        }\n\n        response = test_app.post(\n            \"/api/v1/predict\",\n            json=invalid_data\n        )\n\n        assert response.status_code == 422\n\n        error = response.json()\n        assert \"detail\" in error\n        assert isinstance(error[\"detail\"], list)\n\n        # Verificar que todos os campos inv\u00e1lidos foram identificados\n        field_errors = [err[\"loc\"][-1] for err in error[\"detail\"]]\n        assert \"flight_number\" in field_errors\n        assert \"origin_airport\" in field_errors\n\n    def test_rate_limiting(self, test_app, sample_flight_data):\n        \"\"\"Testa rate limiting da API\"\"\"\n\n        # Fazer muitas requisi\u00e7\u00f5es rapidamente\n        responses = []\n        for i in range(50):  # Assumindo limite de 100/min\n            response = test_app.post(\n                \"/api/v1/predict\",\n                json=sample_flight_data\n            )\n            responses.append(response)\n\n        # Todas as primeiras requisi\u00e7\u00f5es devem funcionar\n        success_responses = [r for r in responses if r.status_code == 200]\n        rate_limited = [r for r in responses if r.status_code == 429]\n\n        # Deve haver pelo menos algumas respostas de sucesso\n        assert len(success_responses) &gt; 0\n\n        # Se houver rate limiting, verificar headers\n        if rate_limited:\n            limited_response = rate_limited[0]\n            headers = limited_response.headers\n            assert \"X-RateLimit-Limit\" in headers\n            assert \"X-RateLimit-Remaining\" in headers\n\nclass TestDatabaseIntegration:\n    \"\"\"Testes de integra\u00e7\u00e3o com banco de dados\"\"\"\n\n    def test_flight_prediction_storage(\n        self, \n        test_db, \n        sample_flight_data,\n        test_model_artifacts\n    ):\n        \"\"\"Testa armazenamento de predi\u00e7\u00f5es no banco\"\"\"\n\n        from src.services.database import DatabaseService\n\n        db_service = DatabaseService(test_db)\n\n        # Simular predi\u00e7\u00e3o\n        prediction_data = {\n            \"flight_id\": \"AA123_20240115\",\n            \"flight_number\": \"AA123\",\n            \"prediction\": {\n                \"delay_minutes\": 25.5,\n                \"probability_delayed\": 0.75,\n                \"confidence\": 0.82\n            },\n            \"input_features\": sample_flight_data,\n            \"model_version\": \"1.0.0\",\n            \"prediction_time\": datetime.now()\n        }\n\n        # Salvar predi\u00e7\u00e3o\n        prediction_id = db_service.save_prediction(prediction_data)\n        assert prediction_id is not None\n\n        # Recuperar predi\u00e7\u00e3o\n        stored_prediction = db_service.get_prediction(prediction_id)\n        assert stored_prediction is not None\n        assert stored_prediction.flight_number == \"AA123\"\n        assert stored_prediction.delay_minutes == 25.5\n\n    def test_historical_data_query(self, test_db, sample_historical_data):\n        \"\"\"Testa consultas de dados hist\u00f3ricos\"\"\"\n\n        from src.services.database import DatabaseService\n\n        db_service = DatabaseService(test_db)\n\n        # Inserir dados hist\u00f3ricos (mock)\n        # Em um teste real, voc\u00ea popularia a tabela de hist\u00f3rico\n\n        # Testar consulta por aeroporto\n        jfk_data = db_service.get_historical_data(\n            origin_airport=\"JFK\",\n            start_date=datetime.now() - timedelta(days=30),\n            end_date=datetime.now()\n        )\n\n        # Verificar estrutura dos dados\n        assert isinstance(jfk_data, list)\n\n        if jfk_data:  # Se houver dados\n            record = jfk_data[0]\n            expected_fields = [\n                \"flight_number\", \"origin_airport\", \n                \"destination_airport\", \"delay_minutes\"\n            ]\n\n            for field in expected_fields:\n                assert hasattr(record, field)\n\n    def test_model_metrics_tracking(self, test_db):\n        \"\"\"Testa rastreamento de m\u00e9tricas do modelo\"\"\"\n\n        from src.services.database import DatabaseService\n\n        db_service = DatabaseService(test_db)\n\n        # Simular m\u00e9tricas de performance\n        metrics_data = {\n            \"model_version\": \"1.0.0\",\n            \"evaluation_date\": datetime.now(),\n            \"mae\": 12.5,\n            \"rmse\": 18.7,\n            \"r2_score\": 0.76,\n            \"accuracy_threshold_15min\": 0.82,\n            \"sample_count\": 1000,\n            \"dataset_period\": \"2024-01-01_to_2024-01-31\"\n        }\n\n        # Salvar m\u00e9tricas\n        metrics_id = db_service.save_model_metrics(metrics_data)\n        assert metrics_id is not None\n\n        # Recuperar m\u00e9tricas mais recentes\n        latest_metrics = db_service.get_latest_model_metrics()\n        assert latest_metrics is not None\n        assert latest_metrics.model_version == \"1.0.0\"\n        assert latest_metrics.mae == 12.5\n\nclass TestExternalAPIIntegration:\n    \"\"\"Testes de integra\u00e7\u00e3o com APIs externas\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_weather_api_integration(self, mock_weather_service):\n        \"\"\"Testa integra\u00e7\u00e3o com API de clima\"\"\"\n\n        # Testar obten\u00e7\u00e3o de clima atual\n        weather = await mock_weather_service.get_current_weather(\"JFK\")\n\n        assert weather is not None\n        assert \"temperature\" in weather\n        assert \"weather_score\" in weather\n        assert 0 &lt;= weather[\"weather_score\"] &lt;= 1\n\n        # Verificar que o servi\u00e7o foi chamado\n        mock_weather_service.get_current_weather.assert_called_once_with(\"JFK\")\n\n    @pytest.mark.asyncio\n    async def test_airport_api_integration(self, mock_airport_service):\n        \"\"\"Testa integra\u00e7\u00e3o com API de aeroportos\"\"\"\n\n        # Testar obten\u00e7\u00e3o de informa\u00e7\u00f5es do aeroporto\n        airport_info = await mock_airport_service.get_airport_info(\"JFK\")\n\n        assert airport_info is not None\n        assert \"name\" in airport_info\n        assert \"timezone\" in airport_info\n        assert \"capacity_score\" in airport_info\n\n        # Testar n\u00edvel de congestionamento\n        congestion = await mock_airport_service.get_congestion_level(\"JFK\")\n        assert isinstance(congestion, float)\n        assert 0 &lt;= congestion &lt;= 1\n\n    @pytest.mark.asyncio\n    async def test_api_failure_handling(self):\n        \"\"\"Testa tratamento de falhas em APIs externas\"\"\"\n\n        from src.services.external_apis import WeatherService\n        from unittest.mock import patch\n        import aiohttp\n\n        with patch('aiohttp.ClientSession.get') as mock_get:\n            # Simular timeout\n            mock_get.side_effect = asyncio.TimeoutError(\"API timeout\")\n\n            weather_service = WeatherService()\n\n            # Deve retornar dados padr\u00e3o em caso de falha\n            weather = await weather_service.get_current_weather(\"JFK\")\n\n            assert weather is not None\n            assert \"error\" in weather or \"default\" in weather\n\n    @pytest.mark.asyncio\n    async def test_api_retry_mechanism(self):\n        \"\"\"Testa mecanismo de retry para APIs\"\"\"\n\n        from src.services.external_apis import WeatherService\n        from unittest.mock import patch, AsyncMock\n\n        with patch('aiohttp.ClientSession.get') as mock_get:\n            # Primeira chamada falha, segunda funciona\n            mock_response = AsyncMock()\n            mock_response.status = 200\n            mock_response.json.return_value = {\"temperature\": 25.0}\n\n            mock_get.side_effect = [\n                aiohttp.ClientConnectorError(\"Connection failed\"),\n                mock_response\n            ]\n\n            weather_service = WeatherService(max_retries=2)\n            weather = await weather_service.get_current_weather(\"JFK\")\n\n            # Deve ter funcionado na segunda tentativa\n            assert weather[\"temperature\"] == 25.0\n            assert mock_get.call_count == 2\n</code></pre>"},{"location":"tests/integration/#testes-de-pipeline-ml","title":"\ud83e\udd16 Testes de Pipeline ML","text":""},{"location":"tests/integration/#test_ml_pipelinepy","title":"\ud83d\udd04 test_ml_pipeline.py","text":"<pre><code># tests/integration/test_ml_pipeline.py\nimport pytest\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nimport tempfile\n\nclass TestMLPipelineIntegration:\n    \"\"\"Testes completos do pipeline de ML\"\"\"\n\n    def test_complete_training_pipeline(\n        self, \n        sample_historical_data,\n        tmp_path\n    ):\n        \"\"\"Testa pipeline completo de treinamento\"\"\"\n\n        from src.ml.trainer import ModelTrainer\n        from src.ml.preprocessor import DataPreprocessor\n\n        # Preparar dados\n        preprocessor = DataPreprocessor()\n\n        # Preprocessing\n        processed_data = preprocessor.fit_transform(sample_historical_data)\n        assert processed_data is not None\n        assert len(processed_data) &gt; 0\n\n        # Dividir dados\n        X = processed_data.drop(['delay_minutes'], axis=1)\n        y = processed_data['delay_minutes']\n\n        # Treinar modelo\n        trainer = ModelTrainer(model_save_path=tmp_path)\n\n        model, metrics = trainer.train(X, y)\n\n        # Verificar modelo treinado\n        assert model is not None\n        assert metrics is not None\n\n        # Verificar m\u00e9tricas\n        assert \"mae\" in metrics\n        assert \"rmse\" in metrics\n        assert \"r2_score\" in metrics\n        assert metrics[\"mae\"] &gt; 0\n        assert metrics[\"rmse\"] &gt; 0\n\n    def test_prediction_pipeline_end_to_end(\n        self,\n        sample_flight_data,\n        test_model_artifacts\n    ):\n        \"\"\"Testa pipeline completo de predi\u00e7\u00e3o\"\"\"\n\n        from src.ml.predictor import FlightDelayPredictor\n\n        # Inicializar preditor com modelo de teste\n        predictor = FlightDelayPredictor(\n            model_path=test_model_artifacts[\"model_path\"],\n            scaler_path=test_model_artifacts[\"scaler_path\"]\n        )\n\n        # Fazer predi\u00e7\u00e3o\n        prediction = predictor.predict(sample_flight_data)\n\n        # Verificar estrutura da predi\u00e7\u00e3o\n        assert prediction is not None\n        assert \"delay_minutes\" in prediction\n        assert \"probability_delayed\" in prediction\n        assert \"confidence\" in prediction\n\n        # Verificar valores\n        assert isinstance(prediction[\"delay_minutes\"], (int, float))\n        assert 0 &lt;= prediction[\"probability_delayed\"] &lt;= 1\n        assert 0 &lt;= prediction[\"confidence\"] &lt;= 1\n\n    def test_feature_engineering_pipeline(self, sample_historical_data):\n        \"\"\"Testa pipeline de engenharia de features\"\"\"\n\n        from src.ml.feature_engineer import FeatureEngineer\n\n        engineer = FeatureEngineer()\n\n        # Aplicar engenharia de features\n        enhanced_data = engineer.transform(sample_historical_data)\n\n        # Verificar novas features criadas\n        expected_features = [\n            'hour_sin', 'hour_cos',           # Features c\u00edclicas\n            'day_of_week_sin', 'day_of_week_cos',\n            'month_sin', 'month_cos',\n            'weather_category',               # Feature categ\u00f3rica\n            'congestion_level',               # Feature derivada\n            'route_popularity',               # Feature de rota\n            'airline_delay_history'           # Feature hist\u00f3rica\n        ]\n\n        for feature in expected_features:\n            assert feature in enhanced_data.columns, f\"Feature {feature} n\u00e3o encontrada\"\n\n        # Verificar que n\u00e3o h\u00e1 valores nulos em features cr\u00edticas\n        critical_features = ['hour_sin', 'hour_cos', 'weather_category']\n        for feature in critical_features:\n            if feature in enhanced_data.columns:\n                assert not enhanced_data[feature].isnull().any(), f\"Valores nulos em {feature}\"\n\n    def test_model_validation_pipeline(\n        self,\n        sample_historical_data,\n        tmp_path\n    ):\n        \"\"\"Testa pipeline de valida\u00e7\u00e3o do modelo\"\"\"\n\n        from src.ml.validator import ModelValidator\n        from src.ml.trainer import ModelTrainer\n\n        # Treinar modelo para valida\u00e7\u00e3o\n        trainer = ModelTrainer(model_save_path=tmp_path)\n\n        # Preparar dados\n        X = sample_historical_data.drop(['delay_minutes'], axis=1)\n        y = sample_historical_data['delay_minutes']\n\n        # Treinar\n        model, _ = trainer.train(X, y)\n\n        # Validar modelo\n        validator = ModelValidator()\n\n        validation_results = validator.validate_model(\n            model=model,\n            X_test=X,\n            y_test=y\n        )\n\n        # Verificar resultados da valida\u00e7\u00e3o\n        assert validation_results is not None\n        assert \"metrics\" in validation_results\n        assert \"validation_passed\" in validation_results\n\n        metrics = validation_results[\"metrics\"]\n        assert \"accuracy_by_threshold\" in metrics\n        assert \"confusion_matrix\" in metrics\n        assert \"classification_report\" in metrics\n\n    def test_data_drift_detection(self, sample_historical_data):\n        \"\"\"Testa detec\u00e7\u00e3o de drift nos dados\"\"\"\n\n        from src.ml.drift_detector import DataDriftDetector\n\n        # Dividir dados em refer\u00eancia e atual\n        split_idx = len(sample_historical_data) // 2\n        reference_data = sample_historical_data[:split_idx]\n        current_data = sample_historical_data[split_idx:]\n\n        # Simular drift modificando distribui\u00e7\u00e3o\n        current_data_with_drift = current_data.copy()\n        current_data_with_drift['weather_score'] = current_data_with_drift['weather_score'] * 0.5  # Artificial drift\n\n        detector = DataDriftDetector()\n\n        # Detectar drift\n        drift_report = detector.detect_drift(\n            reference_data=reference_data,\n            current_data=current_data_with_drift\n        )\n\n        # Verificar relat\u00f3rio\n        assert drift_report is not None\n        assert \"overall_drift\" in drift_report\n        assert \"feature_drift\" in drift_report\n        assert \"drift_score\" in drift_report\n\n        # Deve detectar drift na feature weather_score\n        feature_drift = drift_report[\"feature_drift\"]\n        assert \"weather_score\" in feature_drift\n\n        weather_drift = feature_drift[\"weather_score\"]\n        assert weather_drift[\"has_drift\"] == True\n        assert weather_drift[\"p_value\"] &lt; 0.05  # Significativo\n\n    def test_model_monitoring_pipeline(\n        self,\n        test_model_artifacts,\n        sample_flight_data\n    ):\n        \"\"\"Testa pipeline de monitoramento do modelo\"\"\"\n\n        from src.ml.monitor import ModelMonitor\n\n        monitor = ModelMonitor(\n            model_path=test_model_artifacts[\"model_path\"],\n            metadata_path=test_model_artifacts[\"metadata_path\"]\n        )\n\n        # Simular predi\u00e7\u00f5es para monitoramento\n        predictions = []\n        actuals = []\n\n        for i in range(10):\n            flight_data = sample_flight_data.copy()\n            flight_data[\"flight_number\"] = f\"TEST{i:03d}\"\n\n            # Fazer predi\u00e7\u00e3o\n            pred = monitor.predict_and_log(flight_data)\n            predictions.append(pred[\"delay_minutes\"])\n\n            # Simular valor real (para teste)\n            actual_delay = np.random.normal(pred[\"delay_minutes\"], 10)\n            actuals.append(max(0, actual_delay))  # Delays n\u00e3o podem ser negativos\n\n            # Registrar resultado real\n            monitor.log_actual_result(\n                flight_id=flight_data[\"flight_number\"],\n                actual_delay=actual_delay\n            )\n\n        # Gerar relat\u00f3rio de performance\n        performance_report = monitor.generate_performance_report()\n\n        assert performance_report is not None\n        assert \"current_mae\" in performance_report\n        assert \"current_rmse\" in performance_report\n        assert \"prediction_count\" in performance_report\n        assert \"drift_alerts\" in performance_report\n\n        # Verificar alertas se houver degrada\u00e7\u00e3o significativa\n        if performance_report[\"current_mae\"] &gt; 20:  # Threshold de alerta\n            assert len(performance_report[\"drift_alerts\"]) &gt; 0\n\n    def test_automated_retraining_trigger(\n        self,\n        sample_historical_data,\n        tmp_path\n    ):\n        \"\"\"Testa trigger autom\u00e1tico de retreinamento\"\"\"\n\n        from src.ml.retraining import AutoRetrainingService\n\n        service = AutoRetrainingService(\n            model_dir=tmp_path,\n            performance_threshold=0.15,  # MAE threshold\n            data_drift_threshold=0.1\n        )\n\n        # Simular degrada\u00e7\u00e3o de performance\n        mock_current_performance = {\n            \"mae\": 25.0,  # Acima do threshold\n            \"rmse\": 35.0,\n            \"prediction_count\": 1000\n        }\n\n        # Verificar se retreinamento \u00e9 necess\u00e1rio\n        should_retrain = service.should_trigger_retraining(\n            current_performance=mock_current_performance,\n            baseline_mae=15.0  # Performance baseline\n        )\n\n        assert should_retrain == True\n\n        # Testar retreinamento autom\u00e1tico\n        if should_retrain:\n            retrain_results = service.execute_retraining(\n                training_data=sample_historical_data\n            )\n\n            assert retrain_results is not None\n            assert \"new_model_path\" in retrain_results\n            assert \"performance_improvement\" in retrain_results\n            assert \"retrain_timestamp\" in retrain_results\n\nclass TestEndToEndWorkflow:\n    \"\"\"Testes de fluxo completo end-to-end\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_complete_prediction_workflow(\n        self,\n        test_app,\n        sample_flight_data,\n        mock_weather_service,\n        mock_airport_service,\n        test_model_artifacts\n    ):\n        \"\"\"Testa fluxo completo desde requisi\u00e7\u00e3o at\u00e9 resposta\"\"\"\n\n        with pytest.MonkeyPatch.context() as mp:\n            mp.setattr(\"src.ml.model_loader.MODEL_PATH\", test_model_artifacts[\"model_dir\"])\n\n            # 1. Fazer requisi\u00e7\u00e3o de predi\u00e7\u00e3o\n            start_time = datetime.now()\n\n            response = test_app.post(\n                \"/api/v1/predict\",\n                json=sample_flight_data\n            )\n\n            end_time = datetime.now()\n            processing_time = (end_time - start_time).total_seconds()\n\n            # 2. Verificar resposta\n            assert response.status_code == 200\n            result = response.json()\n\n            # 3. Verificar que todos os servi\u00e7os foram chamados\n            mock_weather_service.get_current_weather.assert_called()\n            mock_airport_service.get_airport_info.assert_called()\n\n            # 4. Verificar performance (deve ser &lt; 5 segundos)\n            assert processing_time &lt; 5.0\n\n            # 5. Verificar estrutura completa da resposta\n            assert \"prediction\" in result\n            assert \"confidence\" in result\n            assert \"metadata\" in result\n            assert \"external_data\" in result\n\n            external_data = result[\"external_data\"]\n            assert \"weather\" in external_data\n            assert \"airport_info\" in external_data\n\n    def test_batch_processing_workflow(\n        self,\n        test_app,\n        sample_flight_data,\n        test_model_artifacts\n    ):\n        \"\"\"Testa processamento em lote completo\"\"\"\n\n        # Criar lote de voos\n        batch_size = 20\n        flights = []\n\n        for i in range(batch_size):\n            flight = sample_flight_data.copy()\n            flight[\"flight_number\"] = f\"BATCH{i:03d}\"\n            flight[\"scheduled_departure\"] = (\n                datetime.fromisoformat(flight[\"scheduled_departure\"]) + \n                timedelta(hours=i)\n            ).isoformat()\n            flights.append(flight)\n\n        with pytest.MonkeyPatch.context() as mp:\n            mp.setattr(\"src.ml.model_loader.MODEL_PATH\", test_model_artifacts[\"model_dir\"])\n\n            # Processar lote\n            response = test_app.post(\n                \"/api/v1/predict/batch\",\n                json={\"flights\": flights}\n            )\n\n            assert response.status_code == 200\n\n            result = response.json()\n            predictions = result[\"predictions\"]\n\n            # Verificar que todas as predi\u00e7\u00f5es foram processadas\n            assert len(predictions) == batch_size\n\n            # Verificar que n\u00e3o h\u00e1 falhas\n            failed_predictions = [p for p in predictions if p[\"status\"] == \"error\"]\n            assert len(failed_predictions) == 0\n\n            # Verificar tempo de processamento por item\n            total_time = result[\"metadata\"][\"total_processing_time_ms\"]\n            avg_time_per_item = total_time / batch_size\n\n            # Deve processar cada item em menos de 500ms\n            assert avg_time_per_item &lt; 500\n</code></pre>"},{"location":"tests/integration/#execucao-dos-testes","title":"\ud83e\uddea Execu\u00e7\u00e3o dos Testes","text":""},{"location":"tests/integration/#comandos-de-execucao","title":"\ud83d\ude80 Comandos de Execu\u00e7\u00e3o","text":"<pre><code># Executar apenas testes de integra\u00e7\u00e3o\npytest tests/integration/ -v\n\n# Executar com cobertura espec\u00edfica\npytest tests/integration/ --cov=src --cov-report=html\n\n# Executar testes espec\u00edficos\npytest tests/integration/test_api_integration.py::TestAPIIntegration::test_predict_endpoint_complete_flow -v\n\n# Executar testes de integra\u00e7\u00e3o com logs detalhados\npytest tests/integration/ -v -s --log-cli-level=INFO\n\n# Executar testes paralelos (requer pytest-xdist)\npytest tests/integration/ -n auto\n\n# Executar apenas testes marcados como 'slow'\npytest tests/integration/ -m \"integration and not slow\"\n</code></pre>"},{"location":"tests/integration/#configuracao-de-ambiente-para-testes","title":"\u2699\ufe0f Configura\u00e7\u00e3o de Ambiente para Testes","text":"<pre><code># docker-compose.test.yml\nversion: '3.8'\n\nservices:\n  test-db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: test_flight_delays\n      POSTGRES_USER: test_user\n      POSTGRES_PASSWORD: test_pass\n    ports:\n      - \"5433:5432\"\n    volumes:\n      - test_db_data:/var/lib/postgresql/data\n\n  test-redis:\n    image: redis:7-alpine\n    ports:\n      - \"6380:6379\"\n\n  test-api:\n    build:\n      context: .\n      dockerfile: docker/api.Dockerfile\n    environment:\n      - DATABASE_URL=postgresql://test_user:test_pass@test-db:5432/test_flight_delays\n      - REDIS_URL=redis://test-redis:6379/0\n      - ENVIRONMENT=test\n    depends_on:\n      - test-db\n      - test-redis\n    ports:\n      - \"8001:8000\"\n\nvolumes:\n  test_db_data:\n</code></pre>"},{"location":"tests/integration/#makefile-para-testes","title":"\ud83d\udd27 Makefile para Testes","text":"<pre><code># Makefile (adi\u00e7\u00f5es para integra\u00e7\u00e3o)\n.PHONY: test-integration test-e2e test-all-integration\n\ntest-integration:\n    @echo \"\ud83d\udd17 Executando testes de integra\u00e7\u00e3o...\"\n    pytest tests/integration/ -v --cov=src --cov-report=term-missing\n\ntest-e2e:\n    @echo \"\ud83c\udf10 Executando testes end-to-end...\"\n    docker-compose -f docker-compose.test.yml up -d\n    sleep 10\n    pytest tests/e2e/ -v\n    docker-compose -f docker-compose.test.yml down\n\ntest-all-integration: test-integration test-e2e\n    @echo \"\u2705 Todos os testes de integra\u00e7\u00e3o conclu\u00eddos!\"\n\nsetup-test-env:\n    @echo \"\u2699\ufe0f Configurando ambiente de teste...\"\n    docker-compose -f docker-compose.test.yml up -d test-db test-redis\n    sleep 5\n\ncleanup-test-env:\n    @echo \"\ud83e\uddf9 Limpando ambiente de teste...\"\n    docker-compose -f docker-compose.test.yml down -v\n</code></pre>"},{"location":"tests/integration/#relatorios-de-integracao","title":"\ud83d\udcca Relat\u00f3rios de Integra\u00e7\u00e3o","text":""},{"location":"tests/integration/#exemplo-de-relatorio-de-teste","title":"\ud83d\udccb Exemplo de Relat\u00f3rio de Teste","text":"<pre><code>================== RELAT\u00d3RIO DE TESTES DE INTEGRA\u00c7\u00c3O ==================\n\n\ud83d\udd17 Testes de API Completos:\n  \u2705 test_predict_endpoint_complete_flow         PASSED  (2.45s)\n  \u2705 test_batch_prediction_endpoint             PASSED  (1.87s)\n  \u2705 test_model_info_endpoint                   PASSED  (0.32s)\n  \u2705 test_health_check_complete                 PASSED  (0.89s)\n  \u2705 test_error_handling_invalid_data           PASSED  (0.21s)\n  \u2705 test_rate_limiting                         PASSED  (3.12s)\n\n\ud83d\uddc4\ufe0f Testes de Integra\u00e7\u00e3o Database:\n  \u2705 test_flight_prediction_storage             PASSED  (0.67s)\n  \u2705 test_historical_data_query                 PASSED  (0.45s)\n  \u2705 test_model_metrics_tracking                PASSED  (0.38s)\n\n\ud83c\udf10 Testes de APIs Externas:\n  \u2705 test_weather_api_integration               PASSED  (0.28s)\n  \u2705 test_airport_api_integration               PASSED  (0.19s)\n  \u2705 test_api_failure_handling                  PASSED  (1.15s)\n  \u2705 test_api_retry_mechanism                   PASSED  (2.34s)\n\n\ud83e\udd16 Testes de Pipeline ML:\n  \u2705 test_complete_training_pipeline            PASSED  (8.67s)\n  \u2705 test_prediction_pipeline_end_to_end        PASSED  (1.23s)\n  \u2705 test_feature_engineering_pipeline          PASSED  (2.11s)\n  \u2705 test_model_validation_pipeline             PASSED  (3.45s)\n  \u2705 test_data_drift_detection                  PASSED  (1.89s)\n  \u2705 test_model_monitoring_pipeline             PASSED  (2.76s)\n  \u2705 test_automated_retraining_trigger          PASSED  (4.32s)\n\n\ud83c\udf10 Testes End-to-End:\n  \u2705 test_complete_prediction_workflow          PASSED  (4.21s)\n  \u2705 test_batch_processing_workflow             PASSED  (6.78s)\n\n\ud83d\udcca RESUMO:\n  Total de Testes: 18\n  Passou: 18 (100%)\n  Falhou: 0 (0%)\n  Tempo Total: 42.58s\n  Cobertura: 89.4%\n\n\u2705 TODOS OS TESTES DE INTEGRA\u00c7\u00c3O PASSARAM!\n</code></pre>"},{"location":"tests/integration/#proximos-passos","title":"\ud83d\udd17 Pr\u00f3ximos Passos","text":"<ol> <li>\ud83d\udcca Cobertura - An\u00e1lise detalhada de cobertura</li> <li>\ud83e\uddea Testes - Executar suite completa</li> <li>\ud83d\udcd3 Notebooks - An\u00e1lise explorat\u00f3ria</li> </ol>"},{"location":"tests/integration/#referencias","title":"\ud83d\udcde Refer\u00eancias","text":"<ul> <li>\ud83e\uddea pytest-asyncio - Testes async</li> <li>\ud83c\udf10 TestClient - Testes FastAPI</li> <li>\ud83d\uddc4\ufe0f SQLAlchemy Testing - Testes com banco</li> </ul>"},{"location":"tests/running-tests/","title":"\ud83e\uddea Executando Testes","text":"<p>Guia completo para executar e entender os testes automatizados do projeto Machine Learning Engineer Challenge.</p>"},{"location":"tests/running-tests/#visao-geral-dos-testes","title":"\ud83d\udccb Vis\u00e3o Geral dos Testes","text":"<p>O projeto possui uma su\u00edte abrangente de testes automatizados com 100+ testes cobrindo todas as camadas da aplica\u00e7\u00e3o:</p> <ul> <li>\u26a1 Testes de API - Endpoints FastAPI</li> <li>\ud83e\udd16 Testes de ML - Pipeline de Machine Learning  </li> <li>\ud83d\uddc4\ufe0f Testes de Servi\u00e7os - Camada de dados</li> <li>\ud83d\udd04 Testes de Integra\u00e7\u00e3o - Fluxos completos</li> <li>\ud83d\udee0\ufe0f Testes de Utilit\u00e1rios - Fun\u00e7\u00f5es auxiliares</li> </ul>"},{"location":"tests/running-tests/#executando-testes_1","title":"\ud83d\ude80 Executando Testes","text":""},{"location":"tests/running-tests/#comando-rapido","title":"\u26a1 Comando R\u00e1pido","text":"<pre><code># Com ambiente Poetry ativo\ntask test\n\n# Ou sem ativar ambiente\npoetry run task test\n\n# Execu\u00e7\u00e3o direta com pytest\npoetry run pytest\n</code></pre>"},{"location":"tests/running-tests/#com-relatorio-de-cobertura","title":"\ud83d\udcca Com Relat\u00f3rio de Cobertura","text":"<pre><code># Testes com coverage\ntask test-cov\n\n# Ou comando completo\npoetry run pytest --cov=src --cov-report=term-missing --cov-report=html\n\n# Visualizar relat\u00f3rio HTML\nopen htmlcov/index.html  # macOS\nstart htmlcov/index.html  # Windows\nxdg-open htmlcov/index.html  # Linux\n</code></pre>"},{"location":"tests/running-tests/#execucao-seletiva","title":"\ud83c\udfaf Execu\u00e7\u00e3o Seletiva","text":"<pre><code># Executar arquivo espec\u00edfico\npytest tests/test_routers.py -v\n\n# Executar classe espec\u00edfica\npytest tests/test_ml_pipeline.py::TestModelTraining -v\n\n# Executar teste espec\u00edfico\npytest tests/test_routers.py::TestAPIMain::test_health_endpoint -v\n\n# Executar por marca\u00e7\u00e3o\npytest -m \"not slow\" -v\n\n# Executar testes que cont\u00e9m palavra\npytest -k \"test_predict\" -v\n</code></pre>"},{"location":"tests/running-tests/#estrutura-dos-testes","title":"\ud83d\udcca Estrutura dos Testes","text":""},{"location":"tests/running-tests/#organizacao-dos-arquivos","title":"\ud83d\uddc2\ufe0f Organiza\u00e7\u00e3o dos Arquivos","text":"<pre><code>tests/\n\u251c\u2500\u2500 \ud83e\uddea conftest.py              # Configura\u00e7\u00f5es e fixtures globais\n\u251c\u2500\u2500 \u26a1 test_routers.py          # Testes dos endpoints da API\n\u251c\u2500\u2500 \ud83e\udd16 test_ml_pipeline.py      # Testes do pipeline de ML\n\u251c\u2500\u2500 \ud83d\udd04 test_integration.py      # Testes de integra\u00e7\u00e3o end-to-end\n\u251c\u2500\u2500 \ud83d\uddc4\ufe0f test_services.py         # Testes da camada de servi\u00e7os\n\u251c\u2500\u2500 \ud83d\udee0\ufe0f test_utils.py            # Testes de utilit\u00e1rios\n\u2514\u2500\u2500 \ud83d\udccb run_tests.py            # Script de execu\u00e7\u00e3o personalizado\n</code></pre>"},{"location":"tests/running-tests/#cobertura-atual","title":"\ud83d\udcc8 Cobertura Atual","text":"<pre><code>======================== Coverage Report ========================\nName                           Stmts   Miss  Cover   Missing\n------------------------------------------------------------\nsrc/routers/main.py               45      3    93%   \nsrc/routers/model/predict.py      68      5    93%   \nsrc/routers/model/load.py         42      2    95%   \nsrc/routers/model/history.py      35      1    97%   \nsrc/services/database.py          58      8    86%   \n------------------------------------------------------------\nTOTAL                            248     19    92%\n</code></pre>"},{"location":"tests/running-tests/#testes-da-api-test_routerspy","title":"\u26a1 Testes da API (test_routers.py)","text":""},{"location":"tests/running-tests/#cobertura-dos-endpoints","title":"\ud83c\udfaf Cobertura dos Endpoints","text":"Classe de Teste Endpoints Testados Cen\u00e1rios <code>TestAPIMain</code> <code>/</code>, <code>/health</code>, <code>/docs</code> Status codes, response format <code>TestPredictEndpoint</code> <code>/model/predict</code> Predi\u00e7\u00e3o \u00fanica, batch, valida\u00e7\u00e3o <code>TestModelLoader</code> <code>/model/load/*</code> Carregamento, upload, erros <code>TestHistoryEndpoint</code> <code>/model/history/</code> Pagina\u00e7\u00e3o, filtros, estat\u00edsticas <code>TestAPIErrorHandling</code> Todos C\u00f3digos de erro, mensagens <p>Exemplo de execu\u00e7\u00e3o: <pre><code># Executar apenas testes da API\npytest tests/test_routers.py -v\n\n# Sa\u00edda esperada:\ntests/test_routers.py::TestAPIMain::test_root_endpoint PASSED\ntests/test_routers.py::TestAPIMain::test_health_endpoint PASSED\ntests/test_routers.py::TestPredictEndpoint::test_predict_success PASSED\ntests/test_routers.py::TestPredictEndpoint::test_predict_batch PASSED\n...\n======================== 25 passed in 8.45s ========================\n</code></pre></p>"},{"location":"tests/running-tests/#casos-de-teste-principais","title":"\ud83d\udca1 Casos de Teste Principais","text":"<p>Health Check: <pre><code>def test_health_endpoint(api_client):\n    response = api_client.get(\"/health\")\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"status\"] in [\"healthy\", \"unhealthy\"]\n    assert \"timestamp\" in data\n    assert \"components\" in data\n</code></pre></p> <p>Predi\u00e7\u00e3o com Valida\u00e7\u00e3o: <pre><code>def test_predict_with_validation(api_client, sample_flight_data):\n    response = api_client.post(\"/model/predict\", json=sample_flight_data)\n    assert response.status_code == 200\n\n    data = response.json()\n    assert \"prediction\" in data\n    assert \"probability\" in data[\"prediction\"]\n    assert 0 &lt;= data[\"prediction\"][\"probability\"] &lt;= 1\n</code></pre></p>"},{"location":"tests/running-tests/#testes-de-ml-test_ml_pipelinepy","title":"\ud83e\udd16 Testes de ML (test_ml_pipeline.py)","text":""},{"location":"tests/running-tests/#cobertura-do-pipeline","title":"\ud83e\udde0 Cobertura do Pipeline","text":"Classe de Teste Componente Testado Valida\u00e7\u00f5es <code>TestDataProcessing</code> Preprocessamento Limpeza, transforma\u00e7\u00f5es <code>TestFeatureEngineering</code> Feature engineering Cria\u00e7\u00e3o de features <code>TestModelTraining</code> Treinamento Algoritmos, hiperpar\u00e2metros <code>TestModelEvaluation</code> Avalia\u00e7\u00e3o M\u00e9tricas, valida\u00e7\u00e3o cruzada <code>TestModelPersistence</code> Persist\u00eancia Salvamento, carregamento <p>Exemplo de execu\u00e7\u00e3o: <pre><code># Executar testes de ML\npytest tests/test_ml_pipeline.py -v --tb=short\n\n# Com logs de ML\npytest tests/test_ml_pipeline.py -v -s\n</code></pre></p>"},{"location":"tests/running-tests/#testes-de-qualidade-do-modelo","title":"\ud83d\udcca Testes de Qualidade do Modelo","text":"<pre><code>def test_model_accuracy_threshold():\n    \"\"\"Garante que o modelo atende crit\u00e9rio m\u00ednimo de qualidade\"\"\"\n    model = load_trained_model()\n    X_test, y_test = load_test_data()\n\n    predictions = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n\n    # Crit\u00e9rio de qualidade: m\u00ednimo 85% de accuracy\n    assert accuracy &gt;= 0.85, f\"Accuracy {accuracy:.2f} abaixo do m\u00ednimo 0.85\"\n</code></pre>"},{"location":"tests/running-tests/#testes-de-integracao-test_integrationpy","title":"\ud83d\udd04 Testes de Integra\u00e7\u00e3o (test_integration.py)","text":""},{"location":"tests/running-tests/#fluxos-end-to-end","title":"\ud83c\udf10 Fluxos End-to-End","text":"<pre><code>def test_complete_prediction_workflow(api_client):\n    \"\"\"Testa fluxo completo: carregar modelo \u2192 predi\u00e7\u00e3o \u2192 hist\u00f3rico\"\"\"\n\n    # 1. Carregar modelo\n    load_response = api_client.get(\"/model/load/default\")\n    assert load_response.status_code == 200\n\n    # 2. Fazer predi\u00e7\u00e3o\n    predict_response = api_client.post(\"/model/predict\", json=sample_data)\n    assert predict_response.status_code == 200\n\n    prediction_id = predict_response.json()[\"prediction_id\"]\n\n    # 3. Verificar no hist\u00f3rico\n    history_response = api_client.get(\"/model/history/\")\n    assert history_response.status_code == 200\n\n    history_data = history_response.json()\n    prediction_ids = [p[\"prediction_id\"] for p in history_data[\"predictions\"]]\n    assert prediction_id in prediction_ids\n</code></pre>"},{"location":"tests/running-tests/#testes-de-performance","title":"\ud83c\udfaf Testes de Performance","text":"<pre><code>@pytest.mark.performance\ndef test_prediction_performance():\n    \"\"\"Garante que predi\u00e7\u00f5es s\u00e3o executadas em tempo aceit\u00e1vel\"\"\"\n    import time\n\n    start_time = time.time()\n\n    # Fazer m\u00faltiplas predi\u00e7\u00f5es\n    for _ in range(100):\n        response = api_client.post(\"/model/predict\", json=sample_data)\n        assert response.status_code == 200\n\n    elapsed_time = time.time() - start_time\n\n    # Crit\u00e9rio: m\u00e1ximo 10ms por predi\u00e7\u00e3o em m\u00e9dia\n    assert elapsed_time / 100 &lt; 0.01, f\"Performance inadequada: {elapsed_time/100:.3f}s por predi\u00e7\u00e3o\"\n</code></pre>"},{"location":"tests/running-tests/#testes-de-servicos-test_servicespy","title":"\ud83d\uddc4\ufe0f Testes de Servi\u00e7os (test_services.py)","text":""},{"location":"tests/running-tests/#testes-de-database","title":"\ud83d\udcbe Testes de Database","text":"<pre><code>def test_database_connection():\n    \"\"\"Testa conex\u00e3o com banco de dados\"\"\"\n    from src.services.database import get_database\n\n    db = get_database()\n    assert db is not None\n\n    # Teste de inser\u00e7\u00e3o\n    test_doc = {\"test\": \"data\", \"timestamp\": datetime.now()}\n    result = db.predictions.insert_one(test_doc)\n    assert result.inserted_id is not None\n\n    # Limpeza\n    db.predictions.delete_one({\"_id\": result.inserted_id})\n</code></pre>"},{"location":"tests/running-tests/#configuracao-dos-testes","title":"\ud83d\udee0\ufe0f Configura\u00e7\u00e3o dos Testes","text":""},{"location":"tests/running-tests/#fixtures-principais-conftestpy","title":"\ud83d\udccb Fixtures Principais (conftest.py)","text":"<pre><code>@pytest.fixture\ndef api_client():\n    \"\"\"Cliente de teste para a API\"\"\"\n    from fastapi.testclient import TestClient\n    from src.routers.main import app\n\n    return TestClient(app)\n\n@pytest.fixture\ndef sample_flight_data():\n    \"\"\"Dados de exemplo para testes\"\"\"\n    return {\n        \"features\": {\n            \"airline\": \"American Airlines\",\n            \"flight_number\": \"AA123\",\n            \"departure_airport\": \"JFK\",\n            \"arrival_airport\": \"LAX\",\n            \"scheduled_departure\": \"2024-01-15T10:00:00\",\n            \"scheduled_arrival\": \"2024-01-15T14:00:00\"\n        }\n    }\n\n@pytest.fixture\ndef mock_model():\n    \"\"\"Modelo mock para testes sem depend\u00eancias\"\"\"\n    class MockModel:\n        def predict(self, X):\n            return [0] * len(X)  # Sempre n\u00e3o cancelado\n        def predict_proba(self, X):\n            return [[0.8, 0.2]] * len(X)\n\n    return MockModel()\n</code></pre>"},{"location":"tests/running-tests/#configuracao-pytestini","title":"\u2699\ufe0f Configura\u00e7\u00e3o pytest.ini","text":"<pre><code>[tool:pytest]\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\naddopts = \n    -v\n    --strict-markers\n    --tb=short\n    --cov=src\n    --cov-report=term-missing\n    --cov-fail-under=85\n\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    unit: marks tests as unit tests\n    performance: marks tests as performance tests\n</code></pre>"},{"location":"tests/running-tests/#relatorios-de-cobertura","title":"\ud83d\udcca Relat\u00f3rios de Cobertura","text":""},{"location":"tests/running-tests/#coverage-html","title":"\ud83c\udfaf Coverage HTML","text":"<pre><code># Gerar relat\u00f3rio HTML completo\npoetry run pytest --cov=src --cov-report=html --cov-report=term\n\n# Estrutura do relat\u00f3rio gerado:\nhtmlcov/\n\u251c\u2500\u2500 index.html              # P\u00e1gina principal\n\u251c\u2500\u2500 src_routers_main_py.html   # Coverage por arquivo\n\u2514\u2500\u2500 ...                     # Outros arquivos\n</code></pre>"},{"location":"tests/running-tests/#coverage-xml-cicd","title":"\ud83d\udccb Coverage XML (CI/CD)","text":"<pre><code># Para integra\u00e7\u00e3o com CI/CD\npoetry run pytest --cov=src --cov-report=xml\n\n# Gera: coverage.xml\n</code></pre>"},{"location":"tests/running-tests/#debugging-de-testes","title":"\ud83d\udea8 Debugging de Testes","text":""},{"location":"tests/running-tests/#execucao-com-debug","title":"\ud83d\udd0d Execu\u00e7\u00e3o com Debug","text":"<pre><code># Modo verbose com traceback completo\npytest -vvv --tb=long\n\n# Parar no primeiro erro\npytest -x\n\n# Executar com pdb (debugger)\npytest --pdb\n\n# Mostrar prints durante execu\u00e7\u00e3o\npytest -s\n\n# Executar apenas testes que falharam na \u00faltima execu\u00e7\u00e3o\npytest --lf\n</code></pre>"},{"location":"tests/running-tests/#logs-detalhados","title":"\ud83d\udccb Logs Detalhados","text":"<pre><code>import logging\n\ndef test_with_logging(caplog):\n    \"\"\"Teste que captura logs\"\"\"\n    with caplog.at_level(logging.INFO):\n        # C\u00f3digo que gera logs\n        pass\n\n    assert \"Expected log message\" in caplog.text\n</code></pre>"},{"location":"tests/running-tests/#marcacoes-de-teste","title":"\ud83c\udfaf Marca\u00e7\u00f5es de Teste","text":""},{"location":"tests/running-tests/#usando-markers","title":"\ud83c\udff7\ufe0f Usando Markers","text":"<pre><code>import pytest\n\n@pytest.mark.slow\ndef test_expensive_operation():\n    \"\"\"Teste que demora para executar\"\"\"\n    pass\n\n@pytest.mark.integration\ndef test_full_workflow():\n    \"\"\"Teste de integra\u00e7\u00e3o\"\"\"\n    pass\n\n@pytest.mark.parametrize(\"input,expected\", [\n    (\"JFK\", \"New York\"),\n    (\"LAX\", \"Los Angeles\"),\n])\ndef test_airport_codes(input, expected):\n    \"\"\"Teste parametrizado\"\"\"\n    pass\n</code></pre> <p>Executar por marca\u00e7\u00e3o: <pre><code># Apenas testes r\u00e1pidos\npytest -m \"not slow\"\n\n# Apenas testes de integra\u00e7\u00e3o\npytest -m integration\n\n# Combina\u00e7\u00f5es\npytest -m \"integration and not slow\"\n</code></pre></p>"},{"location":"tests/running-tests/#cicd-integration","title":"\ud83d\udd04 CI/CD Integration","text":""},{"location":"tests/running-tests/#github-actions","title":"\ud83d\ude80 GitHub Actions","text":"<pre><code># .github/workflows/tests.yml\nname: Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.12\n\n    - name: Install Poetry\n      run: pip install poetry\n\n    - name: Install dependencies\n      run: poetry install\n\n    - name: Run tests\n      run: poetry run pytest --cov=src --cov-report=xml\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v1\n</code></pre>"},{"location":"tests/running-tests/#melhores-praticas","title":"\ud83d\udcda Melhores Pr\u00e1ticas","text":""},{"location":"tests/running-tests/#principios-de-bons-testes","title":"\u2705 Princ\u00edpios de Bons Testes","text":"<ol> <li>\ud83c\udfaf AAA Pattern: Arrange, Act, Assert</li> <li>\ud83d\udd2c Isolamento: Testes independentes</li> <li>\ud83d\udccb Nomenclatura clara: Nomes descritivos</li> <li>\u26a1 Velocidade: Testes r\u00e1pidos</li> <li>\ud83d\udd01 Determinismo: Resultados consistentes</li> </ol>"},{"location":"tests/running-tests/#evitar","title":"\ud83d\udea8 Evitar","text":"<ul> <li>\u274c Testes interdependentes</li> <li>\u274c Hard-coded values sem contexto</li> <li>\u274c Testes muito longos</li> <li>\u274c M\u00faltiplas assertivas n\u00e3o relacionadas</li> <li>\u274c Dados de teste n\u00e3o realistas</li> </ul>"},{"location":"tests/running-tests/#suporte","title":"\ud83d\udcde Suporte","text":""},{"location":"tests/running-tests/#problemas-com-testes","title":"\ud83d\udc1b Problemas com Testes","text":"<ul> <li>\ud83d\udd27 Troubleshooting</li> <li>\ud83d\udcd6 Coverage Detalhado</li> <li>\ud83d\udd04 Testes de Integra\u00e7\u00e3o</li> <li>\ud83d\udc1b Issues</li> </ul>"}]}